{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(input_, filter_size, out_channels, stride_size, padding_name, is_training, \n",
    "               activation_fn=None, identifier=''):\n",
    "    \n",
    "    in_channels = input_.shape[-1].value\n",
    "\n",
    "    # initialize filter weights \n",
    "    filter_weights = tf.Variable(tf.truncated_normal(shape=[filter_size, filter_size, in_channels, out_channels]))\n",
    "    \n",
    "    conv_output = tf.nn.conv2d(input_, filter_weights, strides = [1, stride_size, stride_size, 1], \n",
    "                               padding = padding_name, name=identifier)\n",
    "\n",
    "    print(\"conv_output shape\", conv_output.shape)\n",
    "    out_channels = conv_output.shape[-1].value\n",
    "    #\n",
    "    gamma = tf.Variable(tf.ones([out_channels]))\n",
    "    beta = tf.Variable(tf.zeros([out_channels]))\n",
    "    \n",
    "    popu_mean = tf.Variable(tf.zeros([out_channels]), trainable=False)\n",
    "    popu_var = tf.Variable(tf.ones([out_channels]), trainable=False)\n",
    "    \n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    def batch_norm_training():\n",
    "        \n",
    "        batch_mean, batch_var = tf.nn.moments(conv_output, [0,1,2])\n",
    "        \n",
    "        decay=0.99\n",
    "        train_mean = tf.assign(popu_mean, popu_mean * decay + batch_mean*(1-decay))\n",
    "        train_var = tf.assign(popu_var, popu_var * decay + batch_var*(1-decay))\n",
    "\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            normalized_conv_output = (conv_output - batch_mean) / tf.sqrt(batch_var + epsilon)\n",
    "            return gamma * normalized_conv_output + beta\n",
    "            \n",
    "    def batch_norm_inference():\n",
    "        normalized_conv_output = (conv_output - popu_mean) / tf.sqrt(popu_var + epsilon)\n",
    "        return gamma * normalized_conv_output + beta\n",
    "        \n",
    "    batch_normalized_output = tf.cond(is_training, batch_norm_training, batch_norm_inference)\n",
    "    \n",
    "    if activation_fn:\n",
    "        return activation_fn(batch_normalized_output)\n",
    "    else:\n",
    "        return batch_normalized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_polling_2d(input_, filter_size, stride_size, padding_name, identifier):\n",
    "    return tf.nn.max_pool(input_, ksize=[1, filter_size, filter_size, 1], strides=[1, stride_size, stride_size, 1],\n",
    "                          padding=padding_name, name=identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(input_, is_training, filter_numbers, stage, block, filters=2):\n",
    "    \n",
    "    prefix = 's' + str(stage) + '_' + str(block)\n",
    "    main_branch = prefix + '_main_branch_layer'\n",
    "    \n",
    "    F1, F2, F3 = filter_numbers\n",
    "    activation_fn = tf.nn.relu\n",
    "    short_cut = input_\n",
    "    conv_output = conv_layer(input_, 1, F1, 1, \"VALID\", is_training, tf.nn.relu, main_branch+'1')\n",
    "    conv_output = conv_layer(conv_output, filters, F2, 1, \"SAME\", is_training, tf.nn.relu, main_branch+'2')\n",
    "    conv_output = conv_layer(conv_output, 1, F3, 1, \"VALID\", is_training, None, main_branch+'3')\n",
    "    \n",
    "    # add conv_output and short_cut\n",
    "    print(prefix + \"_conv_output:\", conv_output.shape)\n",
    "    print(prefix + \"_short_cut:\", short_cut.shape)\n",
    "    merge_output = tf.add(conv_output, short_cut)\n",
    "    return activation_fn(merge_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_output shape (3, 4, 4, 2)\n",
      "conv_output shape (3, 4, 4, 4)\n",
      "conv_output shape (3, 4, 4, 6)\n",
      "s1_1_conv_output: (3, 4, 4, 6)\n",
      "s1_1_short_cut: (3, 4, 4, 6)\n",
      "out shape: (3, 4, 4, 6)\n",
      "out = [[[[1.86426163e-01 1.09220004e+00 1.49914622e+00 0.00000000e+00\n",
      "    4.10130620e-01 3.02119160e+00]\n",
      "   [0.00000000e+00 2.28442326e-01 0.00000000e+00 9.30261552e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.24325192e+00\n",
      "    1.50421178e+00 7.57815540e-02]\n",
      "   [1.37787247e+00 0.00000000e+00 0.00000000e+00 5.04302025e-01\n",
      "    1.14520073e-01 1.85038030e-01]]\n",
      "\n",
      "  [[7.90697932e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "    0.00000000e+00 2.25848150e+00]\n",
      "   [0.00000000e+00 7.33518600e-03 0.00000000e+00 8.78578186e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [2.78966331e+00 6.95649505e-01 1.67922139e+00 0.00000000e+00\n",
      "    8.54227364e-01 3.63426375e+00]\n",
      "   [1.31497681e-01 0.00000000e+00 0.00000000e+00 1.79516482e+00\n",
      "    1.79905856e+00 0.00000000e+00]]\n",
      "\n",
      "  [[1.69496405e+00 1.91817832e+00 3.28873467e+00 0.00000000e+00\n",
      "    1.71810889e+00 1.62804008e+00]\n",
      "   [0.00000000e+00 3.00467062e+00 3.71689498e-01 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 9.95096922e-01 0.00000000e+00\n",
      "    0.00000000e+00 1.35298193e+00]\n",
      "   [1.32722843e+00 1.87578344e+00 3.37120593e-01 2.02689815e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 0.00000000e+00 0.00000000e+00 6.17119133e-01\n",
      "    8.21442306e-02 7.38363862e-01]\n",
      "   [7.67303288e-01 1.00273073e+00 0.00000000e+00 2.68851548e-01\n",
      "    2.64099717e+00 1.17034531e+00]\n",
      "   [0.00000000e+00 3.21955180e+00 6.07612193e-01 1.28157043e+00\n",
      "    4.89560068e-01 0.00000000e+00]\n",
      "   [3.58550906e-01 4.98850942e-01 0.00000000e+00 0.00000000e+00\n",
      "    7.23476112e-01 0.00000000e+00]]]\n",
      "\n",
      "\n",
      " [[[2.84533405e+00 8.74780416e-02 2.26457477e+00 0.00000000e+00\n",
      "    0.00000000e+00 2.33777761e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 6.59313977e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.98850393e+00 0.00000000e+00 5.14067471e-01 0.00000000e+00\n",
      "    0.00000000e+00 1.32337737e+00]\n",
      "   [0.00000000e+00 5.81561327e-01 0.00000000e+00 5.73205948e-01\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[1.53336883e-01 1.29534674e+00 2.80857992e+00 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 3.22471857e+00 8.77568007e-01 0.00000000e+00\n",
      "    0.00000000e+00 1.01429129e+00]\n",
      "   [0.00000000e+00 5.83035469e-01 0.00000000e+00 4.88585889e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [4.41121548e-01 2.62476236e-01 0.00000000e+00 1.04300165e+00\n",
      "    7.87670314e-02 0.00000000e+00]]\n",
      "\n",
      "  [[4.21004474e-01 0.00000000e+00 0.00000000e+00 2.24311876e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [7.39839137e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "    0.00000000e+00 8.84053349e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 2.46276736e-01 0.00000000e+00\n",
      "    1.33030844e+00 0.00000000e+00]\n",
      "   [9.30670023e-01 0.00000000e+00 0.00000000e+00 7.78815627e-01\n",
      "    7.20051885e-01 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 1.21450388e+00 0.00000000e+00 7.68439591e-01\n",
      "    1.79375544e-01 0.00000000e+00]\n",
      "   [0.00000000e+00 3.18827295e+00 0.00000000e+00 1.36211908e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 7.89717913e-01 0.00000000e+00\n",
      "    1.50141096e+00 6.20305181e-01]\n",
      "   [0.00000000e+00 2.82233804e-01 5.13679624e-01 9.65122342e-01\n",
      "    2.76619554e-01 0.00000000e+00]]]\n",
      "\n",
      "\n",
      " [[[1.35169053e+00 0.00000000e+00 2.29753637e+00 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "    0.00000000e+00 1.54058444e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 3.62204909e-01\n",
      "    4.16068912e-01 0.00000000e+00]]\n",
      "\n",
      "  [[1.23134589e+00 0.00000000e+00 0.00000000e+00 2.52406645e+00\n",
      "    0.00000000e+00 1.17182732e-03]\n",
      "   [0.00000000e+00 3.44101667e-01 0.00000000e+00 0.00000000e+00\n",
      "    9.68813241e-01 1.12069845e-01]\n",
      "   [6.64213419e-01 1.79840469e+00 2.57945991e+00 0.00000000e+00\n",
      "    0.00000000e+00 4.63402629e-01]\n",
      "   [6.73023641e-01 3.55748951e-01 0.00000000e+00 1.13297355e+00\n",
      "    1.45948339e+00 0.00000000e+00]]\n",
      "\n",
      "  [[5.17626762e-01 2.39129472e+00 1.24833226e+00 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 8.03861499e-01\n",
      "    0.00000000e+00 0.00000000e+00]\n",
      "   [1.13849247e+00 0.00000000e+00 7.93793499e-01 0.00000000e+00\n",
      "    0.00000000e+00 1.66507268e+00]\n",
      "   [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.07827210e+00\n",
      "    0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "  [[0.00000000e+00 6.13284469e-01 0.00000000e+00 7.59147882e-01\n",
      "    8.44069064e-01 1.94404095e-01]\n",
      "   [0.00000000e+00 0.00000000e+00 9.63651478e-01 3.71932924e-01\n",
      "    5.69531441e-01 0.00000000e+00]\n",
      "   [1.54631197e+00 0.00000000e+00 0.00000000e+00 9.52461123e-01\n",
      "    8.51870000e-01 2.58272290e-01]\n",
      "   [0.00000000e+00 4.83402550e-01 0.00000000e+00 1.77440643e-02\n",
      "    0.00000000e+00 0.00000000e+00]]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as test:\n",
    "    A_prev = tf.placeholder(\"float\", [3, 4, 4, 6])\n",
    "    is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "    X = np.random.randn(3, 4, 4, 6)\n",
    "    A = identity_block(A_prev, is_training, filter_numbers = [2, 4, 6], stage = 1,block = 1)\n",
    "    test.run(tf.global_variables_initializer())\n",
    "    out = test.run([A], feed_dict={A_prev: X, is_training:True})\n",
    "    print(\"out shape:\", out[0].shape)\n",
    "    print(\"out = \" + str(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(input_, is_training, filter_numbers, stage, block, filters=2, strides=2):\n",
    "    \n",
    "    prefix = 's' + str(stage) + '_conv_block' + str(block)\n",
    "    main_branch = prefix + '_main_branch_layer'\n",
    "    short_cut_branch = prefix + 'short_cut_branch_layer'\n",
    "    \n",
    "    F1, F2, F3 = filter_numbers\n",
    "    activation_fn = tf.nn.relu\n",
    "    short_cut = input_\n",
    "    \n",
    "    conv_output = conv_layer(input_, 1, F1, strides, \"VALID\", is_training, activation_fn, main_branch+'1')\n",
    "    conv_output = conv_layer(conv_output, filters, F2, 1, \"SAME\", is_training, activation_fn, main_branch+'2')\n",
    "    conv_output = conv_layer(conv_output, 1, F3, 1, \"VALID\", is_training, None, main_branch + '3')\n",
    "    \n",
    "    short_cut = conv_layer(short_cut, 1, F3, strides, \"VALID\", is_training, None, short_cut_branch + '1')\n",
    "    \n",
    "    # add conv_output and short_cut\n",
    "    print(\"conv_output:\", conv_output.shape)\n",
    "    print(\"short_cut:\", short_cut.shape)\n",
    "    merge_output = tf.add(conv_output, short_cut)\n",
    "    return activation_fn(merge_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_output shape (3, 2, 2, 2)\n",
      "conv_output shape (3, 2, 2, 4)\n",
      "conv_output shape (3, 2, 2, 6)\n",
      "conv_output shape (3, 2, 2, 6)\n",
      "conv_output: (3, 2, 2, 6)\n",
      "short_cut: (3, 2, 2, 6)\n",
      "out shape: (3, 2, 2, 6)\n",
      "out = [[[[0.         1.6671418  0.         0.         0.         0.        ]\n",
      "   [2.6019182  1.3565595  2.7631233  0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         1.1593063  0.         2.2558084 ]\n",
      "   [0.62923217 0.         1.5977818  0.15540045 0.8058351  0.93204105]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         1.0328366  1.2184459  0.        ]\n",
      "   [2.0531018  0.         1.6534753  0.         0.         0.22060102]]\n",
      "\n",
      "  [[0.         2.120238   0.         0.         0.         0.        ]\n",
      "   [0.         0.         0.         2.2181234  3.047166   2.1592522 ]]]\n",
      "\n",
      "\n",
      " [[[0.19914399 0.         0.250405   0.3909369  0.         0.        ]\n",
      "   [0.24958289 0.5537703  0.         2.0094137  0.8390813  0.7181118 ]]\n",
      "\n",
      "  [[0.05527666 1.4917161  0.         0.40387866 0.         0.        ]\n",
      "   [0.41045353 0.         1.3521879  0.37948564 0.95025307 0.76760745]]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as test:\n",
    "    A_prev = tf.placeholder(\"float\", [3, 4, 4, 6])\n",
    "    is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "    X = np.random.randn(3, 4, 4, 6)\n",
    "    A = convolutional_block(A_prev, is_training, filter_numbers = [2, 4, 6], stage=1, block=1)\n",
    "    test.run(tf.global_variables_initializer())\n",
    "    out = test.run([A], feed_dict={A_prev: X, is_training:True})\n",
    "    print(\"out shape:\", out[0].shape)\n",
    "    print(\"out = \" + str(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Building your ResNet model (50 layers)\n",
    "\n",
    "You now have the necessary blocks to build a very deep ResNet. The following figure describes in detail the architecture of this neural network. \"ID BLOCK\" in the diagram stands for \"Identity block,\" and \"ID BLOCK x3\" means you should stack 3 identity blocks together.\n",
    "\n",
    "<img src=\"images/resnet_kiank.png\" style=\"width:850px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 5** </u><font color='purple'>  : **ResNet-50 model** </center></caption>\n",
    "\n",
    "The details of this ResNet-50 model are:\n",
    "- Zero-padding pads the input with a pad of (3,3)\n",
    "- Stage 1:\n",
    "    - The 2D Convolution has 64 filters of shape (7,7) and uses a stride of (2,2). Its name is \"conv1\".\n",
    "    - BatchNorm is applied to the channels axis of the input.\n",
    "    - MaxPooling uses a (3,3) window and a (2,2) stride.\n",
    "- Stage 2:\n",
    "    - The convolutional block uses three set of filters of size [64,64,256], \"f\" is 3, \"s\" is 1 and the block is \"a\".\n",
    "    - The 2 identity blocks use three set of filters of size [64,64,256], \"f\" is 3 and the blocks are \"b\" and \"c\".\n",
    "- Stage 3:\n",
    "    - The convolutional block uses three set of filters of size [128,128,512], \"f\" is 3, \"s\" is 2 and the block is \"a\".\n",
    "    - The 3 identity blocks use three set of filters of size [128,128,512], \"f\" is 3 and the blocks are \"b\", \"c\" and \"d\".\n",
    "- Stage 4:\n",
    "    - The convolutional block uses three set of filters of size [256, 256, 1024], \"f\" is 3, \"s\" is 2 and the block is \"a\".\n",
    "    - The 5 identity blocks use three set of filters of size [256, 256, 1024], \"f\" is 3 and the blocks are \"b\", \"c\", \"d\", \"e\" and \"f\".\n",
    "- Stage 5:\n",
    "    - The convolutional block uses three set of filters of size [512, 512, 2048], \"f\" is 3, \"s\" is 2 and the block is \"a\".\n",
    "    - The 2 identity blocks use three set of filters of size [512, 512, 2048], \"f\" is 3 and the blocks are \"b\" and \"c\".\n",
    "- The 2D Average Pooling uses a window of shape (2,2) and its name is \"avg_pool\".\n",
    "- The flatten doesn't have any hyperparameters or name.\n",
    "- The Fully Connected (Dense) layer reduces its input to the number of classes using a softmax activation. Its name should be `'fc' + str(classes)`.\n",
    "\n",
    "**Exercise**: Implement the ResNet with 50 layers described in the figure above. We have implemented Stages 1 and 2. Please implement the rest. (The syntax for implementing Stages 3-5 should be quite similar to that of Stage 2.) Make sure you follow the naming convention in the text above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet:\n",
    "    \n",
    "    def __init__(self, class_num):\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "        self.keep_probability = tf.placeholder(tf.float32, name=\"keep_probability\")\n",
    "        self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "        self.Y = tf.placeholder(tf.float32, shape=[None, class_num])\n",
    "\n",
    "        # Stage 1\n",
    "        x = conv_layer(self.X, 7, 64, 2, \"VALID\", self.is_training, tf.nn.relu, 'stage_1')\n",
    "        x = max_polling_2d(x, 3, 2, \"VALID\", \"stage_1_max_pooling\")\n",
    "\n",
    "        # stage 2\n",
    "        x = convolutional_block(x, self.is_training, [64,64,256], 2, 1, filters=3, strides=1)\n",
    "        x = identity_block(x, self.is_training, [64,64,256], 2, 2, filters=3)\n",
    "        x = identity_block(x, self.is_training, [64,64,256], 2, 3, filters=3)\n",
    "\n",
    "        # stage 3\n",
    "        x = convolutional_block(x, self.is_training, [128,128,512], 3, 1, filters=3, strides=2)\n",
    "        x = identity_block(x, self.is_training, [128,128,512], 3, 2, filters=3)\n",
    "        x = identity_block(x, self.is_training, [128,128,512], 3, 3, filters=3)\n",
    "        x = identity_block(x, self.is_training, [128,128,512], 3, 4, filters=3)\n",
    "\n",
    "#         # stage 4\n",
    "#         x = convolutional_block(x, self.is_training, [256, 256, 1024], 4, 1, filters=3, strides=2)\n",
    "#         x = identity_block(x, self.is_training, [256, 256, 1024], 4, 2, filters=3)\n",
    "#         x = identity_block(x, self.is_training, [256, 256, 1024], 4, 3, filters=3)\n",
    "#         x = identity_block(x, self.is_training, [256, 256, 1024], 4, 4, filters=3)\n",
    "#         x = identity_block(x, self.is_training, [256, 256, 1024], 4, 5, filters=3)\n",
    "#         x = identity_block(x, self.is_training, [256, 256, 1024], 4, 6, filters=3)\n",
    "\n",
    "#         # stage 5\n",
    "#         x = convolutional_block(x, self.is_training, [512, 512, 2048], 5, 1, filters=3, strides=2)\n",
    "#         x = identity_block(x, self.is_training, [512, 512, 2048], 5, 2, filters=3)\n",
    "#         x = identity_block(x, self.is_training, [512, 512, 2048], 5, 3, filters=3)\n",
    "\n",
    "        print(\"input to dense layer:\", x.shape)\n",
    "        x = tf.reshape(x, [-1, x.shape[1].value*x.shape[2].value*x.shape[3].value])\n",
    "        print(\"input to dense layer:\", x.shape)\n",
    "    #     x = Flatten()(x)\n",
    "\n",
    "        dense = tf.layers.dense(inputs=x, units=1024, activation=tf.nn.relu)\n",
    "#         dropout = tf.nn.dropout(dense, self.keep_probability)\n",
    "\n",
    "        self.logits = tf.layers.dense(inputs=dense, units=class_num)\n",
    "        self.prediction = tf.nn.softmax(self.logits, name=\"softmax_tensor\")\n",
    "\n",
    "        print(\"prediction shape:\", self.prediction.shape)\n",
    "        print(\"Y shape:\", self.Y.shape)\n",
    "        # loss\n",
    "        self.loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        self.train_op = self.optimizer.minimize(self.loss_op)\n",
    "        \n",
    "        # Accuracy\n",
    "        self.correct_pred = tf.equal(tf.argmax(self.prediction, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n",
    "    \n",
    "    def train(self, epochs=10, batch_size=128):\n",
    "        \n",
    "        test_valid_size = 256\n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            sess.run(init)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "\n",
    "                for batch in range(mnist.train.num_examples//batch_size + 1):\n",
    "                    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                    _, loss = sess.run([self.train_op, self.loss_op] , feed_dict={self.X:batch_x, self.Y:batch_y, self.is_training:True})\n",
    "                    acc = sess.run(self.accuracy, feed_dict={\n",
    "                            self.X: mnist.validation.images[:test_valid_size],\n",
    "                            self.Y: mnist.validation.labels[:test_valid_size], self.is_training:False})\n",
    "                    if batch % 50 == 0:\n",
    "                        print(\"batch \" + str(batch) + \", Minibatch Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "                        \n",
    "            test_acc = sess.run(self.accuracy, feed_dict={\n",
    "            self.X: mnist.test.images[:test_valid_size],\n",
    "            self.Y: mnist.test.labels[:test_valid_size], self.is_training:False})\n",
    "            print('Testing Accuracy: {}'.format(test_acc))\n",
    "    \n",
    "#     def test(self):\n",
    "#         test_valid_size = 256\n",
    "#         with tf.Session() as sess:\n",
    "#             # Calculate Test Accuracy\n",
    "#             print(\"here\")\n",
    "#             test_acc = sess.run(self.accuracy, feed_dict={\n",
    "#             self.X: mnist.test.images[:test_valid_size],\n",
    "#             self.Y: mnist.test.labels[:test_valid_size], self.is_training:False})\n",
    "#             print('Testing Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/tensorflow/mnist/input_data\", one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_output shape (?, 11, 11, 64)\n",
      "conv_output shape (?, 5, 5, 64)\n",
      "conv_output shape (?, 5, 5, 64)\n",
      "conv_output shape (?, 5, 5, 256)\n",
      "conv_output shape (?, 5, 5, 256)\n",
      "conv_output: (?, 5, 5, 256)\n",
      "short_cut: (?, 5, 5, 256)\n",
      "conv_output shape (?, 5, 5, 64)\n",
      "conv_output shape (?, 5, 5, 64)\n",
      "conv_output shape (?, 5, 5, 256)\n",
      "s2_2_conv_output: (?, 5, 5, 256)\n",
      "s2_2_short_cut: (?, 5, 5, 256)\n",
      "conv_output shape (?, 5, 5, 64)\n",
      "conv_output shape (?, 5, 5, 64)\n",
      "conv_output shape (?, 5, 5, 256)\n",
      "s2_3_conv_output: (?, 5, 5, 256)\n",
      "s2_3_short_cut: (?, 5, 5, 256)\n",
      "conv_output shape (?, 3, 3, 128)\n",
      "conv_output shape (?, 3, 3, 128)\n",
      "conv_output shape (?, 3, 3, 512)\n",
      "conv_output shape (?, 3, 3, 512)\n",
      "conv_output: (?, 3, 3, 512)\n",
      "short_cut: (?, 3, 3, 512)\n",
      "conv_output shape (?, 3, 3, 128)\n",
      "conv_output shape (?, 3, 3, 128)\n",
      "conv_output shape (?, 3, 3, 512)\n",
      "s3_2_conv_output: (?, 3, 3, 512)\n",
      "s3_2_short_cut: (?, 3, 3, 512)\n",
      "conv_output shape (?, 3, 3, 128)\n",
      "conv_output shape (?, 3, 3, 128)\n",
      "conv_output shape (?, 3, 3, 512)\n",
      "s3_3_conv_output: (?, 3, 3, 512)\n",
      "s3_3_short_cut: (?, 3, 3, 512)\n",
      "conv_output shape (?, 3, 3, 128)\n",
      "conv_output shape (?, 3, 3, 128)\n",
      "conv_output shape (?, 3, 3, 512)\n",
      "s3_4_conv_output: (?, 3, 3, 512)\n",
      "s3_4_short_cut: (?, 3, 3, 512)\n",
      "input to dense layer: (?, 3, 3, 512)\n",
      "input to dense layer: (?, 4608)\n",
      "prediction shape: (?, 10)\n",
      "Y shape: (?, 10)\n"
     ]
    }
   ],
   "source": [
    "resNet = ResNet(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, Minibatch Loss= 3.9124, Training Accuracy= 0.102\n",
      "batch 50, Minibatch Loss= 0.9122, Training Accuracy= 0.074\n",
      "batch 100, Minibatch Loss= 0.4972, Training Accuracy= 0.094\n",
      "batch 150, Minibatch Loss= 0.3825, Training Accuracy= 0.191\n",
      "batch 200, Minibatch Loss= 0.3823, Training Accuracy= 0.328\n",
      "batch 250, Minibatch Loss= 0.2498, Training Accuracy= 0.691\n",
      "batch 300, Minibatch Loss= 0.2710, Training Accuracy= 0.895\n",
      "batch 350, Minibatch Loss= 0.2500, Training Accuracy= 0.918\n",
      "batch 400, Minibatch Loss= 0.1698, Training Accuracy= 0.938\n",
      "batch 0, Minibatch Loss= 0.0922, Training Accuracy= 0.941\n",
      "batch 50, Minibatch Loss= 0.0946, Training Accuracy= 0.953\n",
      "batch 100, Minibatch Loss= 0.1208, Training Accuracy= 0.941\n",
      "batch 150, Minibatch Loss= 0.0478, Training Accuracy= 0.953\n",
      "batch 200, Minibatch Loss= 0.1751, Training Accuracy= 0.945\n",
      "batch 250, Minibatch Loss= 0.1487, Training Accuracy= 0.949\n",
      "batch 300, Minibatch Loss= 0.1572, Training Accuracy= 0.953\n",
      "batch 350, Minibatch Loss= 0.1936, Training Accuracy= 0.953\n",
      "batch 400, Minibatch Loss= 0.0715, Training Accuracy= 0.945\n",
      "batch 0, Minibatch Loss= 0.0203, Training Accuracy= 0.949\n",
      "batch 50, Minibatch Loss= 0.0269, Training Accuracy= 0.957\n",
      "batch 100, Minibatch Loss= 0.0558, Training Accuracy= 0.953\n",
      "batch 150, Minibatch Loss= 0.0083, Training Accuracy= 0.957\n",
      "batch 200, Minibatch Loss= 0.0134, Training Accuracy= 0.961\n",
      "batch 250, Minibatch Loss= 0.0681, Training Accuracy= 0.957\n",
      "batch 300, Minibatch Loss= 0.0571, Training Accuracy= 0.957\n",
      "batch 350, Minibatch Loss= 0.0141, Training Accuracy= 0.961\n",
      "batch 400, Minibatch Loss= 0.0068, Training Accuracy= 0.961\n",
      "Testing Accuracy: 0.98828125\n"
     ]
    }
   ],
   "source": [
    "resNet.train(epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
