{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Why initialize weights randomly like:\n",
    "> np.random.randn(D) / np.sqrt(D)\n",
    "* Read the paper [\"Efficient BackProp\"](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initialization is important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing to 0 (or constant)\n",
    "\n",
    "* Common to initialize weights to 0 in linear models (e.g., linear regression) and it works fine\n",
    "* But not work in neural network, why?\n",
    "\n",
    "\n",
    "\n",
    "* If weights are initialized to be constant, all units calculate the same feature, it is like having only 1 unit in that layer. In other words, adding more units will not increase the expressiveness of the neural network.\n",
    "\n",
    "> Initializing weights randomly allows us to break this symmetry and also allows us to make use of all the hitting units in the neural network\n",
    "\n",
    "* Now we know that we would be better initialize weights randomly. Then, we want to know what distribution should they come from and what are the parameters of this distribution\n",
    "* Before answering this question, we first look at vanishing gradients and exploring gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradients\n",
    "\n",
    "* When it comes to neural neural networks, one premise is that deeper is better\n",
    "    * Researchers have found that by adding more layers, the nerual network can have less hitting units per layer and achieve better performance (add paper)\n",
    "* Researchers believe that the sigmoid (\"S\" shape) activation function was the best possible possible activation function. This could be due to the fact that sigmoid fuction has nice derivatives. \n",
    "    * for sigmoind, the derivative is: \n",
    "    \n",
    "    $$ output * (1 - output) $$\n",
    "    \n",
    "    * for tanh, the derivative is: \n",
    "    \n",
    "    $$ 1 - output * output $$\n",
    "    \n",
    "    \n",
    "* These functions are <b style=\"color:red\">smooth</b> meaning they are differentiable everywhere. The differentiability is important because the learning method is gradient descent and we can not do gradient descent if we can not take derivatives. \n",
    "\n",
    "**Problem with the sigmoid in deep network**\n",
    "* The neural nework has basic form: $ y = f(g(h(..p(x)..))) $, where $f, g, h$ all the way down to $p$ represent neural network layers/\n",
    "* Due to the chain rule of calculus, the derivative with respect to the weights at the first layer is calculated by multiplying the derivative at each layer that comes after that. \n",
    "$$ {dy \\over dw_1} = {df \\over dg} * {dg \\over dh} * ...$$\n",
    "* The max value of derivative of sigmoid function is 0.25, if we multiple 0.25 with large amound of times, the outcome would infinitly approach zero. This is when we are assuming that we are able to get the peak value of derivative of sigmoid at every layer. In normal scenarios, the derivative we get would be smaller than 0.25, in which case the derivatives are diminishing even faster. \n",
    "<img src=\"images/derivative_sigmoid.png\" alt=\"Drawing\" style=\"width:50%;height:50%\"/>\n",
    "* Therefore, by using sigmoid, a deep neural network would have a lots of derivatives very close to zero causing the neural network to learn very slowly. Because of this, the \"standard\" backpropagation is not a good way to train a very deep neural network.\n",
    "* We can address the limitation of standard backpropagation in many ways:\n",
    "    * <b style=\"color:red\">Greedy layer-wise unsupervised pre-training</b>, developed by Geoff Hinton's \n",
    "    * Or use <b style=\"color:red\">ReLu</b> activation function\n",
    "        * No pre-training required\n",
    "        * Training the whole neural network (sometimes called \"end-to-end training\") from scrach with backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploding Gradients\n",
    "\n",
    "* Happens when we multiply $ w * w * w * w * ...$, where $ |w| > 1 $\n",
    "* May happen in recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In order to avoid both vanishing gradient and exploding gradients, we need to initialize weights just right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to Initialize Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
