{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TensorFlow Graph and Session\n",
    "\n",
    "* TensorFlow’s api is built around the idea of a computational graph, a way of performing a mathematical process.\n",
    "\n",
    "> TensorFlow separates the defining of a computational graph from executing that computational graph by having them happen in separate places: a `graph` defines the operations, but the operations only happen within a `session`. Graphs and sessions are created independently. A graph is like a blueprint, and a session is like a construction site. Details refer to [Graphs and Sessions](https://www.tensorflow.org/versions/r1.3/programmers_guide/graphs)\n",
    "\n",
    "<img src=\"images/tf_graph.png\" alt=\"tf_graph\" style=\"width:60%;height:60%\"/>\n",
    "\n",
    "* TensorFlow graph has two general computation units: \n",
    "    * `Nodes`, represent mathematical operations\n",
    "    * `Edges`, represent tensors (multi-dimensional arrays) that hold values passing through the graph\n",
    "* Note that: when we are constructing the graph, values are not known and they are represented by either `variables` or `placeholders`\n",
    "* When we start running the computation on the constructed graph, we will feed values into the graph through the variables and placeholders.\n",
    "    * `Variables` store values that can be updated during computation and are initialized before graph computation You must explicitly call operation like `tf.global_variables_initializer` with session.run to initialize all variables. Typically, weights and biases are represented by variables.\n",
    "    * `Placeholders` store values that can not be updated during the computation and are provided by user (outside of the graph). Typically, external inputs such as training, testing samples and some hyperparameters are represented by placeholders.\n",
    "\n",
    "**Why Graph**\n",
    "* One obvious reason is that Neural Network is essentially a graph\n",
    "* More importantly, it makes taking partial derivative with respect to certain variable easier. \n",
    "    * For example, the partial derivative of E with respect to C is 1 and it does not depend on D. The edges of the graph tell us which way to calculate the derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> At this point TensorFlow has already started managing a lot of state for us. There's already an implicit default graph, for example. Internally, the default graph lives in the `_default_graph_stack`, but we don't have access to that directly. We use [`tf.get_default_graph()`](https://www.tensorflow.org/api_docs/python/tf/get_default_graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default graph:  <tensorflow.python.framework.ops.Graph object at 0x10fadfc50>\n",
      "all operations:  []\n"
     ]
    }
   ],
   "source": [
    "graph = tf.get_default_graph()\n",
    "print(\"default graph: \", graph)\n",
    "print(\"all operations: \", graph.get_operations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, there isn't anything in the graph. We’ll need to put everything we want TensorFlow to compute into that graph. Let's start with a simple constant input value of one.\n",
    "\n",
    "> TensorFlow uses protocol buffers internally. ([Protocol buffers](https://developers.google.com/protocol-buffers/) are sort of like a Google-strength JSON.) Printing the `node_def` for the constant operation above shows what's in TensorFlow's protocol buffer representation for the number one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all operations:  [<tf.Operation 'Const' type=Const>]\n",
      "first operations:  name: \"Const\"\n",
      "op: \"Const\"\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"value\"\n",
      "  value {\n",
      "    tensor {\n",
      "      dtype: DT_FLOAT\n",
      "      tensor_shape {\n",
      "      }\n",
      "      float_val: 1.0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_value = tf.constant(1.0)\n",
    "operations = graph.get_operations()\n",
    "print(\"all operations: \", operations)\n",
    "print(\"first operations: \", operations[0].node_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create an new graph by using [tf.Graph](https://www.tensorflow.org/api_docs/python/tf/Graph) and set it as the default graph by using `tf.Graph.as_default` context manager, which overrides the current default graph for the lifetime of the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "default_g = tf.get_default_graph()\n",
    "assert g is not default_g\n",
    "with g.as_default():\n",
    "  # Define operations and tensors in `g`.\n",
    "  c = tf.constant(30.0)\n",
    "  assert c.graph is g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why TensorFlow defines its own versions of objects such as variables instead of just using a normal Python variable? [One of the TensorFlow tutorials has an explanation](https://www.tensorflow.org/get_started/mnist/pros#deep-mnist-for-experts):\n",
    "> To do efficient numerical computing in Python, we typically use libraries like NumPy that do expensive operations such as matrix multiplication outside Python, using highly efficient code implemented in another language. Unfortunately, there can still be a lot of overhead from switching back to Python every operation. This overhead is especially bad if you want to run computations on GPUs or in a distributed manner, where there can be a high cost to transferring data.\n",
    "\n",
    "> TensorFlow also does its heavy lifting outside Python, but it takes things a step further to avoid this overhead. Instead of running a single expensive operation independently from Python, TensorFlow lets us describe a graph of interacting operations that run entirely outside Python. This approach is similar to that used in Theano or Torch.\n",
    "\n",
    "> The role of the Python code is therefore to build this external computation graph, and to dictate which parts of the computation graph should be run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described above, graph is like a blueprint that sketches the neural network architecture and it does not contains any numerical values so far. If we inspect our input_value, we see it is a constant 32-bit float tensor of no dimension: just one number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_2:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this doesn't tell us what that number is. To evaluate input_value and get a numerical value out, we need to create a “session” where graph operations can be evaluated and then explicitly ask to evaluate or “run” input_value. \n",
    "\n",
    "Note `the session picks up the default graph by default`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.Session()\n",
    "\n",
    "* A \"TensorFlow Session\" is an environment for running a graph. A graph only run computations after creation of a session. The session is in charge of allocating the operations to GPU(s) and/or CPU(s), including remote machines. \n",
    "* Why we need session?\n",
    "    * Session is kind of separating running a constructed tensorflow graph from the construction of that tensorflow graph. It will allocate necessary computational resources (be it a CPU, a GPU or multiple GPUs) via configuration when we actually running the graph\n",
    "* The following code creates a session instance `sess` using [`tf.Session`](https://www.tensorflow.org/api_docs/python/tf/Session). We defines a session within a `with` block. So, after running the with block, the session will close automatically.\n",
    "    * The `sess.run()` function then evaluates the tensor and returns the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# hello_constant = tf.constant('Hello World!')\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(input_value)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Side Note\n",
    "\n",
    "* Tensorflow has some syntax or boilerplate code, such as session, that has nothing to do with the machine learning or deep learning. They exist for the tensor flow to work. \n",
    "* Therefore, provided you well understand the fundamental components/concepts of machine learning and deep learning, you can work with any framework to construct even tremendous complex deep neural network.\n",
    "    * Data + Model + Model Assumption\n",
    "    * Loss function\n",
    "    * Optimizer that minimize loss with respect to model parameters\n",
    "    * Make predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TensorFlow Basics\n",
    "\n",
    "\n",
    "### What are Tensors?\n",
    "\n",
    "* Tensors are the standard way of representing data in TensorFlow.\n",
    "* Tensors are multidimensional arrays, an extension of two-dimensional tables (matrices) to data with higher dimension.\n",
    "    * Tensors with rank 0 is scalar\n",
    "    * Tensors with rank 1 is vector\n",
    "    * Tensors with rank 2 is matrix (table of numbers)\n",
    "    * Tensors with rank 3 is 3-tensor (cube of numbers)\n",
    "    * Tensors with rank 4 or higher, you image it\n",
    "\n",
    "### tf.placeholder()\n",
    "\n",
    "* You can’t just set your X variable to your training data and put it in TensorFlow, because over time you'll want your TensorFlow model to take in different training data with different parameters. You need `tf.placeholder()`\n",
    "\n",
    "* `tf.placeholder()` returns a tensor that gets its value from data passed to the `tf.session.run()` function through `feed_dict` argument, allowing you to set the input right before the session runs and optionally to constrain shape of input as well. .\n",
    "\n",
    "* [`tf.placeholder()`](https://www.tensorflow.org/api_docs/python/tf/placeholder) accepts parameters:\n",
    "    * `dtype`: The type of elements in the tensor to be fed.\n",
    "    * `shape`: The shape of the tensor to be fed (optional). If the shape is not specified, you can feed a tensor of any shape.\n",
    "    * `name`: A name for the operation (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"A_2:0\", shape=(5, 5), dtype=float32)\n",
      "Tensor(\"Placeholder_2:0\", dtype=float32)\n",
      "Tensor(\"MatMul_81:0\", shape=(5, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "A = tf.placeholder(tf.float32, shape=(5,5), name='A')\n",
    "print(A)\n",
    "v = tf.placeholder(tf.float32)\n",
    "print(v)\n",
    "w = tf.matmul(A, v)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Bias in TensorFlow\n",
    "\n",
    "The most common operation in neural networks is calculating the linear combination of inputs, weights, and biases. As a reminder, we can write the output of the linear operation as\n",
    "\n",
    "$$y = xW + b$$\n",
    "\n",
    "$W$ is a matrix of the weights connecting two layers. The output $y$, the input $x$ and the biases $b$ are all vectors.\n",
    "\n",
    "* The goal of training a neural network is to modify weights and biases to best predict the labels. \n",
    "\n",
    "\n",
    "> In order to use weights and bias, you'll need a Tensor that can be modified. This leaves out `tf.placeholder()` and `tf.constant()`, since those Tensors can't be modified. This is where <b style='color: red'>tf.Variable</b> class comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.Variable()\n",
    "\n",
    "> Internally, a `tf.Variable` stores a persistent tensor. Specific ops allow you to read and modify the values of this tensor. These modifications are visible across multiple tf.Sessions, so multiple workers can see the same values for a tf.Variable.\n",
    "\n",
    "> Variables are manipulated via the tf.Variable class. A tf.Variable represents a tensor whose value can be changed by running ops on it. Unlike tf.Tensor objects, a tf.Variable exists outside the context of a single session.run call.\n",
    "\n",
    "> Internally, a tf.Variable stores a persistent tensor. Specific ops allow you to read and modify the values of this tensor. These modifications are visible across multiple tf.Sessions, so multiple workers can see the same values for a tf.Variable.\n",
    "\n",
    "* The tf.Variable class creates a tensor with an initial value that can be modified, much like a normal Python variable. \n",
    "* This tensor stores its state in the session, so you must initialize the state of the tensor manually. You'll use the `tf.global_variables_initializer()` function to initialize the state of all the Variable tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- All operations at the very beginning --------------------\n",
      "------All operations after define a variable x ----------------\n",
      "Variable/initial_value\n",
      "Variable\n",
      "Variable/Assign\n",
      "Variable/read\n",
      "---------------------------------------------------------------\n",
      "5\n",
      "------All operations after session closed ----------------\n",
      "Variable/initial_value\n",
      "Variable\n",
      "Variable/Assign\n",
      "Variable/read\n",
      "init\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "print(\"----- All operations at the very beginning --------------------\")    \n",
    "for op in graph.get_operations(): print(op.name)\n",
    "    \n",
    "x = tf.Variable(5)\n",
    "\n",
    "print(\"------All operations after define a variable x ----------------\")    \n",
    "for op in graph.get_operations(): print(op.name)\n",
    "print(\"---------------------------------------------------------------\")  \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(x.eval())\n",
    "    \n",
    "print(\"------All operations after session closed ----------------\")    \n",
    "for op in graph.get_operations(): print(op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might expect that adding a variable (e.g., `x = tf.Variable(5)`) would add one operation to the graph, but in fact that one line adds four operations.\n",
    "\n",
    "> The `tf.global_variables_initializer()` call returns an operation that will initialize all TensorFlow variables from the graph. You call the operation using a session to initialize all the variables as shown above. Using the tf.Variable class allows us to change the weights and bias, but an initial value needs to be chosen.\n",
    "\n",
    "> Initializing the weights with random numbers from a normal distribution is good practice. Randomizing the weights helps the model from becoming stuck in the same place every time you train it. \n",
    "\n",
    "> Similarly, choosing weights from a normal distribution prevents any one weight from overwhelming other weights. You'll use the `tf.truncated_normal()` function to generate random numbers from a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.truncated_normal()\n",
    "\n",
    "* The [`tf.truncated_normal()`](https://www.tensorflow.org/api_docs/python/tf/truncated_normal) function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean.\n",
    "\n",
    "```\n",
    "truncated_normal(\n",
    "    shape,\n",
    "    mean=0.0,\n",
    "    stddev=1.0,\n",
    "    dtype=tf.float32,\n",
    "    seed=None,\n",
    "    name=None\n",
    ")\n",
    "```\n",
    "\n",
    "* Since the weights are already helping prevent the model from getting stuck, you don't need to randomize the bias. Let's use the simplest solution, setting the bias to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_175:0' shape=(120, 5) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "n_features = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f.zeros()\n",
    "* The tf.zeros() function returns a tensor with all zeros.\n",
    "* Many other similar functions, refer to [document](https://www.tensorflow.org/api_guides/python/constant_op#truncated_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_1:0' shape=(5,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "n_labels = 5\n",
    "bias = tf.Variable(tf.zeros(n_labels))\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session’s feed_dict\n",
    "\n",
    "* Use the feed_dict parameter in <b>tf.session.run()</b> to set the placeholder tensor\n",
    "* <b>Note</b>: If the data passed to the feed_dict doesn’t match the tensor type and can’t be cast into the tensor type, you’ll get the error “ValueError: invalid literal for...”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.89380491]\n",
      " [ 0.76145381]\n",
      " [ 0.35286978]\n",
      " [-0.47771859]\n",
      " [ 0.92436922]] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    output = session.run(w, feed_dict={A: np.random.randn(5,5), v:np.random.randn(5,1)})\n",
    "    print(output, type(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> tensor flow does real matrix multiplication. Therefore, when you input a vector, you have to specify all dimensions\n",
    "\n",
    "```\n",
    "v:np.random.randn(5,1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Math\n",
    "\n",
    "Getting the input is great, but now you need to use it. You're going to use basic math functions that everyone knows and loves - add, subtract, multiply, and divide - with tensors. (There's many more math functions you can check out in the [documentation](https://www.tensorflow.org/api_guides/python/math_ops).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition\n",
    "\n",
    "We’ll start with the add function. The `tf.add()` function does exactly what you expect it to do. It takes in two numbers, two tensors, or one of each, and returns their sum as a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add\n",
      "Tensor(\"Variable_3/read:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Const:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "weight = tf.Variable(5)\n",
    "input_value = tf.constant(4)\n",
    "z = tf.add(weight, input_value)\n",
    "\n",
    "# This shows how the multiplication operation tracks \n",
    "# where its inputs come from: they come from other \n",
    "# operations in the graph; one is a variable and the\n",
    "# other is a constant\n",
    "op = graph.get_operations()[-1]\n",
    "print(op.name)\n",
    "for op_input in op.inputs: print(op_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtraction and Multiplication\n",
    "Here’s an example with subtraction and multiplication.\n",
    "\n",
    "```python\n",
    "x = tf.subtract(10, 4) # 6\n",
    "y = tf.multiply(2, 5)  # 10\n",
    "```\n",
    "\n",
    "The x tensor will evaluate to 6, because 10 - 4 = 6. The y tensor will evaluate to 10, because 2 * 5 = 10. That was easy!\n",
    "\n",
    "### Converting types\n",
    "\n",
    "It may be necessary to convert between types to make certain operators work together. For example, if you tried the following, it would fail with an exception:\n",
    "\n",
    "```python\n",
    "# Fails with ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32:\n",
    "tf.subtract(tf.constant(2.0),tf.constant(1)) \n",
    "```\n",
    "\n",
    "That's because the constant 1 is an integer but the constant 2.0 is a floating point value and subtract expects them to match.\n",
    "\n",
    "In cases like these, you can either make sure your data is all of the same type, or you can cast a value to another type. In this case, converting the 2.0 to an integer before subtracting, like so, will give the correct result:\n",
    "\n",
    "```python\n",
    "tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))   # 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Quiz** The code below is a simple algorithm using division and subtraction. Convert the following algorithm in regular Python to TensorFlow and print the results of the session. You can use tf.constant() for the values 10, 2, and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "# Convert the following to TensorFlow:\n",
    "# x = 10\n",
    "# y = 2\n",
    "# z = x/y - 1\n",
    "\n",
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "z = tf.subtract(tf.divide(x,y),tf.cast(tf.constant(1), tf.float64))\n",
    "\n",
    "# Print z from a session\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Softmax\n",
    "\n",
    "> The softmax function squashes it's inputs, typically called <b style='color: red'>logits</b> or logit scores, to be between 0 and 1 and also normalizes the outputs such that they all sum up to 1. \n",
    "\n",
    ">This means the output of the softmax function is equivalent to a categorical probability distribution. It's the perfect function to use as the output activation for a network predicting multiple classes.\n",
    "\n",
    "<img src=\"images/softmax.png\" alt=\"Drawing\" style=\"width:60%;height:60%\"/>\n",
    "\n",
    "* We're using TensorFlow to build neural networks and, appropriately, there's a function for calculating softmax.\n",
    "* <b style='color: red'>tf.nn.softmax()</b> implements the softmax function for you. It takes in logits and returns softmax activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.45659032  0.3382504   0.20515925]\n"
     ]
    }
   ],
   "source": [
    "def compute_softmax():\n",
    "    output = None\n",
    "    logit_data = [1.2, 0.9, 0.4]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Calculate the softmax of the logits\n",
    "    softmax = tf.nn.softmax(logits)    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Feed in the logit data\n",
    "        output = sess.run(softmax,   feed_dict={logits: logit_data} )\n",
    "\n",
    "    return output\n",
    "\n",
    "print(compute_softmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Cross Entropy\n",
    "\n",
    "When you're using softmax, your output is a vector that contains probability distribution over output logits. You can express your data labels as a vector using what's called `one-hot encoding`.\n",
    "\n",
    "`one-hot encoding` means that you have a vector the length of which is the number of classes, and the target label is marked with a 1 while the other labels are set to 0. In the case of classifying digits, our label vector for the image of the number 4 would be:\n",
    "\n",
    "y = [0,0,0,0,1,0,0,0,0,0]\n",
    "\n",
    "And our output prediction vector could be something like:\n",
    "\n",
    "$\\hat{y}$ = [0.047,0.048,0.061,0.07,0.330,0.062,0.001,0.213,0.013,0.150].\n",
    "\n",
    "\n",
    "We want our error to be proportional to how far apart these two vectors are. To calculate this distance, we'll use the `cross entropy`. Then, our goal while training the network is to make our prediction vectors as close as possible to the label vectors by minimizing the cross entropy. The cross entropy calculation is shown below:\n",
    "\n",
    "<img src=\"images/cross_entropy.png\" alt=\"Drawing\" style=\"width:60%;height:60%\"/>\n",
    "\n",
    "As you can see above, the cross entropy is the sum of the label vector times the natural log of the prediction vector. Note that this formula is not symmetric! Flipping the vectors is a bad idea because the label vector has a lot of zeros and taking the log of zero will cause an error.\n",
    "\n",
    "What's cool about using one-hot encoding for the label vector is that $y_j$ is 0 except for the one true class. Then, all terms in that sum except for where $y_j=1$ are zero and the cross entropy is simply $D=−ln(\\hat{y})$ for the true label. For example, if your input image is of the digit 4 and it's labeled 4, then only the output of the unit corresponding to 4 matters in the cross entropy cost.\n",
    "\n",
    "* As with the softmax function, TensorFlow has a function to do the cross entropy calculations for us.\n",
    "* To create a cross entropy function in TensorFlow, you'll need to use two new functions:\n",
    "    * <b style='color: red'>tf.reduce_sum()</b>, takes an array of numbers and sums them together.\n",
    "    * <b style='color: red'>tf.log()</b>, takes the natural log of a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.78396875, -1.08396883, -1.58396877])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.reduce_sum([1, 2, 3, 4, 5])  # 15\n",
    "x = tf.log(100.0)  # 4.60517"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "0.783969\n"
     ]
    }
   ],
   "source": [
    "# probability distribution vector\n",
    "prod_dist_data = np.array([0.45659032, 0.3382504, 0.20515925])\n",
    "# one hot vector\n",
    "one_hot_data = np.array([1.0, 0.0, 0.0])\n",
    "\n",
    "print(prod_dist_data.shape)\n",
    "\n",
    "prod_dist = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(prod_dist)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(cross_entropy, feed_dict={prod_dist:prod_dist_data, one_hot:one_hot_data})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow provides a function computing the the cross entropy called [<b style='color: red'>tf.nn.softmax_cross_entropy_with_logits()</b>](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "\n",
    "* It computes softmax cross entropy between logits and labels.\n",
    "\n",
    "* Measures the probability error in discrete classification tasks in which the classes are mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is labeled with one and only one label: an image can be a dog or a truck, but not both.\n",
    "\n",
    "> While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution. If they are not, the computation of the gradient will be incorrect.\n",
    "\n",
    "> If using exclusive labels (wherein one and only one class is true at a time), see sparse_softmax_cross_entropy_with_logits.\n",
    "\n",
    "* Other important points\n",
    "\n",
    "> This operation expects **unscaled logits**, since it performs a softmax on logits internally for efficiency. Do not call this operation with the output of softmax, as it will produce incorrect results.\n",
    "\n",
    "> `logits` and `labels` must have the same shape, e.g. `[batch_size, num_classes]` and the same dtype (either float16, float32, or float64).\n",
    "\n",
    "> To avoid confusion, it is required to pass only named arguments to this function.\n",
    "\n",
    "The most important arguments for this function are:\n",
    "* `labels`: Each row labels[i] must be a valid probability distribution.\n",
    "* `logits`: Unscaled log probabilities.\n",
    "* `dim`: The class dimension. Defaulted to -1 which is the last dimension.\n",
    "\n",
    "It returns a **1-D tensor of length `batch_size`** of the same type as logits with the softmax cross entropy loss.\n",
    "\n",
    "Backpropagation in this version of softmax cross entropy will happen only into logits. To calculate a cross entropy loss that allows backpropagation into both logits and labels, see [tf.nn.softmax_cross_entropy_with_logits_v2](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit shape (3, 3)\n",
      "one hot shape (3, 3)\n",
      "cross entropy: [0.78396875 0.7679496  1.2729189 ]\n",
      "cross entropy mean 0.9416124\n"
     ]
    }
   ],
   "source": [
    "logit = np.array([[1.2, 0.9, 0.4],[0.8, 0.9, 1.4],[1.2, 0.9, 1.1]])\n",
    "one_hot = np.array([[1.0, 0.0, 0.0],[0, 0, 1.0],[0, 1.0, 0]])\n",
    "\n",
    "print('logit shape', logit.shape)\n",
    "print('one hot shape', one_hot.shape)\n",
    "\n",
    "logit_placeholder = tf.placeholder(tf.float32)\n",
    "one_hot_placeholder = tf.placeholder(tf.float32)\n",
    "\n",
    "# NOTE: you should take the tf.log() of the softmax\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit_placeholder, labels=one_hot_placeholder)\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(cross_entropy, feed_dict={logit_placeholder:logit, one_hot_placeholder:one_hot})\n",
    "    print('cross entropy:', output)\n",
    "    \n",
    "# tf.nn.softmax_cross_entropy_with_logits returns a 1-D tensor of length batch_size of the same type \n",
    "# as logits. \n",
    "# Each element in the returned tensor is a cross entropy for a logit with its corresponding one hot vector. \n",
    "# To compute the average cross entropy of all training examples, we use tf.reduce_mean function.\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit_placeholder, \n",
    "                                                                       labels=one_hot_placeholder))\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(cross_entropy, feed_dict={logit_placeholder:logit, one_hot_placeholder:one_hot})\n",
    "    print('cross entropy mean', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[tf.reduce_mean](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/reduce_mean) computes the mean of elements across specified dimensions of a tensor.\n",
    "\n",
    "> Reduces input_tensor along the dimensions given in axis. Unless keep_dims is true, the rank of the tensor is reduced by 1 for each entry in axis. If keep_dims is true, the reduced dimensions are retained with length 1.\n",
    "\n",
    "> If axis has no entries, all dimensions are reduced, and a tensor with a single element is returned\n",
    "\n",
    "The most important arguments for this function:\n",
    "* `input_tensor`: The tensor to reduce. Should have numeric type.\n",
    "* `axis`: The dimensions to reduce. If None (the default), reduces all dimensions.\n",
    "* `keep_dims`: If true, retains reduced dimensions with length 1.\n",
    "\n",
    "It returns the reduced tensor.\n",
    "\n",
    "As shown in the code, we are using `tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))` to compute the loss of all the training examples in a mini-batch. \n",
    "* `tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)` computes a 1-D tensor in which each element is the softmax cross entropy value for one training example and there are totally `batch_size` of them.\n",
    "* Then, the `tf.reduce_mean` computes the mean of the `batch_size` number of softmax cross entropy values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow ReLU\n",
    "\n",
    "* ReLU, a non-linear activation function, or rectified linear unit. The ReLU function is 0 for negative inputs and x for all inputs x>0.\n",
    "* TensorFlow provides the ReLU function as <b style='color: red'>tf.nn.relu()</b>, as shown below.\n",
    "\n",
    "```python\n",
    "# Hidden Layer with ReLU activation function\n",
    "hidden_layer = tf.add(tf.matmul(features, hidden_weights), hidden_biases)\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "output = tf.add(tf.matmul(hidden_layer, output_weights), output_biases\n",
    "```\n",
    "\n",
    "The above code applies the tf.nn.relu() function to the hidden_layer, effectively turning off any negative weights and acting like an on/off switch. Adding additional layers, like the output layer, after an activation function turns the model into a nonlinear function. This nonlinearity allows the network to solve more complex problems.\n",
    "\n",
    "* Below you'll use the ReLU function to turn a linear single layer network into a non-linear multilayer network.\n",
    "<img src=\"images/two_layer_nn.png\" alt=\"Drawing\" style=\"width:60%;height:60%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.11000013   8.44000053]\n",
      " [  0.           0.        ]\n",
      " [ 24.01000214  38.23999786]]\n"
     ]
    }
   ],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "output = None\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# forward pass\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "# Print session results\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Gradient Descent Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this section, we will use [`tf.train.GradientDescentOptimizer()`](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer) to compute the minimal value of a simple convex function. \n",
    "\n",
    "$$u^2 + u + 1$$\n",
    "\n",
    "* You can refer to this [document](https://www.tensorflow.org/api_guides/python/train) to check many other optimizers.\n",
    "    * [`tf.train.GradientDescentOptimizer()`](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer)\n",
    "```python\n",
    "__init__(\n",
    "    learning_rate,\n",
    "    use_locking=False,\n",
    "    name='GradientDescent'\n",
    ")\n",
    "```\n",
    "    * [`tf.train.AdamOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)\n",
    "```python\n",
    "__init__(\n",
    "    learning_rate=0.001,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    epsilon=1e-08,\n",
    "    use_locking=False,\n",
    "    name='Adam'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "u = tf.Variable(20.0)\n",
    "cost = u*u + u + 1\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"GradientDescent_1\"\n",
      "op: \"NoOp\"\n",
      "input: \"^GradientDescent_1/update_Variable_3/ApplyGradientDescent\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 0.3 is the learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.3)\n",
    "train_op = optimizer.minimize(cost)\n",
    "print(train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0, cost 67.990, u = 7.700\n",
      "i = 1, cost 11.508, u = 2.780\n",
      "i = 2, cost 2.471, u = 0.812\n",
      "i = 3, cost 1.025, u = 0.025\n",
      "i = 4, cost 0.794, u = -0.290\n",
      "i = 5, cost 0.757, u = -0.416\n",
      "i = 6, cost 0.751, u = -0.466\n",
      "i = 7, cost 0.750, u = -0.487\n",
      "i = 8, cost 0.750, u = -0.495\n",
      "i = 9, cost 0.750, u = -0.498\n",
      "i = 10, cost 0.750, u = -0.499\n",
      "i = 11, cost 0.750, u = -0.500\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    for i in range(12):\n",
    "        session.run(train_op)\n",
    "        print(\"i = %d, cost %.3f, u = %.3f\" % (i , cost.eval(), u.eval()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `minimize(cost)` function of the optimizer automatically (1) computes gradients through a whole network and (2) applies gradients to variables (i.e., carrying out the backward step for learning).\n",
    "* This method simply combines calls `compute_gradients()` and `apply_gradients()`. If you want to process the gradient before applying them call compute_gradients() and apply_gradients() explicitly instead of using this function.\n",
    "\n",
    "**compute_gradients()**\n",
    "\n",
    "```python\n",
    "compute_gradients(\n",
    "    loss,\n",
    "    var_list=None,\n",
    "    gate_gradients=GATE_OP,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    grad_loss=None\n",
    ")\n",
    "```\n",
    "\n",
    "* Compute gradients of loss for the variables in var_list.\n",
    "* This is the first part of minimize(). It returns a list of (gradient, variable) pairs where \"gradient\" is the gradient for \"variable\". Note that \"gradient\" can be a Tensor, an IndexedSlices, or None if there is no gradient for the given variable.\n",
    "* Args:\n",
    "    * `loss`: A Tensor containing the value to minimize.\n",
    "    * `var_list`: Optional list or tuple of tf.Variable to update to minimize loss. Defaults to the list of variables collected in the graph under the key `GraphKey.TRAINABLE_VARIABLES`.\n",
    "    * `gate_gradients`: How to gate the computation of gradients. Can be GATE_NONE, GATE_OP, or GATE_GRAPH.\n",
    "    aggregation_method: Specifies the method used to combine gradient terms. Valid values are defined in the class AggregationMethod.\n",
    "    * `colocate_gradients_with_ops`: If True, try colocating gradients with the corresponding op.\n",
    "    * `grad_loss`: Optional. A Tensor holding the gradient computed for loss.\n",
    "\n",
    "Returns:\n",
    "A list of (gradient, variable) pairs. Variable is always present, but gradient can be None.\n",
    "\n",
    "**apply_gradients()**\n",
    "\n",
    "```python\n",
    "apply_gradients(\n",
    "    grads_and_vars,\n",
    "    global_step=None,\n",
    "    name=None\n",
    ")\n",
    "```\n",
    "\n",
    "* Apply gradients to variables.\n",
    "* This is the second part of minimize(). It returns an Operation that applies gradients.\n",
    "* Args:\n",
    "    * `grads_and_vars`: List of (gradient, variable) pairs as returned by compute_gradients().\n",
    "    * `global_step`: Optional Variable to increment by one after the variables have been updated.\n",
    "    * `name`: Optional name for the returned operation. Default to the name passed to the Optimizer constructor.\n",
    "\n",
    "Returns:\n",
    "An Operation that applies the specified gradients. If global_step was not None, that operation also increments global_step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0, cost 67.990, u = 7.700\n",
      "i = 1, cost 11.508, u = 2.780\n",
      "i = 2, cost 2.471, u = 0.812\n",
      "i = 3, cost 1.025, u = 0.025\n",
      "i = 4, cost 0.794, u = -0.290\n",
      "i = 5, cost 0.757, u = -0.416\n",
      "i = 6, cost 0.751, u = -0.466\n",
      "i = 7, cost 0.750, u = -0.487\n",
      "i = 8, cost 0.750, u = -0.495\n",
      "i = 9, cost 0.750, u = -0.498\n",
      "i = 10, cost 0.750, u = -0.499\n",
      "i = 11, cost 0.750, u = -0.500\n"
     ]
    }
   ],
   "source": [
    "u = tf.Variable(20.0)\n",
    "cost = u*u + u + 1\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.3)\n",
    "grads_and_vars = optimizer.compute_gradients(cost)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    for i in range(12):\n",
    "        session.run(train_op)\n",
    "        print(\"i = %d, cost %.3f, u = %.3f\" % (i , cost.eval(), u.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]]]\n",
      "<class 'list'>\n",
      "A (2, 3, 3)\n",
      "A Tensor(\"Const_17:0\", shape=(2, 3, 3), dtype=int32)\n",
      "C [[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]\n",
      " [13 14 15]\n",
      " [16 17 18]]\n",
      "C shape (6, 3)\n",
      "B [[ 7  8  9]\n",
      " [16 17 18]]\n",
      "B shape (2, 3)\n"
     ]
    }
   ],
   "source": [
    "a=list([[[1, 2, 3],\n",
    "            [4, 5, 6],\n",
    "            [7, 8, 9]],\n",
    "           [[10,11,12],\n",
    "            [13,14,15],\n",
    "            [16,17,18]]])\n",
    "print(a)\n",
    "print(type(a))\n",
    "A = tf.convert_to_tensor(a)\n",
    "# A = tf.constant(a)\n",
    "# B = tf.constant(b)\n",
    "B = A[:, -1]\n",
    "print(\"A\", A.shape)\n",
    "print(\"A\", A)\n",
    "concatenated=tf.concat(A, axis=1) \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    concatenated = tf.reshape(concatenated, [-1, 3])\n",
    "    C, B = sess.run([concatenated, B])\n",
    "    print(\"C\", C)\n",
    "    print(\"C shape\", C.shape)\n",
    "    print(\"B\", B)\n",
    "    print(\"B shape\", B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between tf.contrib.layers.fully_connected and tf.layers.dense\n",
    "\n",
    "They are essentially the same, [tf.contrib.layers.fully_connected](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected) calls [tf.layers.dense](https://www.tensorflow.org/api_docs/python/tf/layers/dense)\n",
    "Layers in tf.layers (and in tf.contrib.layers) are part of the \"higher-level\" API of tensorflow that takes care of initializing weights and biases. However you can choose an initializer for them.\n",
    "\n",
    "One [major difference](https://stackoverflow.com/questions/44912297/are-tf-layers-dense-and-tf-contrib-layers-fully-connected-interchangeable) is tf.contrib.fully_connected has relu as it's default activation, while tf.layers.dense is a linear activation by default\n",
    "\n",
    "```\n",
    "tf.layers.dense\n",
    "dense(\n",
    "    inputs,\n",
    "    units,\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer=None,\n",
    "    bias_initializer=tf.zeros_initializer(),\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    trainable=True,\n",
    "    name=None,\n",
    "    reuse=None\n",
    ")\n",
    "\n",
    "tf.contrib.layers.fully_connected\n",
    "fully_connected(\n",
    "    inputs,\n",
    "    num_outputs,\n",
    "    activation_fn=tf.nn.relu,\n",
    "    normalizer_fn=None,\n",
    "    normalizer_params=None,\n",
    "    weights_initializer=initializers.xavier_initializer(),\n",
    "    weights_regularizer=None,\n",
    "    biases_initializer=tf.zeros_initializer(),\n",
    "    biases_regularizer=None,\n",
    "    reuse=None,\n",
    "    variables_collections=None,\n",
    "    outputs_collections=None,\n",
    "    trainable=True,\n",
    "    scope=None\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "* `inputs`: A tensor of at least rank 2 and static value for the last dimension; i.e. [batch_size, depth], depth can be the number of features of an example; [None, None, None, channels].\n",
    "* `units/num_outputs`: Integer or long, the number of output units in the layer.\n",
    "* `activation_fn`: Activation function. The default value is a ReLU function. Explicitly set it to None to skip it and maintain a linear activation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may have different ways to define the fully connected output layer.\n",
    "\n",
    "```python\n",
    "## 1. \n",
    "# (1) We first define a fully-connected layer that only performs \n",
    "#     linear transformation of x since we set activation=None.\n",
    "# (2) Then, we define the leaky relu as activation function\n",
    "h = tf.layers.dense(x, n_units, activation=None)\n",
    "h = tf.maximum(alpha * h1, h1)\n",
    "\n",
    "## 2.\n",
    "# (1) We first reshape the tensor from the output of a LSTM network \n",
    "#     to the tensor with shape accepted by the fully-connected layer we will define next.\n",
    "# (2) Then, we define a dense (i.e., fully-connected layer) that only performs \n",
    "#     linear transformation of x since we set activation=None.\n",
    "# (3) Finally, we use tf.nn.sigmoid_cross_entropy_with_logits to compute the cost.\n",
    "# (4) Note that tf.nn.sigmoid_cross_entropy_with_logits will compute sigmoid of the inputted logits \n",
    "#     Therefore, we should not compute sigmoid on logits by ourselves.\n",
    "last_layer_input = reshape(lstm_output)\n",
    "logits = tf.layers.dense(last_layer_input, 1, activation=None)\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits (logits=logits, labels=labels))\n",
    "\n",
    "\n",
    "## 3.\n",
    "# (1) We first reshape the tensor from the output of a LSTM network \n",
    "#     to the tensor with shape accepted by the fully-connected layer we will define next.\n",
    "# (2) Then, we define a fully-connected layer with sigmoid activation function.\n",
    "# (3) Finally, we use tf.losses.mean_squared_error to compute the cost.\n",
    "# (4) Note that tf.losses.mean_squared_error will not compute sigmoid of the inputted logits \n",
    "#     Therefore, we should compute sigmoid on logits by ourselves.\n",
    "last_layer_input = reshape(lstm_output)\n",
    "predictions = tf.contrib.layers.fully_connected(last_layer_input , 1, activation_fn=tf.sigmoid)\n",
    "cost = tf.losses.mean_squared_error(labels, predictions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[document](https://jhui.github.io/2017/03/08/TensorFlow-variable-sharing/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between tf.Variable and tf.get_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) will always create a new variable and thus `tf.Variable` requires that an `initial_value` be specified. \n",
    "\n",
    "```\n",
    "__init__(\n",
    "    initial_value=None,\n",
    "    trainable=True,\n",
    "    collections=None,\n",
    "    validate_shape=True,\n",
    "    caching_device=None,\n",
    "    name=None,\n",
    "    variable_def=None,\n",
    "    dtype=None,\n",
    "    expected_shape=None,\n",
    "    import_scope=None,\n",
    "    constraint=None\n",
    ")\n",
    "```\n",
    "```\n",
    "W = tf.Variable(<initial-value>, name=<optional-name>)\n",
    "```\n",
    "\n",
    "*  [`tf.get_variable`](https://www.tensorflow.org/api_docs/python/tf/get_variable) gets from the graph an existing variable or creates a new one if it does not exists. (It does not require an initial value as using tf.Variable).\n",
    "    * This function requires you to specify the Variable's name. \n",
    "        * This name will be used by other replicas to access the same variable, as well as to name this variable's value when checkpointing and exporting models. \n",
    "    * To create a new variable by using `tf.get_variable`, you have to specify the `shape` of the input in addition to the variable name. The `tf.get_variable` will automatically initialize the variable with the specified shape by using an `initializer`\n",
    "        * If initializer is None (the default), the default initializer passed in the `variable scope` will be used. If that one is None too, a `glorot_uniform_initializer` will be used.\n",
    "        * When the initializer is a `tf.Tensor` you should not specify the variable's shape, as the shape of the initializer tensor will be used.\n",
    "\n",
    "```\n",
    "get_variable(\n",
    "    name,\n",
    "    shape=None,\n",
    "    dtype=None,\n",
    "    initializer=None,\n",
    "    regularizer=None,\n",
    "    trainable=True,\n",
    "    collections=None,\n",
    "    caching_device=None,\n",
    "    partitioner=None,\n",
    "    validate_shape=True,\n",
    "    use_resource=None,\n",
    "    custom_getter=None,\n",
    "    constraint=None\n",
    ")\n",
    "```\n",
    "```\n",
    "W = tf.get_variable(\"W\", shape=[784, 256],\n",
    "       initializer=tf.contrib.layers.xavier_initializer())\n",
    "```     \n",
    "       \n",
    "It would be better using `tf.get_variable()` since it will make it easier to refactor your code if you need to share variables at any time, e.g. in a multi-gpu setting (see the multi-gpu CIFAR example). There is no downside to it.\n",
    "\n",
    "> An new variable is added to the graph collections listed in collections, which defaults to `GraphKeys.GLOBAL_VARIABLES`.\n",
    "> If trainable is True the variable is also added to the graph collection `GraphKeys.TRAINABLE_VARIABLES`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a <tf.Variable 'one/v:0' shape=(1,) dtype=float32_ref>\n",
      "a2 <tf.Variable 'one/v2:0' shape=(1,) dtype=float32_ref>\n",
      "c <tf.Variable 'one/v:0' shape=(1,) dtype=float32_ref>\n",
      "d <tf.Variable 'two/v:0' shape=(1,) dtype=float32_ref>\n",
      "e <tf.Variable 'two/v_1:0' shape=() dtype=int32_ref>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-ad182d3f353c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#Assertion is true, they refer to the same object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#AssertionError: they are different objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#AssertionError: they are different objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"one\"):\n",
    "    a = tf.get_variable(\"v\", [1]) #a.name == \"one/v:0\"\n",
    "    print('a', a)\n",
    "    \n",
    "with tf.variable_scope(\"one\"):\n",
    "    a2 = tf.get_variable(\"v2\", shape=[1]) #ValueError: Variable one/v already exists\n",
    "    print('a2', a2)\n",
    "    \n",
    "# with tf.variable_scope(\"one\"):\n",
    "#     b = tf.get_variable(\"v\", [1]) #ValueError: Variable one/v already exists\n",
    "    \n",
    "with tf.variable_scope(\"one\", reuse = True):\n",
    "    c = tf.get_variable(\"v\", [1]) #c.name == \"one/v:0\"\n",
    "    print('c', c)\n",
    "\n",
    "with tf.variable_scope(\"two\"):\n",
    "    d = tf.get_variable(\"v\", [1]) #d.name == \"two/v:0\"\n",
    "    e = tf.Variable(1, name = \"v\", expected_shape = [1]) #e.name == \"two/v_1:0\"\n",
    "    print('d', d)\n",
    "    print('e', e)\n",
    "\n",
    "assert(a is c)  #Assertion is true, they refer to the same object.\n",
    "assert(a is d)  #AssertionError: they are different objects\n",
    "assert(d is e)  #AssertionError: they are different objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although two variables `d` and `e` have the same name 'v' under the same scope 'two', they are different variables. This is because `tf.Variable()` alway creates an new variable in the current graph. If current graph already contained an operation named \"two/v\", the TensorFlow would append \"_1\", \"_2\", and so on to the name, in order to make it unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two/v:0\n",
      "two/v_1:0\n"
     ]
    }
   ],
   "source": [
    "print(d.name)   #d.name == \"two/v:0\"\n",
    "print(e.name)   #e.name == \"two/v_1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [Hello tensorflow](https://www.oreilly.com/learning/hello-tensorflow)\n",
    "* [TensorFlow tutorials](https://www.tensorflow.org/tutorials/)\n",
    "* [TensorFlow programmers guide](https://www.tensorflow.org/programmers_guide/)\n",
    "* [TensorFlow get started](https://www.tensorflow.org/get_started/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
