{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class-based ANN with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "X, Y = getData()\n",
    "model = ANN([2000, 1000. 500])\n",
    "mode.fit(X, Y, show_fig=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create two classes:\n",
    "* Hidden layer class\n",
    "    * dimension of input and output \n",
    "    * weights and biases of this layer\n",
    "    * forward function, calculates forward of this layer\n",
    "    * (possibly) backpropagation function, calculates backpropagation of layer\n",
    "* Neural Network class\n",
    "    * a set of hidden layers\n",
    "    * fit function to train the network\n",
    "        * calculate forward through the whole network\n",
    "        * calculate total loss function\n",
    "        * optimize the total loss function w.r.t. parameters\n",
    "    * make predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight_and_bias(M_in, M_out):\n",
    "    W = np.random.randn(M_in, M_out) / np.sqrt(M_in)\n",
    "    b = np.zeros(M_out)\n",
    "    return W, b\n",
    "\n",
    "class HiddenLayer(object):\n",
    "    def __init__(self, M_in, M_out, an_id):\n",
    "        self.M_in = M_in\n",
    "        self.M_out = M_out\n",
    "        self.id = an_id\n",
    "        \n",
    "        W, b = init_weight_and_bias(M_in, K)\n",
    "        self.W = tf.Variable(W.astype(np.float32))\n",
    "        self.b = tf.Variable(b.astype(np.float32))\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return tf.nn.relu(tf.add(tf.matmul(X, self.W) + self.b))\n",
    "        \n",
    "    \n",
    "\n",
    "class ANN(object):\n",
    "    def __init__(self, hidden_layer_sizes):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "    \n",
    "    def fit(self, X, Y, learning_rate=10e-7, mu-0.99, decay-0.999, reg=10e-3, batch_sz=100, epochs=100):\n",
    "        \n",
    "        K = len(set(Y))\n",
    "        \n",
    "        X, Y = shuffle(X, Y)\n",
    "        X = X.astype(np.float32)\n",
    "        Y = y2indicator(Y, K).astype(np.float32)\n",
    "        \n",
    "        Xvalid, Yvalid = X[-1000:0], Y[-1000:0]\n",
    "        Yvalid_flat = np.argmax(Yvalid, axis=1)\n",
    "        X, Y = X[0:-1000], Y[0:-1000]\n",
    "        \n",
    "        N, D = X.shape\n",
    "            \n",
    "        # initialize hidden layers\n",
    "        M_in = D\n",
    "        id_seq = 0\n",
    "        self.hidden_layers = []\n",
    "        for M_out in self.hidden_layer_sizes:\n",
    "            h = HiddenLayer(M_in, M_out, id_seq)\n",
    "            self.hidden_layers.append(h)\n",
    "            M_in = M_out\n",
    "            id_seq += 1\n",
    "            \n",
    "    \n",
    "        W, b = init_weight_and_bias(M_in, K)\n",
    "        self.W = tf.Variable(W.astype(np.float32))\n",
    "        self.b = tf.Variable(W.astype(np.float32))\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "        # collect parameters for regularization\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += h.params\n",
    "               \n",
    "        # \n",
    "        X_P = tf.placeholder(tf.float32, shape=(None, D), name = 'X_P')\n",
    "        T_P = tf.placeholder(tf.float32, shape=(None, K), name = 'T_P')\n",
    "        \n",
    "        logit = forward(X_P)\n",
    "        \n",
    "        prediction = predict(X_P)\n",
    "        \n",
    "        # Calculate regularization loss\n",
    "        reg_loss = reg * sum([tf.nn.l2_loss(p) for p in self.params])\n",
    "        \n",
    "        # Calculate total loss\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=T_P)) + reg_loss\n",
    "        \n",
    "        # Create optimizer\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=decay, momentum=mu)\n",
    "        \n",
    "        # Minimize the loss with the optimizer\n",
    "        train_op = optimizer.minimize(loss)\n",
    "        \n",
    "        # To train the model within the Session\n",
    "        costs = []\n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session with sess:\n",
    "            sess.run(init)\n",
    "            \n",
    "            for itr in range(epochs):\n",
    "                int t = 0\n",
    "                for start_i in range(0, N, batch_sz):\n",
    "                    t += 1\n",
    "                    end_i = start_i + batch_sz\n",
    "                    sess.run(train_op, feed_dict={X_P:X[start_i:end_i], Y_P:Y[start_i:end_i]})\n",
    "                    if t % 20 == 0:\n",
    "                        c = sess.run(loss, feed_dict={X_P:Xvalid, Y_P:Yvalid})\n",
    "                        costs.append(c)\n",
    "                        \n",
    "                        p = sess.run(prediction, feed_dict={X_P:Xvalid})\n",
    "                        e = error_rate(Yvalid_flat, p)\n",
    "                        \n",
    "                        print(\"itr:\", itr, \"batch:\", t, \"cost:\", c, \"error rate:\", e)\n",
    "                \n",
    "            \n",
    "    def forward(self, X):\n",
    "        Z = X\n",
    "        for h in self.hidden_layers:\n",
    "            Z = h.forward(Z)\n",
    "        return tf.matmul(Z, self.W) + self.b \n",
    "\n",
    "    def predict(self, X):\n",
    "        logit = self.forward(X)\n",
    "        return tf.argmax(logit, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(balance_ones=True):\n",
    "    # images are 48x48 = 2304 size vectors\n",
    "    Y = []\n",
    "    X = []\n",
    "    first = True\n",
    "    for line in open('fer2013.csv'):\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            row = line.split(',')\n",
    "            Y.append(int(row[0]))\n",
    "            X.append([int(p) for p in row[1].split()])\n",
    "\n",
    "    X, Y = np.array(X) / 255.0, np.array(Y)\n",
    "\n",
    "    if balance_ones:\n",
    "        # balance the 1 class\n",
    "        X0, Y0 = X[Y!=1, :], Y[Y!=1]\n",
    "        X1 = X[Y==1, :]\n",
    "        X1 = np.repeat(X1, 9, axis=0)\n",
    "        X = np.vstack([X0, X1])\n",
    "        Y = np.concatenate((Y0, [1]*len(X1)))\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
