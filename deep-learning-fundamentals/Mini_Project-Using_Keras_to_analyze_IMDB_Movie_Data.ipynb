{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project: Using Keras to analyze IMDB Movie Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will analyze a dataset from IMDB and use it to predict the sentiment analysis of a review.\n",
    "\n",
    "Workspace\n",
    "To open this notebook, you have two options:\n",
    "\n",
    "* You may clone the repo https://github.com/udacity/deep-learning.git from Github and open the notebook IMDB_in_Keras.ipynb in the imdb_keras folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "In this lab, we will preprocess the data for you, and you'll be in charge of building and training the model in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "\n",
    "This lab uses a dataset of 25,000 [IMDB](http://www.imdb.com/) reviews. Each review, comes with a label. A label of 0 is given to a negative review, and a label of 1 is given to a positive review. The goal of this lab is to create a model that will predict the sentiment of a review, based on the words on it. You can see more information about this dataset in the [Keras Datasets](https://keras.io/datasets/) website.\n",
    "\n",
    "Now, the input already comes preprocessed for us for convenience. Each review is encoded as a sequence of indexes, corresponding to the words in the review. The words are ordered by frequency, so the integer 1 corresponds to the most frequent word (\"the\"), the integer 2 to the second most frequent word, etc. By convention, the integer 0 corresponds to unknown words.\n",
    "\n",
    "Then, the sentence is turned into a vector by simply concatenating these integers. For instance, if the sentence is \"To be or not to be.\" and the indices of the words are as follows:\n",
    "\n",
    "* \"to\": 5\n",
    "* \"be\": 8\n",
    "* \"or\": 21\n",
    "* \"not\": 3\n",
    "\n",
    "Then the sentence gets encoded as the vector [5,8,21,3,5,8]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "The data comes preloaded in Keras, which means we don't need to open or read any files manually. The command to load it is the following, which will actually split the words into training and testing sets and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                     num_words=1000,\n",
    "                                                     skip_top=0,\n",
    "                                                     maxlen=None,\n",
    "                                                     seed=113,\n",
    "                                                     start_char=1,\n",
    "                                                     oov_char=2,\n",
    "                                                     index_from=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning of all these arguments is [here](https://keras.io/datasets). But in a nutshell, the most important ones are:\n",
    "\n",
    "* `num_words`: Top most frequent words to consider. This is useful if you don't want to consider very obscure words such as \"Ultracrepidarian\"\n",
    "* `skip_top`: Top words to ignore. This is useful if you don't want to consider the most common words. For example, the word \"the\" would add no information to the review, so we can skip it by setting skip_top to 2 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "[1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "(25000,)\n",
      "[1, 89, 27, 2, 2, 17, 199, 132, 5, 2, 16, 2, 24, 8, 760, 4, 2, 7, 4, 22, 2, 2, 16, 2, 17, 2, 7, 2, 2, 9, 4, 2, 8, 14, 991, 13, 877, 38, 19, 27, 239, 13, 100, 235, 61, 483, 2, 4, 7, 4, 20, 131, 2, 72, 8, 14, 251, 27, 2, 7, 308, 16, 735, 2, 17, 29, 144, 28, 77, 2, 18, 12]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train[0])\n",
    "print(x_test.shape)\n",
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the data (One-hot encoding)\n",
    "\n",
    "We first prepare the data by one-hot encoding it into (0,1)-vectors as follows: If, for example, we have 10 words in our vocabulary, and the vector is (4,1,8), we'll turn it into the vector (1,0,0,1,0,0,0,1,0,0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding the output into vector mode, each of length 1000\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "x_train_e = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test_e = tokenizer.sequences_to_matrix(x_test, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1000)\n",
      "[[ 0.  1.  1. ...,  0.  0.  0.]\n",
      " [ 0.  1.  1. ...,  0.  0.  0.]\n",
      " [ 0.  1.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  1. ...,  0.  0.  0.]\n",
      " [ 0.  1.  1. ...,  0.  0.  0.]\n",
      " [ 0.  1.  1. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_e.shape)\n",
    "print(x_train_e[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll also one-hot encode the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding the output\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "\n",
    "Now it's your turn to use all you've learned! You can build a neural network using Keras, train it, and evaluate it! Make sure you also use methods such as dropout or regularization, and good Keras optimizers to do this. A good accuracy to aim for is 85%. Can your model achieve this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, activation='relu', input_dim=x_train_e.shape[1]))\n",
    "# rate: float between 0 and 1. Fraction of the input units to drop.\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(126, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(num_classes))\n",
    "\n",
    "# Add a sigmoid activation layer\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Compile the model using a loss function and an optimizer.\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Run the model here. Experiment with different batch_size, and number of epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 8s - loss: 0.3832 - acc: 0.8266     \n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 6s - loss: 0.3032 - acc: 0.8718     \n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 6s - loss: 0.2563 - acc: 0.8929     \n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 6s - loss: 0.1756 - acc: 0.9303     \n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 6s - loss: 0.1010 - acc: 0.9620     \n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 6s - loss: 0.0648 - acc: 0.9766     \n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 6s - loss: 0.0453 - acc: 0.9834     \n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 5s - loss: 0.0445 - acc: 0.9844     \n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 6s - loss: 0.0362 - acc: 0.9869     \n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 6s - loss: 0.0312 - acc: 0.9890     \n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 6s - loss: 0.0313 - acc: 0.9886     \n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 7s - loss: 0.0267 - acc: 0.9906     \n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 7s - loss: 0.0267 - acc: 0.9907     \n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 6s - loss: 0.0253 - acc: 0.9914     \n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 6s - loss: 0.0212 - acc: 0.9928     \n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 6s - loss: 0.0228 - acc: 0.9916     \n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 6s - loss: 0.0221 - acc: 0.9924     \n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 6s - loss: 0.0174 - acc: 0.9943     \n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 6s - loss: 0.0187 - acc: 0.9934     \n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 6s - loss: 0.0170 - acc: 0.9942     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x127204b38>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the model. Feel free to experiment with different batch sizes and number of epochs.\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=32,\n",
    "#           epochs=10,\n",
    "#           validation_data=(x_test, y_test), \n",
    "#           verbose=2)\n",
    "\n",
    "model.fit(x_train_e, y_train, batch_size=32, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.84336\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test_e, y_test, verbose=0)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
