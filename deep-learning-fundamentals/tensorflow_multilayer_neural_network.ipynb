{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Neural Network in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hidden Layer NN \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/two_layer_nn_2.png\" alt=\"Drawing\" style=\"width:60%;height:60%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n",
      "(55000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/tensorflow/mnist/input_data\", one_hot=True, reshape=False)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "print(train_features.shape)\n",
    "print(test_features.shape)\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep neural networks use multiple layers with each layer requiring it's own weight and bias. The 'hidden_layer' weight and bias is for the hidden layer. The 'out' weight and bias is for the output layer. If the neural network were deeper, there would be weights and biases for each additional layer.\n",
    "\n",
    "> The MNIST data is made up of 28px by 28px images with a single channel (i.e., one color). The <b style='color:red'>tf.reshape()</b> function above reshapes the 28px by 28px matrices in x into row vectors of 784px.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_4:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"Reshape_5:0\", shape=(?, 784), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "print(x)\n",
    "x_flat = tf.reshape(x, [-1, n_input])\n",
    "print(x_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 5\n",
    "batch_size = 128  # Decrease batch size if you don't have enough memory\n",
    "display_step = 1\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "n_hidden_layer = 256 # layer number of features\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\n",
    "hidden_layer_out = tf.nn.relu(hidden_layer)\n",
    "\n",
    "logits = tf.add(tf.matmul(hidden_layer_out, weights['out']), biases['out'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've seen the linear function  <b style='color:red'>tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])</b> before, also known as  <b style='color:red'>xw + b</b>. Combining linear functions together using a ReLU will give you a two layer network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1 i 0 182.768\n",
      "ep 1 i 60 113.544\n",
      "ep 1 i 120 89.2423\n",
      "ep 1 i 180 73.7277\n",
      "ep 1 i 240 62.1535\n",
      "ep 1 i 300 53.7206\n",
      "ep 1 i 360 47.0001\n",
      "ep 1 i 420 41.8398\n",
      "ep 2 i 0 41.1746\n",
      "ep 2 i 60 37.2542\n",
      "ep 2 i 120 34.0824\n",
      "ep 2 i 180 31.5073\n",
      "ep 2 i 240 29.3111\n",
      "ep 2 i 300 27.4517\n",
      "ep 2 i 360 25.9311\n",
      "ep 2 i 420 24.5025\n",
      "ep 3 i 0 24.3267\n",
      "ep 3 i 60 23.1412\n",
      "ep 3 i 120 22.1078\n",
      "ep 3 i 180 21.1699\n",
      "ep 3 i 240 20.3347\n",
      "ep 3 i 300 19.6054\n",
      "ep 3 i 360 18.9304\n",
      "ep 3 i 420 18.3114\n",
      "ep 4 i 0 18.2076\n",
      "ep 4 i 60 17.6211\n",
      "ep 4 i 120 17.0676\n",
      "ep 4 i 180 16.5983\n",
      "ep 4 i 240 16.1581\n",
      "ep 4 i 300 15.7754\n",
      "ep 4 i 360 15.3986\n",
      "ep 4 i 420 15.0465\n",
      "ep 5 i 0 14.9915\n",
      "ep 5 i 60 14.655\n",
      "ep 5 i 120 14.3724\n",
      "ep 5 i 180 14.0913\n",
      "ep 5 i 240 13.8349\n",
      "ep 5 i 300 13.5316\n",
      "ep 5 i 360 13.284\n",
      "ep 5 i 420 13.0559\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f46c940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "losses = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for ep in range(1, training_epochs+1):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "#             print(batch_x.shape)\n",
    "            sess.run(optimizer, feed_dict={x:batch_x, y:batch_y})\n",
    "            output = sess.run(loss, feed_dict={x:test_features, y:test_labels})\n",
    "            losses.append(output)\n",
    "            \n",
    "            if i % 60 == 0:\n",
    "                print(\"ep\", ep, 'i', i, output)\n",
    "            \n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST library in TensorFlow provides the ability to receive the dataset in batches. Calling the <b style='color:red'>mnist.train.next_batch()</b> function returns a subset of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Hidden Layer NN \n",
    "\n",
    "\n",
    "<img src=\"images/two_hidden_layer.png\" alt=\"Drawing\" style=\"width:60%;height:60%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    Y=[]\n",
    "    X=[]\n",
    "    first=True\n",
    "    for line in open('../data/fer2013/fer2013.csv'):\n",
    "        if first:\n",
    "            first=False\n",
    "        else:\n",
    "            row=line.split(',')\n",
    "            y = int(row[0])\n",
    "\n",
    "            Y.append(y)\n",
    "            X.append([int(p) for p in row[1].split()])\n",
    "    X, Y = np.array(X) / 255.0, np.array(Y)  \n",
    "    return X, Y\n",
    "\n",
    "def error_rate(T, P):\n",
    "    return np.mean(T != P)\n",
    "\n",
    "def y2indicator(y, dims):\n",
    "    N = len(y)\n",
    "    y = y.astype(np.int32)\n",
    "    ind = np.zeros((N, dims))\n",
    "    for i in range(N):\n",
    "        ind[i, y[i]] = 1\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40263, 2304)\n",
      "(40263,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "XA, YA = get_data()\n",
    "# XA, YA = shuffle(XA, YA)\n",
    "\n",
    "XO, YO = XA[YA!=1, :], YA[YA!=1]\n",
    "X1 = XA[YA==1, :]\n",
    "X1 = np.repeat(X1, 9, axis=0)\n",
    "XA = np.vstack([XO, X1])\n",
    "YA = np.concatenate((YO, [1]*len(X1)))\n",
    "\n",
    "XA, YA = shuffle(XA, YA)\n",
    "print(XA.shape)\n",
    "print(YA.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 10e-7\n",
    "reg = 0.01\n",
    "max_iter = 1000\n",
    "print_period = 10\n",
    "        \n",
    "N, D = XA.shape\n",
    "K = len(set(YA))\n",
    "\n",
    "print(\"X.shape:\", XA.shape)\n",
    "print(\"N:\", N)\n",
    "print(\"D:\", D)\n",
    "print(\"K:\", K)\n",
    "\n",
    "        \n",
    "# split data into training set and validation set\n",
    "Xtrain, Ytrain = XA[:-1000], YA[:-1000]\n",
    "Xtest, Ytest= XA[-1000:], YA[-1000:]\n",
    "        \n",
    "# one-hot-encoding the labels for both training set and validation set\n",
    "Ytrain_ind = y2indicator(Ytrain, K)\n",
    "Ytest_ind = y2indicator(Ytest, K)\n",
    "\n",
    "# TODO: try other batch and the whole training set\n",
    "batch_sz = 500\n",
    "\n",
    "print(\"batch_sz:\", batch_sz)\n",
    "n_batches = int(N / batch_sz)\n",
    "\n",
    "print(\"n_batches:\", n_batches)\n",
    "\n",
    "M1 = 300\n",
    "M2 = 100\n",
    "# K = 10\n",
    "\n",
    "W1_init = np.random.randn(D, M1) / 28\n",
    "b1_init = np.zeros(M1)\n",
    "W2_init = np.random.randn(M1, M2) / np.sqrt(M1 + M2)\n",
    "b2_init = np.zeros(M2)\n",
    "W3_init = np.random.randn(M2, K) / np.sqrt(M2 + K)\n",
    "b3_init = np.zeros(K)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, D), name = 'X')\n",
    "T = tf.placeholder(tf.float32, shape=(None, K), name = 'T')\n",
    "\n",
    "W1 = tf.Variable(W1_init.astype(np.float32))\n",
    "b1 = tf.Variable(b1_init.astype(np.float32))\n",
    "W2 = tf.Variable(W2_init.astype(np.float32))\n",
    "b2 = tf.Variable(b2_init.astype(np.float32))\n",
    "W3 = tf.Variable(W3_init.astype(np.float32))\n",
    "b3 = tf.Variable(b3_init.astype(np.float32))\n",
    "\n",
    "# forward\n",
    "Z1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "Z2 = tf.nn.relu(tf.matmul(Z1, W2) + b2)\n",
    "Yish = tf.matmul(Z2, W3) + b3\n",
    "\n",
    "# establish loss/cost function\n",
    "loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=Yish, labels=T))\n",
    "print(\"loss, \", loss)\n",
    "\n",
    "# establish the gradient descent optimizer with respect to the loss function\n",
    "train_op = tf.train.RMSPropOptimizer(learning_rate, decay=0.99, momentum=0.9).minimize(loss)\n",
    "print(\"train_op: \", train_op)\n",
    "\n",
    "#\n",
    "predict_op = tf.argmax(Yish, 1)\n",
    "print(\"predict_op: \", predict_op)\n",
    "\n",
    "#\n",
    "LL = []\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        for j in range(n_batches):\n",
    "  \n",
    "            Xbatch = Xtrain[j * batch_sz : (j * batch_sz + batch_sz),]\n",
    "            Ybatch = Ytrain_ind[j * batch_sz : (j * batch_sz + batch_sz),]\n",
    "            \n",
    "            session.run(train_op, feed_dict={X: Xbatch, T:Ybatch})\n",
    "            if j % print_period == 0:\n",
    "                test_cost = session.run(loss, feed_dict={X: Xtest, T: Ytest_ind})\n",
    "                prediction = session.run(predict_op, feed_dict={X: Xtest})\n",
    "                err = error_rate(prediction, Ytest)\n",
    "                print(\"Cost / err at iteration i=%d, batch j=%d: %.3f / %.3f\" % (i, j, test_cost, err))\n",
    "                LL.append(test_cost)\n",
    "                \n",
    "plt.plot(LL)\n",
    "plt.show()\n",
    "\n",
    "# increase max_iter and notice how the test cost starts to increase.\n",
    "# are we overfitting by adding that extra layer?\n",
    "# how would you add regularization to this model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
