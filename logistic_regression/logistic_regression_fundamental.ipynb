{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Fundamental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: Make prediction through forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(100, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "N = 100  # 100 samples\n",
    "D = 2    # 2 dimensions/features\n",
    "\n",
    "X = np.random.randn(N, D)\n",
    "\n",
    "X[:50,:] = X[:50,:] - 2*np.ones((50, D))\n",
    "X[50:,:] = X[50:,:] + 2*np.ones((50, D))\n",
    "\n",
    "T = np.array([0]*50 + [1]*50)\n",
    "\n",
    "print(X.shape)\n",
    "ones = np.ones((N,1))\n",
    "Xb = np.concatenate((X, ones), axis=1)\n",
    "print(Xb.shape)\n",
    "\n",
    "# randomly initialize weights\n",
    "w = np.random.randn(D + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W):\n",
    "    '''\n",
    "    Compute the output of the Logistic Regression Network\n",
    "    '''\n",
    "    return sigmoid(X.dot(W))\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  1.  0.  0.  1.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "Y = forward(Xb, w)\n",
    "predictions = np.round(Y)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.4\n"
     ]
    }
   ],
   "source": [
    "def classification_rate(Y, P):\n",
    "    return np.mean(Y==P)\n",
    "\n",
    "\n",
    "print(\"score:\", classification_rate(predictions, T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loss function for Logistic Regression: Cross Entropy Error:\n",
    "\n",
    "$$ J = -{tlog(y) + (1-t)log(1-y)} $$\n",
    "\n",
    "where t = target (actual label), y = output of logistic (predicted output)\n",
    "\n",
    "* If t=1, only first term matters, if t=0, only second term matters\n",
    "* Multiple Training Examples:\n",
    "<img src=\"images/cross_entropy.png\" alt=\"Drawing\" style=\"width:60%;height:60%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(T, Y):\n",
    "    J = 0;\n",
    "    N = len(T)\n",
    "    for i in range(N):\n",
    "        if T[i] == 1:\n",
    "            J-=np.log(Y[i])\n",
    "        else:\n",
    "            J-=np.log(1-Y[i])\n",
    "    return J / N\n",
    "\n",
    "# def cross_entropy(T, Y):\n",
    "#     return -np.mean(T*np.log(Y) + (1-T)*np.log(1-Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy: 175.743019268\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross entropy:\", cross_entropy(T, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy: 26.2565022792\n"
     ]
    }
   ],
   "source": [
    "w_b = np.array([0,4,4]) # Bayes classifier weights\n",
    "Y_ = forward(Xb, w_b)\n",
    "print(\"Cross entropy:\", cross_entropy(T, Y_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimize the weights**\n",
    "* Gradient descent throught back propragation\n",
    "* Vectorized form:\n",
    "<img src=\"images/vectorize_form.png\" alt=\"Drawing\" style=\"width:30%;height:30%\"/>\n",
    "* Vectorized matrix form:\n",
    "    * X is N x D, X.T (tranpose X) is D x N \n",
    "    * Y, T are N x 1, (Y-T) is still N x 1\n",
    "    * Multiply X.T (Y-T): \n",
    "        * shape is (D x N)(N x 1) --> (D x 1), which is the correct shape for W\n",
    "        * N gets summed over\n",
    "<img src=\"images/vectorize_matrix_form.png\" alt=\"Drawing\" style=\"width:30%;height:30%\"/>\n",
    "* Vectorized form for bias term:\n",
    "<img src=\"images/vectorize_form_bias.png\" alt=\"Drawing\" style=\"width:57%;height:57%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0890789325707\n",
      "10 0.0882156405002\n",
      "20 0.0875952355412\n",
      "30 0.0871141456062\n",
      "40 0.0867136110172\n",
      "50 0.0863601223747\n",
      "60 0.08603449975\n",
      "70 0.0857257329599\n",
      "80 0.0854274848448\n",
      "90 0.0851360978369\n",
      "final w: [  8.49223425  12.97322782  -1.11169253]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    if i % 10 == 0:\n",
    "        print(i, cross_entropy(T, Y))\n",
    "        \n",
    "    # gradient descent through back propragation updating the weights\n",
    "    w += learning_rate * Xb.T.dot(T - Y)\n",
    "    \n",
    "    # forward propagation calculating the output\n",
    "    Y = sigmoid(Xb.dot(w))\n",
    " \n",
    "print(\"final w:\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement Logistic Regression fit function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 276.637844572\n",
      "10 1.18118374042\n",
      "20 0.373551821885\n",
      "30 0.190401454182\n",
      "40 0.12386126001\n",
      "50 0.0910378734668\n",
      "60 0.0718393809707\n",
      "70 0.059367537013\n",
      "80 0.0506755978232\n",
      "90 0.044307457976\n",
      "final w: [ 14.28726757  19.63086634  -1.915264  ]\n"
     ]
    }
   ],
   "source": [
    "def fit(X, T, epochs=100, learning_rate=0.1):\n",
    "    \n",
    "    N, D = X.shape\n",
    "    \n",
    "    # add bias \n",
    "    ones = np.ones((N,1))\n",
    "    Xb = np.concatenate((X, ones), axis=1)\n",
    "\n",
    "    # randomly initialize weights\n",
    "    w = np.random.randn(D + 1)\n",
    "    \n",
    "#     Y = forward(Xb, w)\n",
    "\n",
    "    # training weights using gradient descent\n",
    "    for i in range(epochs):\n",
    "        Y = forward(Xb, w)\n",
    "        if i % 10 == 0:\n",
    "            print(i, cross_entropy(T, Y))\n",
    "        \n",
    "        w += learning_rate * Xb.T.dot(T - Y)\n",
    "#         Y = forward(Xb, w)\n",
    " \n",
    "    print(\"final w:\", w)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(X, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 55.8385093915\n",
      "10 0.00123981685528\n",
      "20 0.0024845510581\n",
      "30 0.00470411134372\n",
      "40 0.00845191410794\n",
      "50 0.014451426845\n",
      "60 0.0235428591108\n",
      "70 0.0365268816369\n",
      "80 0.0538802169219\n",
      "90 0.0754043690019\n",
      "final w: [ 2.92456498  2.93457206  0.22886414]\n"
     ]
    }
   ],
   "source": [
    "def fit_ridge(X, T, epochs=100, learning_rate=0.1, lamba=0.1):\n",
    "    \n",
    "    N, D = X.shape\n",
    "    \n",
    "    # add bias \n",
    "    ones = np.ones((N,1))\n",
    "    Xb = np.concatenate((X, ones), axis=1)\n",
    "\n",
    "    # randomly initialize weights\n",
    "    w = np.random.randn(D + 1)\n",
    "    \n",
    "    Y = forward(Xb, w)\n",
    "\n",
    "    # training weights using gradient descent\n",
    "    for i in range(epochs):\n",
    "        if i % 10 == 0:\n",
    "            print(i, cross_entropy(T, Y))\n",
    "        \n",
    "        w += learning_rate * (Xb.T.dot(T - Y) - lamba * w)\n",
    "#         w += learning_rate * (np.dot((T - Y).T, Xb) - lamba * w)\n",
    "        Y = forward(Xb, w)\n",
    " \n",
    "    print(\"final w:\", w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_ridge(X, T, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
