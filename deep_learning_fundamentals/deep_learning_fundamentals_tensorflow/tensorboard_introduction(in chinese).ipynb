{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard的使用简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文是对网上一些关于Tensorboard资料的汇总，同时也加上一些了自己的理解。参考文献请看最后的Reference section.\n",
    "\n",
    "当使用Tensorflow训练大量深层的神经网络时，我们希望去跟踪神经网络的整个训练过程中的信息，比如迭代的过程中每一层参数是如何变化与分布的，比如每次循环参数更新后模型在测试集与训练集上的准确率是如何的，比如损失值的变化情况，等等。如果能在训练的过程中将一些信息加以记录并可视化得表现出来，是不是对我们探索模型有更深的帮助与理解呢？\n",
    "\n",
    "Tensorflow官方推出了可视化工具Tensorboard，可以帮助我们实现以上功能，它可以将模型训练过程中的各种数据汇总起来存在自定义的路径与日志文件中，然后在指定的web端可视化地展现这些信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensorboard介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tensorboard的数据形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard可以记录与展示以下数据形式： \n",
    "* 标量Scalars \n",
    "* 图片Images \n",
    "* 音频Audio \n",
    "* 计算图Graph \n",
    "* 数据分布Distribution \n",
    "* 直方图Histograms \n",
    "* 嵌入向量Embeddings\n",
    "\n",
    "<img src='tensorboard_images/tensorboard_overview.png' style='height:400px;width:640px'/>\n",
    "\n",
    "其中 TEXT 是 最新版（应该是 1.3）才加进去的，实验性功能，官方都没怎么介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tensorboard使用流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 首先肯定是先建立一个graph,你想从这个graph中获取某些数据的信息\n",
    "2. 确定要在graph中的哪些节点放置summary operations以记录信息 \n",
    "    * 使用`tf.summary.scalar`记录标量 \n",
    "    * 使用`tf.summary.histogram`记录数据的直方图 \n",
    "    * 使用`tf.summary.distribution`记录数据的分布图 \n",
    "    * 使用`tf.summary.image`记录图像数据 \n",
    "3. 以上summary operations并不会主动去执行计算，除非你命令执行这些summary operations计算(通过`session.run()`), 或者它被其他的需要run的operation所依赖。而我们上一步创建的这些summary operations其实并不被其他节点依赖，因此，我们需要特地去运行所有的summary节点。但是，我们通常会设置很多这样的summary节点，要手动一个一个去启动自然是及其繁琐的，因此我们可以使用`tf.summary.merge_all()`去将所有summary节点合并成一个节点，只要运行这个节点，就能产生所有我们之前设置的summary data。\n",
    "4. 使用`tf.summary.FileWriter`(日志书写器)将运行后输出的数据都保存到本地磁盘中. 主要有三个步骤：\n",
    "    1. 实例化日志书写器：`summary_writer = tf.summary.FileWriter(logdir, graph=sess.graph)`，实例化的同时传入 graph 将当前计算图写入日志\n",
    "    2. 调用日志书写器实例对象summary_writer的`add_summary(summary, global_step=current_step)`方法将所有汇总日志写入文件\n",
    "    3. 调用日志书写器实例对象summary_writer的`close()`方法写入内存，\n",
    "5. 当训练完成后，在命令行使用 `tensorboard --logdir=path/to/log-directory` 来启动 TensorBoard，按照提示在浏览器打开页面，注意把 `path/to/log-directory` 替换成你上面指定的目录。\n",
    "\n",
    "<img src='tensorboard_images/tensorboard_sequence_diagram.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Summary Operations 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.1 **SCOPE** \n",
    "\n",
    "```python\n",
    "tf.name_scope(scope_name)：\n",
    "```\n",
    "\n",
    "* 为Graph中的Tensor添加层级，TensorBoard会按照代码指定的层级进行展示，初始状态下只绘制最高层级的效果，点击后可展开层级看到下一层的细节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.2 **SCALAR**\n",
    "\n",
    "```python\n",
    "tf.summary.scalar(name, tensor, collections=None, family=None)\n",
    "```\n",
    "\n",
    "可视化训练过程中随着迭代次数准确率(val_acc)、损失值(train/test loss)、学习率(learning rate)、每一层的权重和偏置的统计量(mean、std、max/min)等的变化曲线\n",
    "\n",
    "输入参数：\n",
    "\n",
    "* `name`：此操作节点的名字，TensorBoard 中绘制的图形的纵轴也将使用此名字\n",
    "* `tensor`： 需要监控的变量 A real numeric Tensor containing a single value.\n",
    "\n",
    "输出：\n",
    "\n",
    "A scalar Tensor of type string. Which contains a Summary protobuf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.3 **IMAGE**\n",
    "\n",
    "```python\n",
    "tf.summary.image(name, tensor, max_outputs=3, collections=None, family=None)\n",
    "```\n",
    "\n",
    "可视化当前轮训练使用的训练/测试图片或者 feature maps\n",
    "\n",
    "输入参数：\n",
    "\n",
    "* `name`：此操作节点的名字，TensorBoard 中绘制的图形的纵轴也将使用此名字\n",
    "* `tensor`： A 4-D uint8 or float32 Tensor of shape `[batch_size, height, width, channels]` where channels is 1, 3, or 4\n",
    "* `max_outputs`：Max number of batch elements to generate images for\n",
    "\n",
    "输出：\n",
    "\n",
    "A scalar Tensor of type string. Which contains a Summary protobuf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.4 **HISTOGRAM**\n",
    "\n",
    "```python\n",
    "tf.summary.histogram(name, values, collections=None, family=None)\n",
    "```\n",
    "\n",
    "可视化张量的取值分布\n",
    "\n",
    "输入参数：\n",
    "\n",
    "* `name`：此操作节点的名字，TensorBoard 中绘制的图形的纵轴也将使用此名字\n",
    "* `tensor`： A real numeric Tensor. Any shape. Values to use to build the histogram\n",
    "\n",
    "输出：\n",
    "\n",
    "A scalar Tensor of type string. Which contains a Summary protobuf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.5 **MERGE_ALL**\n",
    "\n",
    "```python\n",
    "tf.summary.merge_all(key=tf.GraphKeys.SUMMARIES)\n",
    "```\n",
    "\n",
    "* Merges all summaries collected in the default graph\n",
    "* 因为程序中定义的写日志操作比较多，一一调用非常麻烦，所以TensoorFlow 提供了此函数来整理所有的日志生成操作，eg：`merged = tf.summary.merge_all()`\n",
    "* 此操作不会立即执行，所以，需要明确的运行这个操作`(summary = sess.run(merged))`来得到汇总结果\n",
    "* 最后调用日志书写器实例对象的`add_summary(summary, global_step=current_step)`方法将所有汇总日志写入文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.6 **FILE WRITER**\n",
    "\n",
    "```python\n",
    "tf.summary.FileWriter(logdir, graph=None, flush_secs=120, max_queue=10)\n",
    "```\n",
    "\n",
    "* 负责将事件日志(graph、scalar/image/histogram、event)写入到指定的文件中\n",
    "* 初始化参数：\n",
    "\n",
    "    * `logdir`：事件写入的目录\n",
    "    * `graph`：如果在初始化的时候传入sess,graph的话，相当于调用add_graph() 方法，用于计算图的可视化\n",
    "    * `flush_sec`：How often, in seconds, to flush the added summaries and events to disk.\n",
    "    * `max_queue`：Maximum number of summaries or events pending to be written to disk before one of the ‘add’ calls block.\n",
    "\n",
    "* 其它常用方法：\n",
    "\n",
    "    * `add_event(event)`：Adds an event to the event file\n",
    "    * `add_graph(graph, global_step=None)`：Adds a Graph to the event file，Most users pass a graph in the constructor instead\n",
    "    * `add_summary(summary, global_step=None)`：Adds a Summary protocol buffer to the event file，一定注意要传入 global_step\n",
    "    * `close()`：Flushes the event file to disk and close the file\n",
    "    * `flush()`：Flushes the event file to disk\n",
    "    * `add_meta_graph(meta_graph_def,global_step=None)`\n",
    "    * `add_run_metadata(run_metadata, tag, global_step=None)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tensorboard使用案例\n",
    "\n",
    "我们使用最基础的识别手写字体的案例. 本案例中，我们不追求最优的模型，只是建立一个简单的神经网络，让大家了解如何使用Tensorboard。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 导入包，定义超参数，载入数据\n",
    "\n",
    "\n",
    "2.1.1. 首先还是导入需要的包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2 定义固定的超参数,方便待使用时直接传入。\n",
    "\n",
    "在这里我们不讨论如何选择超参数，而假设我们已经获得了最优的超参数，其中设置\n",
    "* learning rate为0.001，\n",
    "* dropout的保留节点比例为0.9，\n",
    "* epoch为1000.\n",
    "\n",
    "另外，还要设置两个路径，第一个是数据下载下来存放的地方，一个是summary输出保存的地方。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_step = 1000  # 最大迭代次数\n",
    "learning_rate = 0.001   # 学习率\n",
    "dropout = 0.9   # dropout时随机保留神经元的比例\n",
    "\n",
    "data_dir = 'tensorboard/data'   # 样本数据存储的路径\n",
    "log_dir = 'tensorboard'    # 输出日志保存的路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.3 加载数据\n",
    "\n",
    "下载数据是直接调用了tensorflow提供的函数read_data_sets,输入两个参数，第一个是下载到数据存储的路径，第二个one_hot表示是否要将类别标签进行独热编码。它首先回去找制定目录下有没有这个数据文件，没有的话才去下载，有的话就直接读取。所以第一次执行这个命令，速度会比较慢。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/yankang/anaconda/envs/opencv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting tensorboard/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/yankang/anaconda/envs/opencv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting tensorboard/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/yankang/anaconda/envs/opencv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting tensorboard/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting tensorboard/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/yankang/anaconda/envs/opencv/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(data_dir,one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 创建特征与标签的占位符，保存输入的图片数据到summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1 创建tensorflow的默认会话："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2 创建输入数据的占位符，分别创建特征数据x，标签数据y_ \n",
    "\n",
    "在tf.placeholder()函数中传入了3个参数，第一个是定义数据类型为float32；第二个是数据的shape. 特征数据是大小784的向量，标签数据是大小为10的向量，None表示不定死大小，到时候可以传入任何数量的样本；第3个参数是这个占位符的名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "为了在TensorBoard中展示节点名称，设计网络时会常使用tf.name_scope限制命名空间，\n",
    "在这个with下所有的节点都会自动命名为input/xxx这样的格式。\n",
    "定义输入x和y的placeholder，并将输入的一维数据变形为784(由28×28计算得到)的图片存储到另一个tensor，\n",
    "这样就可以使用tf.summary.image将图片数据汇总给TensorBoard展示了。\n",
    "\"\"\"\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10], name='y-input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.3 使用tf.summary.image保存图像信息 \n",
    "\n",
    "特征数据其实就是图像的像素数据拉升成(flatten)一个`[1x784]`的向量，现在如果想在tensorboard上还原出输入的特征数据对应的图片，就需要将拉升的向量转变成`[28x28x1]`的原始像素了，于是可以用tf.reshape()直接重新调整特征数据的维度, 将输入的数据转换成`[28x28x1]`的shape，存储成另一个tensor，命名为image_shaped_input。 \n",
    "\n",
    "为了能使图片在tensorbord上展示出来, 使用`tf.summary.image()`函数将图片数据汇总给tensorbord. tf.summary.image()中传入的第一个参数是命名，第二个是图片数据，第三个是最多展示的张数，此处为10张"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('input_reshape'):\n",
    "    image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    tf.summary.image('input', image_shaped_input, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 创建初始化参数的方法，与参数信息汇总到summary的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1 在构建神经网络模型中，每一层中都需要去初始化参数w,b, 为了使代码方便管理，最好将初始化参数的过程封装成方法function. 创建初始化权重w的方法，生成大小等于传入的shape参数，标准差为0.1,正态分布的随机数，并且将它转换成tensorflow中的variable返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建初始换偏执项b的方法，生成大小为传入参数shape的常数0.1，并将其转换成tensorflow的variable并返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variable(shape):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.2 在训练的过程在参数是不断地在改变和优化的，我们往往想知道每次迭代后参数都做了哪些变化，可以将参数的信息展现在tenorbord上，因此我们专门写一个方法来收录每次的参数信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "计算出Variable的mean,stddev,max和min， \n",
    "对这些标量数据使用tf.summary.scalar进行记录和汇总。 \n",
    "同时，使用tf.summary.histogram直接记录变量var的直方图。 \n",
    "\"\"\" \n",
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        # 计算参数的均值，并使用tf.summary.scaler记录\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "\n",
    "        # 计算参数的标准差\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            # 使用tf.summary.scaler记录记录下标准差，最大值，最小值\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            # 用直方图记录参数的分布\n",
    "            tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 构建神经网络层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.1 创建第一层隐藏层 \n",
    "\n",
    "创建一个构建隐藏层的方法,输入的参数有： \n",
    "* `input_tensor`：特征数据 \n",
    "* `input_dim`：输入数据的维度大小 \n",
    "* `output_dim`：输出数据的维度大小(=隐层神经元个数） \n",
    "* `layer_name`：命名空间 \n",
    "* `act=tf.nn.relu`：激活函数（默认是relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "定一个创建一层神经网络并进行数据汇总的函数nn_layer。 \n",
    "这个函数的输入参数有输入数据input_tensor,输入的维度input_dim,输出的维度output_dim和层名称layer_name，激活函数act则默认使用Relu。 \n",
    "在函数内，显示初始化这层神经网络的权重和偏置，并使用前面定义的variable_summaries对variable进行数据汇总。 \n",
    "然后对输入做矩阵乘法并加上偏置，再将未进行激活的结果使用tf.summary.histogram统计直方图。\n",
    "同时，在使用激活函数后，再使用tf.summary.histogram统计一次。 \n",
    "\"\"\" \n",
    "def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "    \"\"\"Reusable code for making a simple neural net layer.\n",
    "    It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
    "    It also sets up name scoping so that the resultant graph is easy to read,\n",
    "    and adds a number of summary ops.\n",
    "    \"\"\"\n",
    "    # 设置命名空间\n",
    "    with tf.name_scope(layer_name):\n",
    "        # 调用之前的方法初始化权重w，并且调用参数信息的记录方法，记录w的信息\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = weight_variable([input_dim, output_dim])\n",
    "            variable_summaries(weights)\n",
    "        # 调用之前的方法初始化权重b，并且调用参数信息的记录方法，记录b的信息\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = bias_variable([output_dim])\n",
    "            variable_summaries(biases)\n",
    "        # 执行wx+b的线性计算，并且用直方图记录下来\n",
    "        with tf.name_scope('linear_compute'):\n",
    "            preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "            tf.summary.histogram('linear', preactivate)\n",
    "        # 将线性输出经过激励函数，并将输出也用直方图记录下来\n",
    "        activations = act(preactivate, name='activation')\n",
    "        tf.summary.histogram('activations', activations)\n",
    "\n",
    "      # 返回激励层的最终输出\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用隐层创建函数创建一个隐藏层：输入的维度是特征的维度784，神经元个数是500，也就是输出的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = nn_layer(x, 784, 500, 'layer1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.2 创建一个dropout层，,随机关闭掉hidden1的一些神经元，并记录keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('dropout'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "    dropped = tf.nn.dropout(hidden1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.3 创建一个输出层，输入的维度是上一层的输出：500,输出的维度是分类的类别种类：10，激活函数设置为全等映射identity.（暂且先别使用softmax,会放在之后的损失函数中一起计算）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = nn_layer(dropped, 500, 10, 'layer2', act=tf.identity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 创建损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`tf.nn.softmax_cross_entropy_with_logits`对前面输出层的结果进行softmax处理并计算交叉熵损失cross_entropy。计算平均损失，并使用`tf.summary.scalar`进行统计汇总。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-372745cba610>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    # 计算交叉熵损失（每个样本都会有一个损失）\n",
    "    diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "    with tf.name_scope('total'):\n",
    "      # 计算所有样本交叉熵损失的均值\n",
    "      cross_entropy = tf.reduce_mean(diff)\n",
    "\n",
    "tf.summary.scalar('loss', cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 训练，并计算准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6.1 使用AdamOptimizer优化器训练模型，最小化交叉熵损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6.2 计算准确率,并用`tf.summary.scalar()`记录准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "      # 分别将预测和真实的标签中取出最大值的索引，弱相同则返回1(true),不同则返回0(false)\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "      # 求均值即为准确率\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 合并summary operation, 运行初始化变量\n",
    "\n",
    "由于之前定义了非常多的tf.summary的汇总操作，一一执行这些操作态麻烦, 所以这里使用tf.summary.merger_all()直接获取所有汇总操作，以便后面执行。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summaries合并\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 准备训练数据与测试数据，循环执行整个graph进行训练与评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.8.1 准备为训练与测试的输入数据. \n",
    "\n",
    "* 如果是`train==true`，就从mnist.train中获取一个batch样本，并且设置dropout值。\n",
    "* 如果是`train==false`,则获取minist.test的测试数据，并且设置keep_prob为1，即保留所有神经元开启。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_dict(train):\n",
    "    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "    if train:\n",
    "        xs, ys = mnist.train.next_batch(100)\n",
    "        k = dropout\n",
    "    else:\n",
    "        xs, ys = mnist.test.images, mnist.test.labels\n",
    "        k = 1.0\n",
    "    return {x: xs, y_: ys, keep_prob: k}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.8.2 训练模型\n",
    "\n",
    "* 每隔10步，就进行一次summary merge, 并打印一次测试数据集的准确率(accuracy)，然后将测试数据集的各种summary信息写进日志中。 \n",
    "* 每隔100步，记录原信息其他每一步时都记录下训练集的summary信息并写到日志中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at step 0: 0.1311\n",
      "Accuracy at step 10: 0.6706\n",
      "Accuracy at step 20: 0.8277\n",
      "Accuracy at step 30: 0.8661\n",
      "Accuracy at step 40: 0.881\n",
      "Accuracy at step 50: 0.8922\n",
      "Accuracy at step 60: 0.9001\n",
      "Accuracy at step 70: 0.9055\n",
      "Accuracy at step 80: 0.9045\n",
      "Accuracy at step 90: 0.9149\n",
      "Adding run metadata for 99\n",
      "Accuracy at step 100: 0.921\n",
      "Accuracy at step 110: 0.9205\n",
      "Accuracy at step 120: 0.9254\n",
      "Accuracy at step 130: 0.9244\n",
      "Accuracy at step 140: 0.9284\n",
      "Accuracy at step 150: 0.9278\n",
      "Accuracy at step 160: 0.9273\n",
      "Accuracy at step 170: 0.9327\n",
      "Accuracy at step 180: 0.935\n",
      "Accuracy at step 190: 0.9363\n",
      "Adding run metadata for 199\n",
      "Accuracy at step 200: 0.9354\n",
      "Accuracy at step 210: 0.9374\n",
      "Accuracy at step 220: 0.934\n",
      "Accuracy at step 230: 0.9358\n",
      "Accuracy at step 240: 0.9359\n",
      "Accuracy at step 250: 0.9387\n",
      "Accuracy at step 260: 0.942\n",
      "Accuracy at step 270: 0.9427\n",
      "Accuracy at step 280: 0.9433\n",
      "Accuracy at step 290: 0.9451\n",
      "Adding run metadata for 299\n",
      "Accuracy at step 300: 0.9455\n",
      "Accuracy at step 310: 0.9459\n",
      "Accuracy at step 320: 0.9475\n",
      "Accuracy at step 330: 0.9487\n",
      "Accuracy at step 340: 0.9492\n",
      "Accuracy at step 350: 0.9485\n",
      "Accuracy at step 360: 0.9485\n",
      "Accuracy at step 370: 0.951\n",
      "Accuracy at step 380: 0.9507\n",
      "Accuracy at step 390: 0.9507\n",
      "Adding run metadata for 399\n",
      "Accuracy at step 400: 0.9508\n",
      "Accuracy at step 410: 0.9506\n",
      "Accuracy at step 420: 0.9509\n",
      "Accuracy at step 430: 0.9517\n",
      "Accuracy at step 440: 0.9526\n",
      "Accuracy at step 450: 0.9546\n",
      "Accuracy at step 460: 0.9561\n",
      "Accuracy at step 470: 0.957\n",
      "Accuracy at step 480: 0.9579\n",
      "Accuracy at step 490: 0.9574\n",
      "Adding run metadata for 499\n",
      "Accuracy at step 500: 0.9584\n",
      "Accuracy at step 510: 0.9582\n",
      "Accuracy at step 520: 0.9589\n",
      "Accuracy at step 530: 0.9578\n",
      "Accuracy at step 540: 0.9584\n",
      "Accuracy at step 550: 0.9567\n",
      "Accuracy at step 560: 0.9604\n",
      "Accuracy at step 570: 0.9606\n",
      "Accuracy at step 580: 0.9615\n",
      "Accuracy at step 590: 0.9614\n",
      "Adding run metadata for 599\n",
      "Accuracy at step 600: 0.962\n",
      "Accuracy at step 610: 0.9615\n",
      "Accuracy at step 620: 0.9587\n",
      "Accuracy at step 630: 0.9585\n",
      "Accuracy at step 640: 0.9586\n",
      "Accuracy at step 650: 0.9626\n",
      "Accuracy at step 660: 0.9617\n",
      "Accuracy at step 670: 0.958\n",
      "Accuracy at step 680: 0.9615\n",
      "Accuracy at step 690: 0.9634\n",
      "Adding run metadata for 699\n",
      "Accuracy at step 700: 0.962\n",
      "Accuracy at step 710: 0.9619\n",
      "Accuracy at step 720: 0.9641\n",
      "Accuracy at step 730: 0.9643\n",
      "Accuracy at step 740: 0.9617\n",
      "Accuracy at step 750: 0.9638\n",
      "Accuracy at step 760: 0.9659\n",
      "Accuracy at step 770: 0.9637\n",
      "Accuracy at step 780: 0.9598\n",
      "Accuracy at step 790: 0.9636\n",
      "Adding run metadata for 799\n",
      "Accuracy at step 800: 0.9639\n",
      "Accuracy at step 810: 0.9656\n",
      "Accuracy at step 820: 0.9637\n",
      "Accuracy at step 830: 0.9667\n",
      "Accuracy at step 840: 0.9674\n",
      "Accuracy at step 850: 0.9691\n",
      "Accuracy at step 860: 0.9641\n",
      "Accuracy at step 870: 0.968\n",
      "Accuracy at step 880: 0.9681\n",
      "Accuracy at step 890: 0.9681\n",
      "Adding run metadata for 899\n",
      "Accuracy at step 900: 0.9678\n",
      "Accuracy at step 910: 0.9681\n",
      "Accuracy at step 920: 0.9681\n",
      "Accuracy at step 930: 0.9679\n",
      "Accuracy at step 940: 0.97\n",
      "Accuracy at step 950: 0.967\n",
      "Accuracy at step 960: 0.9665\n",
      "Accuracy at step 970: 0.9686\n",
      "Accuracy at step 980: 0.9711\n",
      "Accuracy at step 990: 0.966\n",
      "Adding run metadata for 999\n"
     ]
    }
   ],
   "source": [
    "# 运行初始化所有变量\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 定义两个tf.summary.FileWrite(文件记录器)在不同的子目录，分别用来存放训练和测试的日志数据。 \n",
    "    # 同时，将Session的计算图sess.graph加入训练过程的记录器，这样在TensorBoard的GRAPHS窗口中就能展示整个计算图的可视化效果。 \n",
    "    train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(log_dir + '/test')\n",
    "\n",
    "    \n",
    "    # 进入训练的循环中，\n",
    "    # 1. 每隔10步执行一次merged（数据汇总），accuracy（求测试集上的预测准确率）操作，\n",
    "    #    并使应test_write.add_summary将汇总结果summary和循环步数i写入日志文件; \n",
    "    # 2. 每隔100步，使用tf.RunOption定义Tensorflow运行选项，其中设置trace_level为FULL——TRACE, \n",
    "    #    并使用tf.RunMetadata()定义Tensorflow运行的元信息, 这样可以记录训练是运算时间和内存占用等方面的信息. \n",
    "    #    再执行merged数据汇总操作和train_step训练操作，将汇总summary和训练元信息run_metadata添加到train_writer. \n",
    "    #    平时，则执行merged操作和train_step操作，并添加summary到trian_writer。 \n",
    "    # 3. 所有训练全部结束后，关闭train_writer和test_writer。 \n",
    "    for i in range(max_step):\n",
    "        if i % 10 == 0:  # 记录测试集的summary与accuracy\n",
    "            summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "            test_writer.add_summary(summary, i)\n",
    "            print('Accuracy at step %s: %s' % (i, acc))\n",
    "        else:  # 记录训练集的summary\n",
    "            if i % 100 == 99:  # Record execution stats\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, _ = sess.run([merged, train_step],\n",
    "                                  feed_dict=feed_dict(True),\n",
    "                                  options=run_options,\n",
    "                                  run_metadata=run_metadata)\n",
    "                train_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n",
    "                train_writer.add_summary(summary, i)\n",
    "                print('Adding run metadata for', i)\n",
    "            else:  # Record a summary\n",
    "                summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "                train_writer.add_summary(summary, i)\n",
    "    train_writer.close()\n",
    "    test_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 执行程序，tensorboard生成可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 运行整个程序，在程序中定义的summary node就会将要记录的信息全部保存在指定的logdir路径中了，训练的记录会存一份文件，测试的记录会存一份文件。\n",
    "2. 进入linux命令行，运行以下代码，等号后面加上summary日志保存的路径（在程序第一步中就事先自定义了）\n",
    "```\n",
    "tensorboard --logdir=tensorboard/train\n",
    "```\n",
    "3. 执行命令之后会出现一条信息，上面有网址，将网址在浏览器中打开就可以看到我们定义的可视化信息了:\n",
    "```\n",
    "TensorBoard 1.10.0 at http://localhost:6006 (Press CTRL+C to quit)\n",
    "```\n",
    "4. 将 http://localhost:6006 在浏览器中打开，成功的话如下:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='tensorboard_images/first_page.png'/>\n",
    "\n",
    "于是我们可以从这个web端看到所有收集数据的可视化信息了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Tensorboard Web端解释\n",
    "看到最上面橙色一栏的菜单，分别有7个栏目，都一一对应着我们程序中定义信息的类型\n",
    "* SCALARS, IMAGES, GRAPHS, DISTRIBUTIONS, HISTOGRAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 `SCALARS` 标量的信息\n",
    "\n",
    "我们在程序中用tf.summary.scalars()定义的信息都会在这个窗口。本文程序中定义的标量有：准确率accuracy,dropout的保留率，隐藏层中的参数信息，已经交叉熵损失loss。这些都在SCLARS窗口下显示出来了。\n",
    "\n",
    "点开accuracy,可以看到随着循环次数的增加，两者的准确度也在通趋势增加，值得注意的是，在0到100次的循环中准确率快速激增，100次之后保持微弱地上升趋势，直达1000次时会到达0.967左右 \n",
    "\n",
    "<img src='tensorboard_images/accuracy.png' style='height:300px; width:450px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "点开loss，可见损失的降低趋势:\n",
    "\n",
    "<img src='tensorboard_images/loss.png' style='height:300px; width:450px' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "点开layer1，查看第一个隐藏层的参数信息:\n",
    "\n",
    "<img src='tensorboard_images/layer_1.png'/>\n",
    "    \n",
    "以上，第一排是偏执项b的信息，随着迭代的加深，最大值越来越大，最小值越来越小，\n",
    "\n",
    "> 与此同时，也伴随着方差越来越大, 也即神经元之间的参数差异越来越大。这样的情况是我们愿意看到的, 因为理想的情况下每个神经元都应该去关注不同的特征，所以他们的参数也应有所不同。 \n",
    "\n",
    "第二排是权值w的信息，同理，最大值，最小值，标准差也都有与b相同的趋势，神经元之间的差异越来越明显。w的均值初始化的时候是0，随着迭代其绝对值也越来越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "点开layer2:\n",
    "\n",
    "<img src='tensorboard_images/layer_2.png'/>\n",
    "    \n",
    "图中所示数据和layer1有类似的趋势。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2  `IMAGES` 图片信息 \n",
    "\n",
    "如果你的模型输入是图像（的像素值），然后你想看看模型每次的输入图像是什么样的，以保证每次输入的图像没有问题（因为你可能在模型中对图像做了某种变换，而这种变换是很容易出问题的），IMAGES 面板就可以显示出相应的输入图像，默认显示最新的输入图像. \n",
    "\n",
    "在程序中我们设置了一处保存了图像信息，就是在转变了输入特征的shape，然后记录到了summary image中，于是在tensorboard中就会还原出原始的图片了:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='tensorboard_images/images.png'/>\n",
    "\n",
    "整个窗口总共展现了10张图片（根据代码中的参数10）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 `AUDIO` 声音的信息\n",
    "\n",
    "本案例中没有涉及到声音的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4  `Graph`\n",
    "\n",
    "这个应该是最常用的面板了。很多时候我们的模型很复杂，包含很多层，我们想要总体上看下构建的网络到底是什么样的，这时候就用到 GRAPHS 面板了，在这里可以展示出你所构建的网络整体结构，显示数据流的方向和大小，也可以显示训练时每个节点的用时、耗费的内存大小以及参数多少。\n",
    "\n",
    "默认显示的图分为两部分：主图（Main Graph）和辅助节点（Auxiliary Nodes）。其中主图显示的就是网络结构，辅助节点则显示的是初始化、训练、保存等节点。我们可以双击某个节点或者点击节点右上角的 `+` 来展开查看里面的情况，也可以对齐进行缩放，每个节点的命名都是我们在代码中使用 `tf.name_scope()` 定义好的。\n",
    "\n",
    "<img src='tensorboard_images/main_graph.png'/>\n",
    "\n",
    "双击图中任意一个operation节点，将会显示该节点的详细信息：\n",
    "\n",
    "<img src='tensorboard_images/layer1_graph.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 `DISTRIBUTIONS` \n",
    "\n",
    "DISTRIBUTIONS 主要用来展示网络中各参数随训练步数的增加的变化情况，可以说是`多分位数折线图`的堆叠. 这里查看的是神经元输出的分布，有激活函数之前的分布，激活函数之后的分布等。\n",
    "\n",
    "<img src='tensorboard_images/distribution.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 `HISTOGRAM` \n",
    "\n",
    "`HISTOGRAMS` 和 `DISTRIBUTIONS` 是对同一数据不同方式的展现。与 DISTRIBUTIONS 不同的是，HISTOGRAMS 可以说是`频数分布直方图`的堆叠。\n",
    "\n",
    "<img src='tensorboard_images/histogram.png'/>\n",
    "\n",
    "我们可以点击左下角的蓝色小框来放大任意一个图：\n",
    "\n",
    "<img src='tensorboard_images/histogram_detail.png'/>\n",
    "\n",
    "图中：\n",
    "* 横轴表示权重值，纵轴表示训练步数。颜色越深表示时间越早，越浅表示时间越晚（越接近训练结束）。\n",
    "* HISTOGRAMS 还有个 Histogram mode，有两个选项：OVERLAY 和 OFFSET。选择 OVERLAY 时横轴为权重值，纵轴为频数，每一条折线为训练步数。颜色深浅与上面同理。默认为 OFFSET 模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "1. [Tensorflow的可视化工具Tensorboard的初步使用](https://blog.csdn.net/sinat_33761963/article/details/62433234)\n",
    "2. [TensorBoard 简介及使用流程](https://blog.csdn.net/gsww404/article/details/78605784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv",
   "language": "python",
   "name": "opencv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
