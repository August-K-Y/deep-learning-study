{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Momentum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Keeping track of gradients of previous rounds\n",
    "* Speeds up traing, huge performace gains, almost no work. 0.9 is usually fine\n",
    "* Similar to physical momentum - moves you in the direction you were already going\n",
    "* Two flavors:\n",
    "    * Regular momentum\n",
    "    * Nesterov Momentum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Momentum\n",
    "\n",
    "* Intuitively, the \"velocity\" is the previous weight change:\n",
    "\n",
    "$$v(t-1) = \\Delta w(t-1) $$\n",
    "\n",
    "* Then we can update the weight as follow:\n",
    "\n",
    "$$ \\Delta w(t) = \\mu v(t-1) - \\eta \\nabla J(t) $$\n",
    "\n",
    "* Combine the above two formula, we get momentum update formula:\n",
    "\n",
    "<b style=\"color:red\"> $$ \\Delta w(t) = \\mu \\Delta w(t-1) - \\eta \\nabla J(t) $$ </b>\n",
    "\n",
    "where $\\mu$ is the momentum parameter and $\\eta$ is the learning rate\n",
    "\n",
    "* Then, the formula of updating weight at time $t$ would be:\n",
    "\n",
    "<b style=\"color:red\"> $$ w = w + \\Delta w(t) $$ </b>\n",
    "\n",
    "** In code form:**\n",
    "\n",
    "```python\n",
    "# using gradient descent\n",
    "dw = 0 # initially\n",
    "\n",
    "v = dw\n",
    "dw = momentum * v - learning_rate * gradient\n",
    "w += dw\n",
    "```\n",
    "\n",
    "```python\n",
    "# using gradient ascent (need to verify)\n",
    "dw = 0 # initially\n",
    "\n",
    "v = dw\n",
    "dw = momentum * v + learning_rate * gradient\n",
    "w += dw\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Momentum\n",
    "\n",
    "* Basic idea: look ahead, then correct yourself if you made a mistake\n",
    "* Kinematics analogy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Variable Learning Rate\n",
    "\n",
    "* learning rate is a function of time e.g., $\\eta (t)$\n",
    "* sometimes, called \"learning rate scheduling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step decay\n",
    "\n",
    "* Periodically reduce the learning rate by a constant factor \n",
    " * Reduce every 5, 10, or 20 steps\n",
    " \n",
    "<img src=\"images/step_decay_pic.png\" alt=\"step_decay_pic\" height=\"40%\" width=\"40%\"/>\n",
    "\n",
    "\n",
    "```\n",
    "Ex.\n",
    "\n",
    "if iter % 10 == 0:\n",
    "    learning_rate /= 2\n",
    "```\n",
    "\n",
    "### Exponential decay\n",
    "\n",
    "* The learning rate follows an exponential curve:\n",
    "\n",
    "$$ \\eta (t) = A * exp(-kt) $$ \n",
    "\n",
    "<img src=\"images/ex_decay_pic.png\" alt=\"ex_decay_pic\" height=\"60%\" width=\"60%\"/>\n",
    "\n",
    "\n",
    "### 1/t decay\n",
    "\n",
    "* The learning rate decays proportionally to 1/t\n",
    "* Dropoff is slower than exponential decay\n",
    "* You can control the decay rate by changing the proportionality constant\n",
    "\n",
    "   $$ \\eta (t) = \\eta (0) / (1 + kt) $$\n",
    "   \n",
    "<img src=\"images/one_over_t_pic.png\" alt=\"one_over_t_pic\" height=\"60%\" width=\"60%\"/>\n",
    "\n",
    "\n",
    "### Take-away\n",
    "\n",
    "* The above methods have commons:\n",
    "    * Learning rate decreases with time\n",
    "        * Why we want this? \n",
    "> Initially, weights are far from optimal, we want start with a large learning rate so that we can take bigger steps towards the goal (this is the motivation behind momentum as well). However, as we get closer to the minimum, we want to slow down the learning rate to avoid overshooting in which case we may end up bouncing back and forth, or even bouncing out of the minimal valley.\n",
    "\n",
    "* All these methods add more hyperparameters to be optimized\n",
    "* Even choosing between these methods is a hyperparameter choice\n",
    "* Therefore, these techniques may be helpful. But, they also increase your work load\n",
    "\n",
    "### Babysitting method\n",
    "\n",
    "* Do a few epochs, see how it goes by examing the cost/loss graph\n",
    "* If the learning is too slow, we may increase the learning rate. If learning is starting to even out, we may decrease the learning rate\n",
    "* We should be careful about the plateau (a relatively flat portion of the surface), which may cause us temporiarily get stuck since nice monotonically decreasing curve is not guaranteed. But, we may end up getting a steep drop is we are patient enough (again, not guaranteed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adaptive Learning Rate\n",
    "\n",
    "* AdaGrad, \n",
    "* RMSprop, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### AdaGrad (Adaptive Gradient)\n",
    "\n",
    "* <b style=\"color:red\">Dependence of cost on each parameter is not the same</b>. Steep gradient in one direction, but flat in another\n",
    "* Adapt the learning rate for each parameter individually, based on how much \"learning\" it has done so far\n",
    "    * If the learning is too fast, it will slow down the learning.\n",
    "    * It is good for learning at later stage when we usually need slower learning rate.\n",
    "* adaptive method (based on weights changes so far)\n",
    "\n",
    "$$ cache = cache + gradient^2 $$\n",
    "\n",
    "$$  w  \\leftarrow w - \\eta {\\nabla J \\over \\sqrt{cache + \\epsilon}} $$\n",
    "\n",
    "Typically, $ \\epsilon $  is a small number, i.e. around 1E-8 or 1E-10, so we do not divide by 0\n",
    "\n",
    "* The cache is accumulating the squared gradients and it is updated at every batch assuming we are using batch gradient descent\n",
    "* Note, in AdaGrad, everything is element-wise. Each scalar parameter and its learning rate is updated independently of the others\n",
    "\n",
    "> Because each parameter has its own cache, if one parameter has had a lot of large gradients in the past, then its cache will be very large and its effective learning rate will be very small. Also, it will change more slowly in the future. On the other hand, if a parameter has had a lot of small gradient in the past, then its cache will be small and its effective learning rate will be large and thus it will have more opportunity to change in the future\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop (Root Mean Sqare Propagation)\n",
    "\n",
    "* An variation of AdaGrad. \n",
    "> It is motivated by the fact that AdaGrad decreases learning rate too aggressively so that the learning rate would approach zero too quickly when in fact there was still more learning to be done\n",
    "* The reason RMSprop decreases the learning rate too quickly is because the cache is growing too fast. In order to make the cache grows less fast, we decrease it each time we update it. We do it by computing <b style=\"color:red\">on-the-fly (online) moving average</b> of the old cache and the new squared gradient:\n",
    "\n",
    "$$ cache = decay * cache + (1 - decay) * gradient^2 $$\n",
    "\n",
    "where decay is a hyper-parameter that controls the how much weight is given in this average to older vs most recent $gradient^2$. Typically values for the decay is: 0.99, 0.999, etc.\n",
    "\n",
    "> Note, this formula is actually <b style=\"color:red\">exponentially-smoothed average</b> of the squared gradient. The \"cache\" is really estimating the mean of the the squared gradient: $ mean(gradient^2) $\n",
    "\n",
    "> Because we eventually use the square root of the cache, now we can see where RMSprop gets its name from (RMS = \"root mean square\")\n",
    " \n",
    "* We say the cache is \"leaky\", because:\n",
    "\n",
    "> If we had zero gradients for a long time, eventually the cache would shrink back down to zero since it would be decreased by the decay rate on each round \n",
    "\n",
    "* Then, we update the parameter\n",
    "\n",
    "$$  w  \\leftarrow w - \\eta {\\nabla J \\over \\sqrt{cache + \\epsilon}} $$\n",
    "\n",
    "Typically, epsilon is a small number, i.e. $10^{-10}$, so we do not divide by 0\n",
    "\n",
    "* The <b style=\"color:red\">effective learning</b> rate of the above formula is:\n",
    "\n",
    "$$ {\\eta  \\over \\sqrt{cache + \\epsilon}} $$\n",
    "\n",
    "\n",
    "** One ambiguity problem of RMSProp**\n",
    "\n",
    "What is the initial value of cache?\n",
    "\n",
    "Let's assume the initical value of cache is 0 and decay is 0.999, then we have the initial updated cache:\n",
    "\n",
    "$$ 0.001 g^2 $$\n",
    "\n",
    "Then initial update (ignoring epsilon):\n",
    "\n",
    "$$ {\\eta \\over \\sqrt{0.001 g^2}}g $$\n",
    "\n",
    "The effective learning rate actually becomes very large due to the fact that the denominator is very small. What we have to do is to compensate by making the initial learning rate smaller than usual.\n",
    "\n",
    "One solution: initialize <b style=\"color:red\">cache = 1</b>. Then, the effective learning rate is approximately equal to the initial learning rate divide by one:\n",
    "\n",
    "$$ \\Delta w = {\\eta \\over \\sqrt{1 + 0.001 g^2}}g \\approx \\eta g $$\n",
    "\n",
    "So, which initial value for cache (0 or 1) is correct? Major packages have implemented both:\n",
    "    * Tensorflow initializes cache = 1\n",
    "    * Keras initializes cache = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode\n",
    "\n",
    "**AdaGrad**\n",
    "\n",
    "```python\n",
    "# for each batch:\n",
    "cache = cache + gradient**2\n",
    "param = param - learning_rate * gradient / sqrt(cache + epsilon)\n",
    "```\n",
    "\n",
    "**RMSProp**\n",
    "\n",
    "```python\n",
    "# for each batch:\n",
    "cache = decay * cache + (1 - decay) * gradient**2\n",
    "param = param - learning_rate * gradient / sqrt(cache + epsilon)\n",
    "```\n",
    "epsilon = $ 10^{-8}, 10^{-9}, 10^{-10} $, etc ..., \n",
    "decay = 0.9, 0.99, 0.999, 0.9999. etc ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adam Optimizer\n",
    "\n",
    "* Adam optimizer deserves its own section since it is often considered the go-to default these day\n",
    "\n",
    "$$ v_t = decay * v_{t-1} + (1 - decay) * g^2_t \\approx mean(g^2) $$\n",
    "\n",
    "The mean of random variable is its expected value, e.g., mean(X) = E(X)\n",
    "* In particular, when we take the expected value of the square of a random variable, we call this the second moment: \n",
    "$$ E(X^2) =  \\text{2nd (raw) moment of X} $$\n",
    "* In general, the expected value of X to the n is the nth moment of X: \n",
    "$$ E(X^n) = \\text{nth (raw) moment of X} $$\n",
    "\n",
    "Then we know that:\n",
    "$$ v \\approx E(g^2) $$\n",
    "\n",
    "is the second moment of the gradient and calculate it using the exponentially-smoothed average.\n",
    "\n",
    "Similarly, we compute exponentially-smoothed average of $g$, which is the first moment of $g$:\n",
    "\n",
    "$$ m_t = decay * m_{t-1} + (1 - decay) * g_t \\approx mean(g) $$\n",
    "\n",
    "Then we know that:\n",
    "$$ m \\approx E(g) $$\n",
    "\n",
    "* \"m\" stands for \"mean\"\n",
    "* Adam makes use of the 1st and 2nd moments of g, which explains what \"Adam\" stands for: <b style=\"color:red\">\"Adaptive Moment Estimation\"</b>\n",
    "* Adam is a combination of these 2 things: m and v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Same issue as with RMSProp** - what is the initial value of the output of the exponentially-smoothed average? The output is supposed to depend on the last value of the output plus current value of the input. Since there is no last value of the output for the $t=0$, we typically set the output value at $t=0$ to zero. But, buy doing so, we then set the initial value of the output to something like 0.01 times the first input:\n",
    "\n",
    "$$ Y(0) = 0 $$\n",
    "$$ Y(1) = 0.99 * Y(0) + 0.01 * X(1) = 0.01 * X(1) $$\n",
    "\n",
    "assuming decay rate is 0.99\n",
    "Therefore, initially, the output is goint to be biased towards zero. To correct this bias, we do bias correction:\n",
    "\n",
    "$$  \\hat{Y}(t) = {Y(t) \\over {1 - decay^t}} $$\n",
    "\n",
    "Continue to calculate $Y(t)$ as normal\n",
    "\n",
    "When $t$ is small, we are dividing $Y(t)$ by a very small number that makes the initial value bigger. This make senses because the initial value was too small.\n",
    "\n",
    "Example:\n",
    "\n",
    "Given decay = 0.99\n",
    "\n",
    "$$ Y(1) = 0.01 * X(1) $$\n",
    "\n",
    "$$ Y(2) = 0.01 * 0.99 * X(1) + 0.01 * X(2) $$\n",
    "\n",
    "corrected:\n",
    "\n",
    "$$  \\hat{Y}(1) = {0.01 \\over {1 - 0.99^1}}X(1) = X(1)$$\n",
    "\n",
    "$$  \\hat{Y}(2) = {{0.0099*X(1) + 0.01*X(2)} \\over {1 - 0.99^2}} = 0.496*X(1) + 0.503*X(2) $$\n",
    "\n",
    "When $t$ approaches infinity, the bias correction approaches 1, so we quickly converge to the standard exponentially smooth average. The important part is that our output now has the correct range of values when compared with the input for all values of the input including the initial ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to Adam**\n",
    "\n",
    "We perform correction on both $m$ and $v$:\n",
    "\n",
    "$$  \\hat{m}_t = {m_t \\over {1 - \\beta^t_1}} $$\n",
    "\n",
    "$$  \\hat{v}_t = {v_t \\over {1 - \\beta^t_2}} $$\n",
    "\n",
    "Then, we have:\n",
    "\n",
    "$$  w_{t+1}  \\leftarrow w_t - \\eta {\\hat{m}_t  \\over \\sqrt{\\hat{v}_t  + \\epsilon}} $$\n",
    "\n",
    "Typicall values for decay rate $\\beta$: $0.9 ... 0.999$ range, while $\\epsilon$: $10^{-8} ... 10^{-10}$ range\n",
    "\n",
    "\n",
    "**Recap**\n",
    "* Adam is a modern adaptive learning rate technique that is default for many deep learning practitioners\n",
    "* Combines RMSProp (cache mechanism) with momentum (keeping track of old gradients)\n",
    "* Generalize as first and second moments\n",
    "* Apply bias correction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pseudocode for Adam Optimizer**\n",
    "\n",
    "$$ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t $$\n",
    "\n",
    "\n",
    "$$ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g^2_t $$\n",
    "\n",
    "\n",
    "$$  \\hat{m}_t = {m_t \\over {1 - \\beta^t_1}} $$\n",
    "\n",
    "\n",
    "$$  \\hat{v}_t = {v_t \\over {1 - \\beta^t_2}} $$\n",
    "\n",
    "\n",
    "$$  w_{t+1}  \\leftarrow w_t - \\eta {\\hat{m}_t  \\over \\sqrt{\\hat{v}_t  + \\epsilon}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Caveat\n",
    "\n",
    "* May 2017 paper: \"The Marginal Value of Adapive Gradient Method in Machine Learning\"\n",
    " * Experimental results show regular gradient descent performs better than these adaptive techniques\n",
    "* Stick to the fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  Practical Debugging Techniques\n",
    "\n",
    "### Learning Rate \n",
    "\n",
    "* Choose learning rate in log scale of factor 10. $ 10^{-1}, 10^{-2}, 10^{-3},..., etc $\n",
    "* How to determine whether learning rate is too high or too low\n",
    "    * Cost goes to infinity/NaN --> learning rate is too high\n",
    "        * Neural network will continue to train as normal, multiplying NaNs as if they were actual numbers\n",
    "    * Cost converges too slowly --> learning rate is too low\n",
    "        * Try increase learning rate by factor of 10\n",
    "* A good debugging technique:\n",
    "    * Try to turn off all modification except for regular gradient descent (no regularization, no momentum or anything else)\n",
    "    * Learning rate that is too high will mess things up. But learning rate that is too low will not. It will just train more slowly.\n",
    "    * If things still do not work, it could be a problem with your code.\n",
    "\n",
    "\n",
    "### Normalizing cost and regularization penalty\n",
    "\n",
    "* Learning rate will be sensitive to number of training points (i.e., batch size) because cost/loss is sum of individual errros. \n",
    "    * Divide cost/loss by batch size to normalize it\n",
    "* To make regularization penalty independent of number of parameters. \n",
    "    * Divide penalty by number of parameters.\n",
    "    \n",
    "    \n",
    "### Regularization too high\n",
    "\n",
    "* If the regularization parameter $\\lambda$ is too high (e.g., for some particular problem $\\lambda$ of 1 is too high), the error rate may hover around random guessing.\n",
    "* Sometimes, it is useful to turn off regularization completely and make learning rate very small. This may tell you whether the model actually works or whether there is a bug in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in and transforming data...\n",
      "(42000, 784)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_normalized_data():\n",
    "\n",
    "    print(\"Reading in and transforming data...\")\n",
    "\n",
    "    if not os.path.exists('../data/minst/train.csv'):\n",
    "        print('Looking for ../data/minst/train.csv')\n",
    "        print('You have not downloaded the data and/or not placed the files in the correct location.')\n",
    "        print('Please get the data from: https://www.kaggle.com/c/digit-recognizer')\n",
    "        print('Place train.csv in the folder large_files adjacent to the class folder')\n",
    "        exit()\n",
    "\n",
    "    df = pd.read_csv('../data/minst/train.csv')\n",
    "    data = df.as_matrix().astype(np.float32)\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # X is a matrix of all the samples excluding the first column which is label column\n",
    "    X = data[:, 1:]\n",
    "    mu = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    np.place(std, std == 0, 1)\n",
    "    X = (X - mu) / std # normalize the data\n",
    "    Y = data[:, 0]\n",
    "    return X, Y\n",
    "\n",
    "X, Y = get_normalized_data()\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax(sigmoid(X*W1 + b1)*W2 + b2)\n",
    "def forward(X, W1, b1, W2, b2):\n",
    "#     Z = sigmoid(X.dot(W1) + b1)\n",
    "    Z = relu(X.dot(W1) + b1)\n",
    "    Y = softmax(Z.dot(W2) + b2)  \n",
    "    return Y, Z\n",
    "\n",
    "def relu(A):\n",
    "    A[A<0] = 0\n",
    "    return A\n",
    "    \n",
    "def sigmoid(A):\n",
    "    return 1 / (1 + np.exp(-A))\n",
    "\n",
    "def softmax(A):\n",
    "    expA = np.exp(A)\n",
    "    return expA / expA.sum(axis=1, keepdims=True)\n",
    "\n",
    "def derivative_W2(T, Y, Z):\n",
    "    return Z.T.dot(Y - T)\n",
    "\n",
    "def derivative_b2(T, Y):\n",
    "    return (Y - T).sum(axis=0)\n",
    "    \n",
    "def derivative_W1(T, Y, W2, Z, X):\n",
    "#     dA = (Y - T).dot(W2.T) * Z * (1 - Z)\n",
    "    dA = (Y - T).dot(W2.T) * (Z > 0)\n",
    "    return X.T.dot(dA)\n",
    "\n",
    "def derivative_b1(T, Y, W2, Z):\n",
    "#     dz = (Y - T).dot(W2.T) * Z * (1 - Z)\n",
    "    dz = (Y - T).dot(W2.T) * (Z > 0)\n",
    "    return dz.sum(axis=0)\n",
    "\n",
    "def y2indicator(y, k):\n",
    "    N = len(y)\n",
    "    y = y.astype(np.int32)\n",
    "    ind = np.zeros((N, k))\n",
    "    for i in range(N):\n",
    "        ind[i, y[i]] = 1\n",
    "    return ind\n",
    "\n",
    "def predict(p_y):\n",
    "    return np.argmax(p_y, axis=1)\n",
    "\n",
    "def error_rate(p_y, t):\n",
    "    prediction = predict(p_y)\n",
    "    return np.mean(prediction != t)\n",
    "\n",
    "def cross_entropy(p_y, t):\n",
    "    tot = t * np.log(p_y)\n",
    "    return -tot.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientDescentOptimizer(Xbatch, Ybatch, pYbatch, Z, W1, b1, W2, b2, \n",
    "                             reg=0.01, learning_rate=0.001):\n",
    "\n",
    "    # pYbatch, Ybatch, Z, \n",
    "    W2 -= learning_rate * (derivative_W2(Ybatch, pYbatch, Z) + reg * W2)\n",
    "    b2 -= learning_rate * (derivative_b2(Ybatch, pYbatch) + reg * b2)\n",
    "    W1 -= learning_rate * (derivative_W1(Ybatch, pYbatch, W2, Z, Xbatch) + reg * W1)\n",
    "    b1 -= learning_rate * (derivative_b1(Ybatch, pYbatch, W2, Z) + reg * b1)\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSPropOptimizer(Xbatch, Ybatch, pYbatch, Z, W1, b1, W2, b2, \n",
    "                     cache_W1, cache_b1, cache_W2, cache_b2, reg=0.01, learning_rate=0.001):\n",
    "\n",
    "    eps = 0.000001\n",
    "    decay_rate = 0.999\n",
    "    \n",
    "    # calculate gradient descent\n",
    "    gW2 = derivative_W2(Ybatch, pYbatch, Z) + reg * W2\n",
    "    # update cache\n",
    "    cache_W2 = decay_rate * cache_W2 + (1 - decay_rate) * gW2 * gW2\n",
    "    # udpate weight\n",
    "    W2 -= learning_rate * gW2 / (np.sqrt(cache_W2) + eps)\n",
    "\n",
    "    gb2 = derivative_b2(Ybatch, pYbatch) + reg * b2\n",
    "    cache_b2 = decay_rate * cache_b2 + (1 - decay_rate) * gb2 * gb2\n",
    "    b2 -= learning_rate * gb2 / (np.sqrt(cache_b2) + eps)\n",
    "\n",
    "    gW1 = derivative_W1(Ybatch, pYbatch, W2, Z, Xbatch) + reg * W1\n",
    "    cache_W1 = decay_rate * cache_W1 + (1 - decay_rate) * gW1 * gW1\n",
    "    W1 -= learning_rate * gW1 / (np.sqrt(cache_W1) + eps)\n",
    "\n",
    "    gb1 = derivative_b1(Ybatch, pYbatch, W2, Z) + reg * b1\n",
    "    cache_b1 = decay_rate * cache_b1 + (1 - decay_rate) * gb1 * gb1\n",
    "    b1 -= learning_rate * gb1 / (np.sqrt(cache_b1) + eps)\n",
    "\n",
    "    return W1, b1, W2, b2, cache_W1, cache_b1, cache_W2, cache_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdamOptimizer(Xbatch, Ybatch, pYbatch, Z, W1, b1, W2, b2, \n",
    "    mW1, vW1, mb1, vb1, mW2, vW2, mb2, vb2, t, reg=0.01, learning_rate=0.001):\n",
    "\n",
    "    eps = 1e-8\n",
    "    beta_m = 0.9\n",
    "    beta_v = 0.999\n",
    "\n",
    "    # gradients\n",
    "    gW2 = derivative_W2(Ybatch, pYbatch, Z) + reg * W2\n",
    "    gb2 = derivative_b2(Ybatch, pYbatch) + reg * b2\n",
    "    gW1 = derivative_W1(Ybatch, pYbatch, W2, Z, Xbatch) + reg * W1\n",
    "    gb1 = derivative_b1(Ybatch, pYbatch, W2, Z) + reg * b1\n",
    "\n",
    "    # new m\n",
    "    mW2 = beta_m * mW2 + (1 - beta_m) * gW2\n",
    "    mb2 = beta_m * mb2 + (1 - beta_m) * gb2\n",
    "    mW1 = beta_m * mW1 + (1 - beta_m) * gW1\n",
    "    mb1 = beta_m * mb1 + (1 - beta_m) * gb1\n",
    "\n",
    "    # new v\n",
    "    vW2 = beta_v * vW2 + (1 - beta_v) * gW2 * gW2\n",
    "    vb2 = beta_v * vb2 + (1 - beta_v) * gb2 * gb2\n",
    "    vW1 = beta_v * vW1 + (1 - beta_v) * gW1 * gW1\n",
    "    vb1 = beta_v * vb1 + (1 - beta_v) * gb1 * gb1\n",
    "\n",
    "    # correction\n",
    "    correction_m = 1 - beta_m ** t\n",
    "    corr_mW2 = mW2 / correction_m\n",
    "    corr_mb2 = mb2 / correction_m\n",
    "    corr_mW1 = mW1 / correction_m\n",
    "    corr_mb1 = mb1 / correction_m\n",
    "\n",
    "    correction_v = 1 - beta_v ** t\n",
    "    corr_vW2 = vW2 / correction_v\n",
    "    corr_vb2 = vb2 / correction_v\n",
    "    corr_vW1 = vW1 / correction_v\n",
    "    corr_vb1 = vb1 / correction_v\n",
    "\n",
    "    # apply updates to the params\n",
    "    W2 -= learning_rate * corr_mW2 / np.sqrt(corr_vW2 + eps)\n",
    "    b2 -= learning_rate * corr_mb2 / np.sqrt(corr_vb2 + eps)\n",
    "    W1 -= learning_rate * corr_mW1 / np.sqrt(corr_vW1 + eps)\n",
    "    b1 -= learning_rate * corr_mb1 / np.sqrt(corr_vb1 + eps)\n",
    "\n",
    "    return W1, b1, W2, b2, mW1, vW1, mb1, vb1, mW2, vW2, mb2, vb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_mnist_model(X, Y, opt='Gradient', learning_rate=0.001):\n",
    "\n",
    "    max_iter = 20\n",
    "    print_period = 40\n",
    "\n",
    "    reg = 0.01\n",
    "\n",
    "    Xtrain = X[:-1000,]\n",
    "    Ytrain = Y[:-1000]\n",
    "\n",
    "    Xtest = X[-1000:,]\n",
    "    Ytest = Y[-1000:]\n",
    "\n",
    "    # get number of classes\n",
    "    K = len(set(Y))\n",
    "\n",
    "    Ytrain_ind = y2indicator(Ytrain, K)\n",
    "    Ytest_ind = y2indicator(Ytest, K)\n",
    "\n",
    "    N, D = Xtrain.shape\n",
    "    batch_size = 500\n",
    "    n_batches = N / batch_size\n",
    "\n",
    "    M = 300\n",
    "    \n",
    "    # here normalizing weights is important\n",
    "    W1 = np.random.randn(D, M) / np.sqrt(D)\n",
    "    b1 = np.zeros(M)\n",
    "    W2 = np.random.randn(M, K) / np.sqrt(M)\n",
    "    b2 = np.zeros(K)\n",
    "\n",
    "    cache_W2 = 0\n",
    "    cache_b2 = 0\n",
    "    cache_W1 = 0\n",
    "    cache_b1 = 0\n",
    "    \n",
    "    vW1 = 0\n",
    "    vb1 = 0\n",
    "    vW2 = 0\n",
    "    vb2 = 0\n",
    "\n",
    "    LL_batch = []\n",
    "    CR_batch = []\n",
    "\n",
    "    for itr in range(max_iter):\n",
    "        t = 0\n",
    "        for start_b in range(0, N, batch_size):\n",
    "            t += 1\n",
    "            end_b = start_b + batch_size\n",
    "\n",
    "            Xbatch = Xtrain[start_b:end_b]\n",
    "            Ybatch = Ytrain_ind[start_b:end_b]\n",
    "\n",
    "            pYbatch, Z = forward(Xbatch, W1, b1, W2, b2)\n",
    "\n",
    "            if opt == 'Gradient':\n",
    "                W1, b1, W2, b2 = GradientDescentOptimizer(Xbatch, Ybatch, pYbatch, Z, \n",
    "                                                          W1, b1, W2, b2, reg, learning_rate)\n",
    "            elif opt == 'RMSprop':\n",
    "\n",
    "                W1, b1, W2, b2, cache_W1, cache_b1, cache_W2, cache_b2 = RMSPropOptimizer(\n",
    "                                                          Xbatch, Ybatch, pYbatch, Z, \n",
    "                                                          W1, b1, W2, b2, \n",
    "                                                          cache_W1, cache_b1, cache_W2, cache_b2,\n",
    "                                                          reg, learning_rate)\n",
    "            elif opt == 'Adam':\n",
    "                W1, b1, W2, b2, cache_W1, vW1, cache_b1, vb1, cache_W2, vW2, cache_b2, vb2 = AdamOptimizer(Xbatch, Ybatch,\n",
    "                                        pYbatch, Z, W1, b1, W2, b2,\n",
    "                                        cache_W1, vW1, cache_b1, vb1, cache_W2, vW2, cache_b2, vb2, \n",
    "                                        t, reg, learning_rate)\n",
    "\n",
    "            if t % print_period == 0:\n",
    "                pY, _ = forward(Xtest, W1, b1, W2, b2)\n",
    "\n",
    "                ll = cross_entropy(pY, Ytest_ind)\n",
    "                LL_batch.append(ll)\n",
    "                print(\"Cost at iteration itr=%d, j=%d: %.6f\" % (itr, t, ll))\n",
    "\n",
    "                err = error_rate(pY, Ytest)\n",
    "                CR_batch.append(err)\n",
    "                print(\"Error, rate:\", err)\n",
    "\n",
    "    pY, _ = forward(Xtest, W1, b1, W2, b2)\n",
    "    print(\"Final error rate:\", error_rate(pY, Ytest))\n",
    "    return LL_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration itr=0, j=40: 659.296555\n",
      "Error, rate: 0.161\n",
      "Cost at iteration itr=0, j=80: 462.043248\n",
      "Error, rate: 0.112\n",
      "Cost at iteration itr=1, j=40: 387.758834\n",
      "Error, rate: 0.1\n",
      "Cost at iteration itr=1, j=80: 346.104513\n",
      "Error, rate: 0.083\n",
      "Cost at iteration itr=2, j=40: 321.032675\n",
      "Error, rate: 0.08\n",
      "Cost at iteration itr=2, j=80: 301.379749\n",
      "Error, rate: 0.073\n",
      "Cost at iteration itr=3, j=40: 288.492719\n",
      "Error, rate: 0.073\n",
      "Cost at iteration itr=3, j=80: 276.459048\n",
      "Error, rate: 0.068\n",
      "Cost at iteration itr=4, j=40: 267.806303\n",
      "Error, rate: 0.067\n",
      "Cost at iteration itr=4, j=80: 259.774197\n",
      "Error, rate: 0.068\n",
      "Cost at iteration itr=5, j=40: 253.102836\n",
      "Error, rate: 0.065\n",
      "Cost at iteration itr=5, j=80: 247.073147\n",
      "Error, rate: 0.065\n",
      "Cost at iteration itr=6, j=40: 241.697600\n",
      "Error, rate: 0.064\n",
      "Cost at iteration itr=6, j=80: 237.036072\n",
      "Error, rate: 0.063\n",
      "Cost at iteration itr=7, j=40: 232.491991\n",
      "Error, rate: 0.062\n",
      "Cost at iteration itr=7, j=80: 228.673100\n",
      "Error, rate: 0.061\n",
      "Cost at iteration itr=8, j=40: 224.793473\n",
      "Error, rate: 0.061\n",
      "Cost at iteration itr=8, j=80: 221.567188\n",
      "Error, rate: 0.059\n",
      "Cost at iteration itr=9, j=40: 218.270239\n",
      "Error, rate: 0.059\n",
      "Cost at iteration itr=9, j=80: 215.538421\n",
      "Error, rate: 0.058\n",
      "Cost at iteration itr=10, j=40: 212.664386\n",
      "Error, rate: 0.057\n",
      "Cost at iteration itr=10, j=80: 210.309003\n",
      "Error, rate: 0.057\n",
      "Cost at iteration itr=11, j=40: 207.768756\n",
      "Error, rate: 0.055\n",
      "Cost at iteration itr=11, j=80: 205.669451\n",
      "Error, rate: 0.053\n",
      "Cost at iteration itr=12, j=40: 203.443142\n",
      "Error, rate: 0.054\n",
      "Cost at iteration itr=12, j=80: 201.571605\n",
      "Error, rate: 0.05\n",
      "Cost at iteration itr=13, j=40: 199.595216\n",
      "Error, rate: 0.051\n",
      "Cost at iteration itr=13, j=80: 197.901140\n",
      "Error, rate: 0.05\n",
      "Cost at iteration itr=14, j=40: 196.169796\n",
      "Error, rate: 0.052\n",
      "Cost at iteration itr=14, j=80: 194.637463\n",
      "Error, rate: 0.05\n",
      "Cost at iteration itr=15, j=40: 193.087025\n",
      "Error, rate: 0.051\n",
      "Cost at iteration itr=15, j=80: 191.729530\n",
      "Error, rate: 0.05\n",
      "Cost at iteration itr=16, j=40: 190.327820\n",
      "Error, rate: 0.05\n",
      "Cost at iteration itr=16, j=80: 189.127484\n",
      "Error, rate: 0.047\n",
      "Cost at iteration itr=17, j=40: 187.871128\n",
      "Error, rate: 0.049\n",
      "Cost at iteration itr=17, j=80: 186.790633\n",
      "Error, rate: 0.045\n",
      "Cost at iteration itr=18, j=40: 185.696022\n",
      "Error, rate: 0.048\n",
      "Cost at iteration itr=18, j=80: 184.689086\n",
      "Error, rate: 0.045\n",
      "Cost at iteration itr=19, j=40: 183.709771\n",
      "Error, rate: 0.046\n",
      "Cost at iteration itr=19, j=80: 182.793854\n",
      "Error, rate: 0.045\n",
      "Final error rate: 0.045\n"
     ]
    }
   ],
   "source": [
    "# learning_rate = 0.00004\n",
    "costs_G = mlp_mnist_model(X ,Y, 'Gradient', learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration itr=0, j=40: 294.643615\n",
      "Error, rate: 0.075\n",
      "Cost at iteration itr=0, j=80: 219.278834\n",
      "Error, rate: 0.063\n",
      "Cost at iteration itr=1, j=40: 200.270563\n",
      "Error, rate: 0.052\n",
      "Cost at iteration itr=1, j=80: 183.264379\n",
      "Error, rate: 0.051\n",
      "Cost at iteration itr=2, j=40: 175.592798\n",
      "Error, rate: 0.046\n",
      "Cost at iteration itr=2, j=80: 168.311616\n",
      "Error, rate: 0.046\n",
      "Cost at iteration itr=3, j=40: 164.357579\n",
      "Error, rate: 0.041\n",
      "Cost at iteration itr=3, j=80: 158.878251\n",
      "Error, rate: 0.041\n",
      "Cost at iteration itr=4, j=40: 156.851223\n",
      "Error, rate: 0.04\n",
      "Cost at iteration itr=4, j=80: 151.942987\n",
      "Error, rate: 0.041\n",
      "Cost at iteration itr=5, j=40: 151.314832\n",
      "Error, rate: 0.039\n",
      "Cost at iteration itr=5, j=80: 147.068813\n",
      "Error, rate: 0.038\n",
      "Cost at iteration itr=6, j=40: 146.977741\n",
      "Error, rate: 0.038\n",
      "Cost at iteration itr=6, j=80: 143.627292\n",
      "Error, rate: 0.039\n",
      "Cost at iteration itr=7, j=40: 144.081424\n",
      "Error, rate: 0.037\n",
      "Cost at iteration itr=7, j=80: 140.976081\n",
      "Error, rate: 0.037\n",
      "Cost at iteration itr=8, j=40: 141.820586\n",
      "Error, rate: 0.037\n",
      "Cost at iteration itr=8, j=80: 139.013448\n",
      "Error, rate: 0.037\n",
      "Cost at iteration itr=9, j=40: 139.911989\n",
      "Error, rate: 0.036\n",
      "Cost at iteration itr=9, j=80: 137.337111\n",
      "Error, rate: 0.037\n",
      "Cost at iteration itr=10, j=40: 138.448388\n",
      "Error, rate: 0.036\n",
      "Cost at iteration itr=10, j=80: 136.180965\n",
      "Error, rate: 0.037\n",
      "Cost at iteration itr=11, j=40: 137.421249\n",
      "Error, rate: 0.036\n",
      "Cost at iteration itr=11, j=80: 135.316557\n",
      "Error, rate: 0.037\n",
      "Cost at iteration itr=12, j=40: 136.512448\n",
      "Error, rate: 0.034\n",
      "Cost at iteration itr=12, j=80: 134.567315\n",
      "Error, rate: 0.037\n",
      "Cost at iteration itr=13, j=40: 135.958897\n",
      "Error, rate: 0.035\n",
      "Cost at iteration itr=13, j=80: 134.254702\n",
      "Error, rate: 0.038\n",
      "Cost at iteration itr=14, j=40: 135.492883\n",
      "Error, rate: 0.035\n",
      "Cost at iteration itr=14, j=80: 133.956175\n",
      "Error, rate: 0.038\n",
      "Cost at iteration itr=15, j=40: 135.121818\n",
      "Error, rate: 0.036\n",
      "Cost at iteration itr=15, j=80: 133.838661\n",
      "Error, rate: 0.038\n",
      "Cost at iteration itr=16, j=40: 134.957841\n",
      "Error, rate: 0.037\n",
      "Cost at iteration itr=16, j=80: 133.678466\n",
      "Error, rate: 0.038\n",
      "Cost at iteration itr=17, j=40: 134.968757\n",
      "Error, rate: 0.037\n",
      "Cost at iteration itr=17, j=80: 133.747598\n",
      "Error, rate: 0.038\n",
      "Cost at iteration itr=18, j=40: 134.897517\n",
      "Error, rate: 0.036\n",
      "Cost at iteration itr=18, j=80: 133.752462\n",
      "Error, rate: 0.037\n",
      "Cost at iteration itr=19, j=40: 134.931287\n",
      "Error, rate: 0.035\n",
      "Cost at iteration itr=19, j=80: 133.869628\n",
      "Error, rate: 0.038\n",
      "Final error rate: 0.038\n"
     ]
    }
   ],
   "source": [
    "costs_A = mlp_mnist_model(X ,Y, 'Adam', learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration itr=0, j=40: 305.151067\n",
      "Error, rate: 0.065\n",
      "Cost at iteration itr=0, j=80: 203.843230\n",
      "Error, rate: 0.055\n",
      "Cost at iteration itr=1, j=40: 176.585413\n",
      "Error, rate: 0.044\n",
      "Cost at iteration itr=1, j=80: 171.853048\n",
      "Error, rate: 0.044\n",
      "Cost at iteration itr=2, j=40: 155.038483\n",
      "Error, rate: 0.041\n",
      "Cost at iteration itr=2, j=80: 158.899569\n",
      "Error, rate: 0.041\n",
      "Cost at iteration itr=3, j=40: 146.765610\n",
      "Error, rate: 0.037\n",
      "Cost at iteration itr=3, j=80: 150.551619\n",
      "Error, rate: 0.04\n",
      "Cost at iteration itr=4, j=40: 143.089637\n",
      "Error, rate: 0.035\n",
      "Cost at iteration itr=4, j=80: 144.598539\n",
      "Error, rate: 0.039\n",
      "Cost at iteration itr=5, j=40: 140.983015\n",
      "Error, rate: 0.035\n",
      "Cost at iteration itr=5, j=80: 141.820930\n",
      "Error, rate: 0.038\n",
      "Cost at iteration itr=6, j=40: 140.490388\n",
      "Error, rate: 0.035\n",
      "Cost at iteration itr=6, j=80: 139.790070\n",
      "Error, rate: 0.036\n",
      "Cost at iteration itr=7, j=40: 140.464524\n",
      "Error, rate: 0.036\n",
      "Cost at iteration itr=7, j=80: 139.480166\n",
      "Error, rate: 0.038\n",
      "Cost at iteration itr=8, j=40: 141.252533\n",
      "Error, rate: 0.036\n",
      "Cost at iteration itr=8, j=80: 139.995497\n",
      "Error, rate: 0.038\n",
      "Cost at iteration itr=9, j=40: 141.883791\n",
      "Error, rate: 0.036\n",
      "Cost at iteration itr=9, j=80: 140.885434\n",
      "Error, rate: 0.038\n",
      "Cost at iteration itr=10, j=40: 142.847611\n",
      "Error, rate: 0.035\n",
      "Cost at iteration itr=10, j=80: 141.398609\n",
      "Error, rate: 0.035\n",
      "Cost at iteration itr=11, j=40: 143.212098\n",
      "Error, rate: 0.036\n",
      "Cost at iteration itr=11, j=80: 142.385366\n",
      "Error, rate: 0.034\n",
      "Cost at iteration itr=12, j=40: 143.522431\n",
      "Error, rate: 0.035\n",
      "Cost at iteration itr=12, j=80: 142.920683\n",
      "Error, rate: 0.034\n",
      "Cost at iteration itr=13, j=40: 143.858301\n",
      "Error, rate: 0.035\n",
      "Cost at iteration itr=13, j=80: 144.204413\n",
      "Error, rate: 0.034\n",
      "Cost at iteration itr=14, j=40: 144.685134\n",
      "Error, rate: 0.035\n",
      "Cost at iteration itr=14, j=80: 145.091176\n",
      "Error, rate: 0.034\n",
      "Cost at iteration itr=15, j=40: 145.018869\n",
      "Error, rate: 0.035\n",
      "Cost at iteration itr=15, j=80: 145.988099\n",
      "Error, rate: 0.034\n",
      "Cost at iteration itr=16, j=40: 145.600787\n",
      "Error, rate: 0.035\n",
      "Cost at iteration itr=16, j=80: 146.839993\n",
      "Error, rate: 0.034\n",
      "Cost at iteration itr=17, j=40: 146.317874\n",
      "Error, rate: 0.035\n",
      "Cost at iteration itr=17, j=80: 147.269208\n",
      "Error, rate: 0.033\n",
      "Cost at iteration itr=18, j=40: 146.998332\n",
      "Error, rate: 0.034\n",
      "Cost at iteration itr=18, j=80: 147.806592\n",
      "Error, rate: 0.033\n",
      "Cost at iteration itr=19, j=40: 147.512825\n",
      "Error, rate: 0.032\n",
      "Cost at iteration itr=19, j=80: 148.341937\n",
      "Error, rate: 0.032\n",
      "Final error rate: 0.029\n"
     ]
    }
   ],
   "source": [
    "costs_R = mlp_mnist_model(X ,Y, 'RMSprop', learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106e4d278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(costs_G, label='Gradient')\n",
    "plt.plot(costs_A, label='Adam')\n",
    "plt.plot(costs_R, label='RMSprop')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
