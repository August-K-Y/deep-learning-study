{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systematically Tuning Hyperparameters\n",
    "\n",
    "* Many hyperparameters to optimize\n",
    "    * learning rate / decay rate\n",
    "    * momentum\n",
    "    * regularization\n",
    "    * hidden layer size\n",
    "    * number of hidden layers\n",
    "* Need ways to automate this\n",
    "    * Cross Validation, [sklearn cross validation source code](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/cross_validation.py)\n",
    "    * Grid/Random Search, [sklearn grid search source code](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/grid_search.py)\n",
    "* In deep learning, usually, we have very large data. Thus, we just split the data once, int one training set and on validation set. This is occasionally referred to cross-validation as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "* General way to choose hyperparameters\n",
    "    * e.g., we want to know if 4 vs. 5 hidden units better\n",
    "        * Do cross-validation on both, choose the one with the best accuracy\n",
    "        * \"Best\" could be defined as \"statistically significantly better\" if you are into statistics\n",
    "* We do not want to get \"perfect\" accuracy on training data\n",
    "    * Data = signal + noise/outliers\n",
    "    * We want to fit to signal, not noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Split up your data **\n",
    "* Train - train on this data\n",
    "* Validation - validate on this data\n",
    "* Test - try not to touch this data until the very end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** K-Fold Cross Validation **\n",
    "* Split data into K parts, do K iteration - validate on one part, train on the other K - 1 parts\n",
    "* Average teh K scores, choose parameters with highest average score\n",
    "* Keep the test set out of here, \"test\" on validation set\n",
    "* You can use <b style='color:red'>sklearn.cross_validation.KFold</b>\n",
    "* In deep learning, usually data is so big, we just split the data once into one training set and on validation set.\n",
    "* In code\n",
    "\n",
    "```python\n",
    "def crossValidation(model, X, Y, K=5):\n",
    "    X, Y = shuffle(X, Y)\n",
    "    size = len(Y) / K\n",
    "    scores = []\n",
    "    for k in range(K):\n",
    "        xtrain = np.concatenate([ X[:k*size, :], X[(k*size + size):, :] ])\n",
    "        ytrain = np.concatenate([ Y[:k*size], Y[(k*size + size):] ])\n",
    "        xtest = X[k*size:(k*size + size), :]\n",
    "        ytest = Y[k*size:(k*size + size)]\n",
    "        \n",
    "        model.fit(xtrain, ytrain)\n",
    "        score = model.score(xtest, ytest)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "```\n",
    "* Can do statistical test to compare (i.e., T-test), there are Scipy functions for these\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "* <b style='color:red'>Exhaustive / try every combination</b>\n",
    "* May every slow\n",
    "    * You can each independent\n",
    "    * Good opportunity for parallelization\n",
    "    * Hadoop / Spark\n",
    "\n",
    "\n",
    "```python\n",
    "params = {\"learning_rate\" : [0.1, 0.001, 0.0001, 0.00001],\n",
    "              \"momentums\" : [1, 0.1, 0.01, 0.001],\n",
    "              \"regularizations\" : [1, 0.1, 0.01]}\n",
    "  \n",
    "GridSearch(model, X, Y, params):\n",
    "    for lr in params[\"learning_rate\"]:\n",
    "        for mu in params[\"momentums\"]:\n",
    "            for reg in params[\"regularization\"]:\n",
    "                score = cross_validation(lr, mu, reg, data)\n",
    "    \n",
    "```\n",
    "\n",
    "\n",
    "<img src=\"images/model_complexity.png\" alt=\"Drawing\" style=\"width:60%;height:60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "* Instead of looking at every possiblility, move in random directions until score is improved\n",
    "    * Fine to coarse strategy\n",
    "    * Coarse to fine strategy\n",
    "* In pseudo code\n",
    "\n",
    "```python\n",
    "theta = random position in hyperparameter space\n",
    "score_1 = cross_validation(theta, data)\n",
    "for i in range(max_iterations):\n",
    "    next_theta = sample from hypersphere around theta\n",
    "    score_2 = cross_valication(next_theta, data)\n",
    "    if score_2 is better then score_1:\n",
    "        theta = next_theta\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Logarithmically\n",
    "* We know that the difference between learning rates 0.001 and 0.0011 is not that significant\n",
    "* We would rather try numbers on a log scale\n",
    "    * e.g., $10^{-2}, 10^{-3}, 10^{-4}$, etc for learning rate\n",
    "        * Sample uniformly from (-2, -4) (or whatever limits you want)\n",
    "        * R ~ U(-2, -4)\n",
    "        * learning rate = $10^{R}$\n",
    "    * e.g., 0.9, 0.99, 0.999, etc for decay \n",
    "        * R ~ U(-1, -3)\n",
    "        * decay rate = $1 - 10^{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid/Random Search in Code\n",
    "\n",
    "### Get Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spiral():\n",
    "    # Idea: radius -> low...high\n",
    "    #           (don't start at 0, otherwise points will be \"mushed\" at origin)\n",
    "    #       angle = low...high proportional to radius\n",
    "    #               [0, 2pi/6, 4pi/6, ..., 10pi/6] --> [pi/2, pi/3 + pi/2, ..., ]\n",
    "    # x = rcos(theta), y = rsin(theta) as usual\n",
    "\n",
    "    radius = np.linspace(1, 10, 100)\n",
    "    thetas = np.empty((6, 100))\n",
    "    for i in range(6):\n",
    "        start_angle = np.pi*i / 3.0\n",
    "        end_angle = start_angle + np.pi / 2\n",
    "        points = np.linspace(start_angle, end_angle, 100)\n",
    "        thetas[i] = points\n",
    "\n",
    "    # convert into cartesian coordinates\n",
    "    x1 = np.empty((6, 100))\n",
    "    x2 = np.empty((6, 100))\n",
    "    for i in range(6):\n",
    "        x1[i] = radius * np.cos(thetas[i])\n",
    "        x2[i] = radius * np.sin(thetas[i])\n",
    "\n",
    "    # inputs\n",
    "    X = np.empty((600, 2))\n",
    "    X[:,0] = x1.flatten()\n",
    "    X[:,1] = x2.flatten()\n",
    "\n",
    "    # add noise\n",
    "    X += np.random.randn(600, 2)*0.5\n",
    "\n",
    "    # targets\n",
    "    Y = np.array([0]*100 + [1]*100 + [0]*100 + [1]*100 + [0]*100 + [1]*100)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'theano_ann'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0e68d199d245>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano_ann\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mANN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'theano_ann'"
     ]
    }
   ],
   "source": [
    "from theano_ann import ANN\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_spiral()\n",
    "X, Y = shuffle(X, Y)\n",
    "Ntrain = int(0.7 * len(X))\n",
    "\n",
    "Xtrain, Ytrain = X[:Ntrain], Y[:Ntrain]\n",
    "Xtest, Ytest = X[Ntrain:], Y[Ntrain:]\n",
    "\n",
    "hidden_layer_sizes = []\n",
    "learning_rate = []\n",
    "l2_penalties = []\n",
    "\n",
    "best_validation_rate = 0;\n",
    "best_hls = None;\n",
    "best_lr = None;\n",
    "best_l2 = None\n",
    "for hls in hidden_layer_sizes:\n",
    "    for lr in learning_rate:\n",
    "        for l2 in l2_penalties:\n",
    "            model = ANN(hls)\n",
    "            model.fit(Xtrain, Ytrain, learning_rate=lr, reg=l2, mu=0.99, epochs=3000, show_fig=False)\n",
    "            validation_accuracy = model.score(Xtest, Ytest)\n",
    "            if validation_accuracy > best_validation_rate:\n",
    "                best_validation_rate = validation_accuracy\n",
    "                best_hls = hls\n",
    "                best_lr = lr\n",
    "                best_l2 = l2\n",
    "                \n",
    "print(\"Best validation_accuracy:\", best_validation_rate)\n",
    "print(\"Best settings:\")\n",
    "print(\"hidden_layer_sizes:\", best_hls)\n",
    "print(\"learning_rate:\", best_lr)\n",
    "print(\"l2:\", best_l2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_spiral()\n",
    "X, Y = shuffle(X, Y)\n",
    "Ntrain = int(0.7 * len(X))\n",
    "\n",
    "Xtrain, Ytrain = X[:Ntrain], Y[:Ntrain]\n",
    "Xtest, Ytest = X[Ntrain:], Y[Ntrain:]\n",
    "\n",
    "M = 20\n",
    "# numberof hidden layers\n",
    "nHidden = 2\n",
    "log_lr = -4\n",
    "log_l2 = -2\n",
    "max_tries = 30\n",
    "\n",
    "best_validation_rate = 0\n",
    "best_M = None\n",
    "best_nHidden = None\n",
    "best_log_lr = None\n",
    "best_log_l2 = None\n",
    "\n",
    "for itr in range(max_tries):\n",
    "    model = ANN( [[M]]*nHidden)\n",
    "    model.fit(Xtrain, Ytrain, learning_rate=10**log_lr, reg=10**log_l2, mu=0.99, epochs=3000, show_fig=False)\n",
    "    validation_accuracy = model.score(Xtest, Ytest)\n",
    "    if validation_accuracy > best_validation_rate:\n",
    "        best_validation_rate = validation_accuracy\n",
    "        best_M = M\n",
    "        best_nHidden = nHidden\n",
    "        best_log_lr = lr\n",
    "        best_log_l2 = l2\n",
    "        \n",
    "    # get random number -1, 0 or 1   \n",
    "    # Note that each hyperparameter should draw the random number independently\n",
    "    # from the other since the search for each hyperparameter should go independently\n",
    "    nHidden = best_nHidden + np.random.randint(-1,2)\n",
    "    nHidden = max(1, nHidden)\n",
    "    M = best_M + np.random.randint(-1,2)*10\n",
    "    M = max(10, M)\n",
    "    log_lr = best_lr + np.random.randint(-1,2)\n",
    "    log_l2 = best_l2 + np.random.randint(-1,2)\n",
    "                \n",
    "print(\"Best validation_accuracy:\", best_validation_rate)\n",
    "print(\"Best settings:\")\n",
    "print(\"hidden_M:\", best_M)\n",
    "print(\"hidden_layer_sizes:\", best_hls)\n",
    "print(\"learning_rate:\", best_lr)\n",
    "print(\"l2:\", best_l2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
