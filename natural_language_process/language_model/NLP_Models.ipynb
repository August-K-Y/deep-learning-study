{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model represents observations in a form that allows us understand them better and predict new occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A language model captures the distributional statistics of words. In its most basic form, we take each unique word in a corpus, i, and count how many times it occurs.\n",
    "\n",
    "If you have a list/sequence of words, obtaining the counts in Python is as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'humpy': 1, 'again': 1, 'dumpty': 1, 'together': 1})\n"
     ]
    }
   ],
   "source": [
    "words = ['humpy', 'dumpty','together', 'again']\n",
    "counts = Counter(words)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then divide the counts by the total number of words, to normalize the values to a [0, 1] range. What this produces is a prior probability distribution over all unique words, P(i) , and that describes some interesting properties of the corpus, for instance which words are frequent and which ones are rare!\n",
    "\n",
    "If the corpus is large enough, e.g. the entire text on Wikipedia, then the model tends to represent the distribution of the language itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Text\n",
    "\n",
    "One thing we could do with such a model is to generate new text! After all, it is a probability distribution. So we can sample it, right?\n",
    "\n",
    "Again, Python makes this very straightforward, esp. with the random.choices() function included in Python 3.6 and above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choices(list(counts.keys()), weights=list(counts.values()), k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we are using the raw counts - this is okay, as we only need relative weights, not true probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modeling is a very interesting use case for NLP. It happens to be an unsupervised learning problem - where there might not be any clear label or category that applies to each document; instead, each document is treated as a mixture of various topics.\n",
    "\n",
    "[image here]\n",
    "\n",
    "Estimating these mixtures and simultaneously identifying the underlying topics is the goal of topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bag-of-Words: Graphical Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
