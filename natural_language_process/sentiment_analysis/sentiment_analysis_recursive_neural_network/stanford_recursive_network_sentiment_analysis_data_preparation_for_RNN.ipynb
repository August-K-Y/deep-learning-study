{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import string\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file=None):\n",
    "    if data_file == None:\n",
    "        return\n",
    "    with open(data_file) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './data/large_files/stanford_sentiment/parsed_data/'\n",
    "word2idx = load_data(folder + \"sentiment_word2idx.json\")\n",
    "sentiment_binary_train = load_data(folder + \"sentiment_binary_train.json\")\n",
    "sentiment_train = load_data(folder + \"sentiment_train.json\")\n",
    "sentiment_binary_test = load_data(folder + \"sentiment_binary_test.json\")\n",
    "sentiment_test = load_data(folder + \"sentiment_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering: # of training samples and # of test samples\n",
      "# of traing samples:  6920\n",
      "# of test samples:  1821\n"
     ]
    }
   ],
   "source": [
    "# the loaded samples has three type of labels -1,0,1, in which -1 indicates neutral sentiment.\n",
    "# We exclude samples with neutral sentiment.\n",
    "def exclude_neutral_sample(samples:dict):\n",
    "    ssamples = {}\n",
    "    for k, v in samples.items():\n",
    "        if v[3][-1] != -1:\n",
    "            ssamples[k] = v\n",
    "    return ssamples\n",
    "        \n",
    "train_b = exclude_neutral_sample(sentiment_binary_train)\n",
    "test_b = exclude_neutral_sample(sentiment_binary_test)\n",
    "\n",
    "print(\"After filtering: # of training samples and # of test samples\")\n",
    "print(\"# of traing samples: \", len(train_b))\n",
    "print(\"# of test samples: \", len(test_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment(wordidx, idx2word:dict):\n",
    "    wordlist = []\n",
    "    for idx in wordidx:\n",
    "        if idx != -1:\n",
    "            token = idx2word[idx]\n",
    "            if token not in string.punctuation:\n",
    "                wordlist.append(token)\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_samples(samples:dict, idx2word:dict):\n",
    "    comments = []\n",
    "    targets = []\n",
    "    for _, v in samples.items():\n",
    "        if v[3][-1] != -1:\n",
    "            comment = \" \".join(get_comment(v[0], idx2word))\n",
    "            label = v[3][-1]\n",
    "            comments.append(comment)\n",
    "            targets.append(label) \n",
    "    return comments, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3310\n",
      "1 3610\n",
      "-1 0\n"
     ]
    }
   ],
   "source": [
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "train_comments_0, train_targets = get_comments_samples(train_b, idx2word)\n",
    "test_comments_0, test_targets = get_comments_samples(test_b, idx2word)\n",
    "\n",
    "count0 = 0\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "for i in range(len(train_comments_0)):\n",
    "\n",
    "    if train_targets[i] == 0:\n",
    "        count0 += 1\n",
    "    elif train_targets[i] == 1:\n",
    "        count1 += 1\n",
    "    else:\n",
    "        count2 += 1\n",
    "#     print(i, comments[i], targets[i])\n",
    "    \n",
    "print(\"0\", count0)\n",
    "print(\"1\", count1)\n",
    "print(\"-1\", count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "an undeniably gorgeous terminally smitten document of a troubadour his acolytes and the triumph of his band\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(type(train_comments_0))\n",
    "print(train_comments_0[1])\n",
    "print(train_targets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_size 18647\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(idx2word)\n",
    "print('vocabulary_size' , vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text1 = ' '.join(train_comments_0)\n",
    "all_text2 = ' '.join(test_comments_0)\n",
    "words1 = all_text1.split()\n",
    "words2 = all_text2.split()\n",
    "words = words1 + words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.Counter'>\n"
     ]
    }
   ],
   "source": [
    "# Create your dictionary that maps vocab words to integers here\n",
    "from collections import Counter\n",
    "counts = Counter(words)\n",
    "print(type(counts))\n",
    "\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "# Note that index start from 1\n",
    "vocab_to_int = {word:i for i, word in enumerate(vocab, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of vocab <class 'list'>\n",
      "total # of words:  16750\n",
      "first word: the\n",
      "last word: miracles\n",
      "first word index: 1\n",
      "last word index 16750\n"
     ]
    }
   ],
   "source": [
    "print(\"type of vocab\", type(vocab))\n",
    "print('total # of words: ', len(vocab_to_int))\n",
    "print(\"first word:\", vocab[0])\n",
    "print(\"last word:\", vocab[-1])\n",
    "print(\"first word index:\", vocab_to_int[vocab[0]])\n",
    "print(\"last word index\", vocab_to_int[vocab[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {idx:word for word, idx in vocab_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(reviews, vocab_to_int):\n",
    "    # Convert the reviews to integers, same shape as reviews list, but with integers\n",
    "    print('# of reviews before index: ', len(reviews))\n",
    "    reviews_ints = []\n",
    "    for review in reviews:\n",
    "        reviews_ints.append([vocab_to_int[word] for word in review.split()])\n",
    "\n",
    "    print('# of reviews after index: ', len(reviews_ints))\n",
    "    return reviews_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of reviews before index:  6920\n",
      "# of reviews after index:  6920\n"
     ]
    }
   ],
   "source": [
    "x_train = convert_to_int(train_comments_0, vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 4, 8, 217, 5664, 320, 1, 246, 4, 746, 48, 55, 2, 888, 5958, 158, 4, 2, 78, 3283, 488, 2824]\n",
      "all of it works smoothly under the direction of spielberg who does a convincing impersonation here of a director enjoying himself immensely\n",
      "['all', 'of', 'it', 'works', 'smoothly', 'under', 'the', 'direction', 'of', 'spielberg', 'who', 'does', 'a', 'convincing', 'impersonation', 'here', 'of', 'a', 'director', 'enjoying', 'himself', 'immensely']\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(x_train[7])\n",
    "print(train_comments_0[7])\n",
    "text = [index2word[idx] for idx in x_train[7]]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of reviews before index:  1821\n",
      "# of reviews after index:  1821\n"
     ]
    }
   ],
   "source": [
    "x_test = convert_to_int(test_comments_0, vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 0\n",
      "Maximum train example length: 49\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create length to frequency map\n",
    "x_train_lens = Counter([len(x) for x in x_train])\n",
    "print(\"Zero-length reviews: {}\".format(x_train_lens[0]))\n",
    "print(\"Maximum train example length: {}\".format(max(x_train_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 0\n",
      "Maximum test example length: 53\n"
     ]
    }
   ],
   "source": [
    "# Create length to frequency map\n",
    "x_test_lens = Counter([len(x) for x in x_test])\n",
    "print(\"Zero-length reviews: {}\".format(x_test_lens[0]))\n",
    "print(\"Maximum test example length: {}\".format(max(x_test_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(reviews_ints, seq_len):\n",
    "    \n",
    "    # The features created here are the data that we are going to train and test the network\n",
    "\n",
    "    # Create features with shape (len(reviews_ints), seq_len) and initialized with zeros\n",
    "    features = np.zeros((len(reviews_ints), seq_len), dtype=int)\n",
    "\n",
    "    print(features.shape)\n",
    "    # Create list holding the length for each review\n",
    "    lengths = []\n",
    "\n",
    "    # row is the review in forms of a list of integers\n",
    "    for i, row in enumerate(reviews_ints):\n",
    "\n",
    "        # left padding\n",
    "        features[i, :len(row)] = np.array(row)[:seq_len]\n",
    "\n",
    "        # record the length of each review. This might be useful when we want to use sequence_length argument\n",
    "        # of tf.nn.dynamic_rnn(...)\n",
    "        lengths.append(len(row) if len(row) < seq_len else seq_len)\n",
    "        \n",
    "    return features, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10)\n",
      "[[ 4  6  7  2  3  5  7  8  1  0]\n",
      " [ 1  2  3  4  6  7  2  3  5  7]\n",
      " [ 1  2  3  4  6  7  2  3 26  1]\n",
      " [ 8  7  3  0  0  0  0  0  0  0]]\n",
      "[9, 10, 10, 3]\n"
     ]
    }
   ],
   "source": [
    "## test\n",
    "\n",
    "sample1 = [4,6,7,2,3,5,7,8,1]\n",
    "sample2 = [1,2,3,4,6,7,2,3,5,7]\n",
    "sample3 = [1,2,3,4,6,7,2,3,26,1, 11, 12]\n",
    "sample4 = [8,7,3]\n",
    "samples = []\n",
    "samples.append(sample1)\n",
    "samples.append(sample2)\n",
    "samples.append(sample3)\n",
    "samples.append(sample4)\n",
    "features_, lengths_ = padding(samples, 10)\n",
    "\n",
    "print(features_)\n",
    "print(lengths_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 50\n",
    "kk = [1,2,3]\n",
    "pad = [0] * 7\n",
    "np.append(kk, pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6920, 50)\n",
      "(1821, 50)\n"
     ]
    }
   ],
   "source": [
    "x_train_p, x_train_len = padding(x_train, 50)\n",
    "x_test_p, x_text_len = padding(x_test, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    2,  1501,    13,   493,    10,   903,    14,   504,    12,\n",
       "          205,   563,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [    6,    22,     8,     2,   265,  1485,    10,    38,   725,\n",
       "           16,    78,  1602, 11045,     5,    81,     2,  1607,     5,\n",
       "          488,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [   33,    19,  1182,  2685,     8, 11230,   238,  2230,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [   15,     6,     1,   157,     4,    18,     9,   222,     2,\n",
       "         1618,  1460,   208,   154,  5210,  2710,    10,  5159,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [   47,    85,     8,   213,   180,     6,  3236,     7,   150,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [   15,   133,     3,  2196,  1910,    13,     6,     2,   895,\n",
       "          621,     4,  6564,     5,     1,  8912,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [   15,   281,  1046,     6,    11,   704,    11,     8,     6,\n",
       "         1045,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [   37,     4,     8,   217,  5664,   320,     1,   246,     4,\n",
       "          746,    48,    55,     2,   888,  5958,   158,     4,     2,\n",
       "           78,  3283,   488,  2824,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [    1,    44,    32,    71,   380,     3,    60,   867,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [    8,     7, 16486,   433,    12,  2343,    24,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_p[:10,:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6920,)\n",
      "(1821,)\n"
     ]
    }
   ],
   "source": [
    "train_label = np.array(train_targets)\n",
    "test_label = np.array(test_targets)\n",
    "print(train_label.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(features, labels, seq_len, batch_size):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, seq_len, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    \n",
    "    # Only get full batches\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    \n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cell(lstm_units, keep_prob):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(lstm_units)\n",
    "    drop = tf.contrib.rnn.DropoutWapper(cell, output_keep_prob = keep_prob)\n",
    "    return drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 500\n",
    "\n",
    "embedding_dim = 300\n",
    "vocab_size = len(vocab_to_int) + 1\n",
    "\n",
    "num_lstm_layer = 1\n",
    "lstm_size = \n",
    "\n",
    "# input placeholders\n",
    "inputs = tf.placeholder(tf.int32, shape=[None, None], 'inputs')\n",
    "labels = tf.placeholder(tf.int32, shape=[None, None], 'labels')\n",
    "seq_len = tf.placeholder(tf.int32, shape=[None], 'seq_len')\n",
    "keep_prob = tf.placeholder(tf.float32, name = \"keep_prob\")\n",
    "\n",
    "\n",
    "# embedding layer\n",
    "embedding = tf.Variable(tf.random_normal((vocab_size, embedding_dim), -1, 1), name = 'embedding')\n",
    "embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "\n",
    "\n",
    "# LSTM layer\n",
    "cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_lstm_layer)])\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state = initial_state)\n",
    "\n",
    "predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
