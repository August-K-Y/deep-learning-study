{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognition using Logistic Regression\n",
    "\n",
    "* We will focus on class 0 and 1\n",
    "* 4953 samples of class 0, 547 samples of class 1\n",
    "\n",
    "### Class Imbalance\n",
    "* We will implement a binary classifier using logistic Regression. Therefore, we will focus on class 0 and 1.\n",
    "* There are 4953 samples of class 0 and 547 samples of class 1.\n",
    "* What would my classifiction rate be if I just choose 0 every time.\n",
    "    * (4953 - 547) / 4953, therefore imbalance classification problem\n",
    "\n",
    "### 2-class problem vs. 7-class problem \n",
    "* When we swich to softmax, will the problem get easier or harder?\n",
    "    * 2-class: guess at random - expect 50% error\n",
    "    * 7-class: guess at random - expect 6/7 = 86% error\n",
    "    * K class: 1/K chance of being correct\n",
    "* Kaggle top score\" ~70% correct\"\n",
    "\n",
    "    \n",
    "### Solving class imbalance\n",
    "* Suppose we have 1000 samples from class 1, 100 samples from class 2\n",
    "    * Method 1) Pick 100 samples from class 1, now we have 100 vs. 100\n",
    "    * Method 2) Repeat class 2 10 times, now we have 1000 vs. 1000\n",
    "    * Same 'expected' error rate\n",
    "    * But method 2 is better (less variance, more data)\n",
    "* Other options to expand class 2:\n",
    "    * Add Gaussian noise\n",
    "    * Add invariant transformatoins (shift left, right rotate, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with sigmoid (binary classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with softmax (softmas regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(balance_ones=False):\n",
    "    # images are 48x48=2304 size vectors\n",
    "    # N=5500\n",
    "    Y=[]\n",
    "    X=[]\n",
    "    first=True\n",
    "    for line in open('../data/fer2013/fer2013.csv'):\n",
    "        if first:\n",
    "            first=False\n",
    "        else:\n",
    "            row=line.split(',')\n",
    "            y = int(row[0])\n",
    "#             if y == 0 or y == 1:\n",
    "            Y.append(y)\n",
    "            X.append([int(p) for p in row[1].split()])\n",
    "    X, Y = np.array(X) / 255.0, np.array(Y)  \n",
    "    \n",
    "#     if balance_ones:\n",
    "#         XO, YO = X[Y!=1, :], Y[Y!=1]\n",
    "#         X1 = X[Y==1, :]\n",
    "#         X1 = np.repeat(X1, 9, axis=0)\n",
    "#         X = np.vstack([XO, X1])\n",
    "#         Y = np.concatenate((YO, [1]*len(X1)))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(A):\n",
    "    expA = np.exp(A)\n",
    "    return expA / expA.sum(axis=1, keepdims=True)\n",
    "    \n",
    "def cost(T, Y):\n",
    "    tot = T * np.log(Y)\n",
    "    return -tot.sum()\n",
    "\n",
    "def error_rate(T, P):\n",
    "    return np.mean(T != P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we use softmax, this can also be called softmax regression\n",
    "class LogisticRegression(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, Y, epochs=100, learning_rate=10e-8, reg=10e-12, show_fig=False):\n",
    "        \n",
    "        X, Y = shuffle(X, Y)\n",
    "        print(X.shape)\n",
    "        print(Y.shape)\n",
    "        \n",
    "        N, D = X.shape\n",
    "        K = len(set(Y))\n",
    "        \n",
    "        # split data into training set and validation set\n",
    "        X, Y = X[0:-1000], Y[0:-1000]\n",
    "        Xvalid, Yvalid = X[-1000:], Y[-1000:]\n",
    "        \n",
    "        # one-hot-encoding the labels for both training set and validation set\n",
    "        T = y2indicator(Y, K)\n",
    "        Tvalid = y2indicator(Yvalid, K)\n",
    "        print(T.shape)\n",
    "        print(Tvalid.shape)\n",
    "        \n",
    "        # initialize weights randomly\n",
    "        self.W = np.random.randn(D, K) / np.sqrt(D + K)\n",
    "        self.b = np.zeros(K)\n",
    "        \n",
    "        \n",
    "        # train the model\n",
    "        costs = []\n",
    "        best_validation_error = 1\n",
    "        for ep in range(epochs):\n",
    "            \n",
    "            # forward propagation\n",
    "            Py = self.forward(X)\n",
    "\n",
    "            # back propagation\n",
    "            Py_Y = Py - T\n",
    "            self.W -= learning_rate * (X.T.dot(Py_Y) + reg * self.W)\n",
    "            self.b -= learning_rate * (Py_Y.sum(axis=0) + reg * self.b)\n",
    "        \n",
    "             # show performace metrics\n",
    "            if ep % 10 == 0:\n",
    "                Pyvalid = self.forward(Xvalid)\n",
    "                c = cost(Tvalid, Pyvalid)\n",
    "                costs.append(c)\n",
    "                e = error_rate(Yvalid, np.argmax(Pyvalid, axis=1))\n",
    "                print(\"ep:\", ep, \"cost:\", c, \"error:\", e)\n",
    "                if e < best_validation_error:\n",
    "                    best_validation_error = e\n",
    "        print(\"best_validation_error:\", best_validation_error)              \n",
    "\n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()     \n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return softmax(X.dot(self.W) + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-encoding on labels\n",
    "def y2indicator(y, dims):\n",
    "    N = len(y)\n",
    "    y = y.astype(np.int32)\n",
    "    ind = np.zeros((N, dims))\n",
    "    for i in range(N):\n",
    "        ind[i, y[i]] = 1\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40263, 2304)\n",
      "(40263,)\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_data()\n",
    "\n",
    "XO, YO = X[Y!=1, :], Y[Y!=1]\n",
    "X1 = X[Y==1, :]\n",
    "X1 = np.repeat(X1, 9, axis=0)\n",
    "X = np.vstack([XO, X1])\n",
    "Y = np.concatenate((YO, [1]*len(X1)))\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40263, 2304)\n",
      "(40263,)\n",
      "(39263, 7)\n",
      "(1000, 7)\n",
      "ep: 0 cost: 2021.41768959 error: 0.869\n",
      "ep: 10 cost: 1930.68303262 error: 0.786\n",
      "ep: 20 cost: 1925.77484986 error: 0.781\n",
      "ep: 30 cost: 1921.71272939 error: 0.778\n",
      "ep: 40 cost: 1918.01546719 error: 0.773\n",
      "ep: 50 cost: 1914.61777947 error: 0.77\n",
      "ep: 60 cost: 1911.47440339 error: 0.767\n",
      "ep: 70 cost: 1908.54853829 error: 0.772\n",
      "ep: 80 cost: 1905.80997482 error: 0.773\n",
      "ep: 90 cost: 1903.2338262 error: 0.769\n",
      "ep: 100 cost: 1900.79951111 error: 0.771\n",
      "ep: 110 cost: 1898.48993466 error: 0.771\n",
      "ep: 120 cost: 1896.29083002 error: 0.769\n",
      "ep: 130 cost: 1894.19022965 error: 0.768\n",
      "ep: 140 cost: 1892.17804064 error: 0.763\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-aa68cadc8db4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_fig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-1e725d90aea4>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, epochs, learning_rate, reg, show_fig)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# forward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mPy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#             print(Py.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1e725d90aea4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X, Y, epochs=1000, show_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pitfalls\n",
    "\n",
    "```python\n",
    "def softmax(A):\n",
    "    expA = np.exp(A)\n",
    "    # expA / expA.sum() is not gonna work\n",
    "    return expA / expA.sum(axis=1, keepdims=True)\n",
    "    \n",
    "def cost(T, Y):\n",
    "    tot = T * np.log(Y)\n",
    "    # notice this is negative\n",
    "    return -tot.sum()\n",
    "```\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
