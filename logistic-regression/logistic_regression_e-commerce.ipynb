{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for E-Commerce Course Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_mobile</th>\n",
       "      <th>n_products_viewed</th>\n",
       "      <th>visit_duration</th>\n",
       "      <th>is_returning_visitor</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>user_action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.657510</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.568571</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042246</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.659793</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.014745</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_mobile  n_products_viewed  visit_duration  is_returning_visitor  \\\n",
       "0          1                  0        0.657510                     0   \n",
       "1          1                  1        0.568571                     0   \n",
       "2          1                  0        0.042246                     1   \n",
       "3          1                  1        1.659793                     1   \n",
       "4          0                  1        2.014745                     1   \n",
       "\n",
       "   time_of_day  user_action  \n",
       "0            3            0  \n",
       "1            2            1  \n",
       "2            1            0  \n",
       "3            1            2  \n",
       "4            1            2  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/ecommerce_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    df=pd.read_csv('data/ecommerce_data.csv')\n",
    "    data=df.as_matrix()\n",
    "    \n",
    "    # split each row into features and label (the last column)\n",
    "    X=data[:,:-1]\n",
    "    Y=data[:,-1]\n",
    "    \n",
    "    # normalize column n_products_viewed and visti_duration\n",
    "    X[:,1]=(X[:,1]-X[:,1].mean())/X[:,1].std()\n",
    "    X[:,2]=(X[:,2]-X[:,2].mean())/X[:,2].std()\n",
    "\n",
    "        \n",
    "    # do the one-hot-encoding on column time_of_day\n",
    "    N, D = X.shape\n",
    "    X2=np.zeros((N, D + 3))\n",
    "    X2[:, 0:(D-1)] = X[:, 0:(D-1)]\n",
    "\n",
    "    \n",
    "    # the time_of_day is the 5th column (0-based)\n",
    "    for n in range(N):\n",
    "        t = int(X[n,4])\n",
    "        X2[n,t+D-1]=1\n",
    "        \n",
    "    return X2, Y\n",
    "\n",
    "def get_binary_data():\n",
    "    X, Y = get_data()\n",
    "    X2=X[Y<=1]\n",
    "    Y2=Y[Y<=1]\n",
    "    return X2, Y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(298, 8)\n",
      "(298,)\n",
      "(100, 8)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_binary_data()\n",
    "X, Y = shuffle(X, Y)\n",
    "\n",
    "# hold 100 samples for testing\n",
    "Xtrain = X[:-100]\n",
    "Ytrain = Y[:-100]\n",
    "Xtest = X[-100:]\n",
    "Ytest = Y[-100:]\n",
    "\n",
    "print(Xtrain.shape)\n",
    "print(Ytrain.shape)\n",
    "print(Xtest.shape)\n",
    "print(Ytest.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W):\n",
    "    '''\n",
    "    Compute the output of the Logistic Regression Network\n",
    "    '''\n",
    "    return sigmoid(X.dot(W))\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def classification_rate(Y, P):\n",
    "    return np.mean(Y==P)\n",
    "\n",
    "def cross_entropy(T, Y):\n",
    "    J = 0;\n",
    "    N = len(T)\n",
    "    for i in range(N):\n",
    "        if T[i] == 1:\n",
    "            J-=np.log(Y[i])\n",
    "        else:\n",
    "            J-=np.log(1-Y[i])\n",
    "    return J/N\n",
    "\n",
    "# def cross_entropy(T, Y):\n",
    "#     return -np.mean(T*np.log(Y) + (1-T)*np.log(1-Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ridge(X, T, epochs=100, learning_rate=0.1, lamba=0.1):\n",
    "    \n",
    "    N, D = X.shape\n",
    "    \n",
    "    # add bias \n",
    "    ones = np.ones((N,1))\n",
    "    Xb = np.concatenate((X, ones), axis=1)\n",
    "\n",
    "    # randomly initialize weights\n",
    "    w = np.random.randn(D + 1)\n",
    "    \n",
    "    Y = forward(Xb, w)\n",
    "\n",
    "    # training weights using gradient descent\n",
    "    for i in range(epochs):\n",
    "        if i % 10 == 0:\n",
    "            print(i, cross_entropy(T, Y))\n",
    "        \n",
    "        w += learning_rate * (Xb.T.dot(T - Y) - lamba * w)\n",
    "        Y = forward(Xb, w)\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "If create a class for Logistic Regression, \n",
    "we can hold w as a interal object variable \n",
    "such that there is no need to pass in the weights w.\n",
    "'''\n",
    "def predict(X, T, w):\n",
    "    N, D = X.shape\n",
    "    \n",
    "    # add bias \n",
    "    ones = np.ones((N,1))\n",
    "    Xb = np.concatenate((X, ones), axis=1)\n",
    "\n",
    "    \n",
    "    Y = forward(Xb, w)\n",
    "    \n",
    "    print(\"classification rate:\", classification_rate(T, np.round(Y)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.08532758132\n",
      "10 0.19313694999\n",
      "20 0.173882095417\n",
      "30 0.44092113747\n",
      "40 0.228556271098\n",
      "50 0.1822311929\n",
      "60 0.173530344407\n",
      "70 0.173203015252\n",
      "80 0.173479613681\n",
      "90 0.292778065194\n",
      "final w: [ 1.8206699   8.39427415  0.72444741  2.47000937  0.57614225 -0.72158962\n",
      "  0.10495549 -0.65666915 -0.47479969] <class 'numpy.ndarray'>\n",
      "classification rate: 0.976510067114\n",
      "classification rate: 0.95\n"
     ]
    }
   ],
   "source": [
    "w = fit_ridge(Xtrain, Ytrain)\n",
    " \n",
    "print(\"final w:\", w, type(w))\n",
    "predict(Xtrain, Ytrain, w)\n",
    "predict(Xtest, Ytest, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with softmax (softmas regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_rate(Y, P):\n",
    "    return np.mean(Y==P)\n",
    "\n",
    "def error_rate(T, P):\n",
    "    return np.mean(T != P)\n",
    "\n",
    "def softmax_cross_entropy(T, Y):\n",
    "    return -(T * np.log(Y)).mean()\n",
    "\n",
    "def softmax(A):\n",
    "    expA = np.exp(A)\n",
    "    return expA / expA.sum(axis=1, keepdims=True)\n",
    "\n",
    "# one-hot-encoding on labels\n",
    "def y2indicator(y, dims):\n",
    "    N = len(y)\n",
    "    y = y.astype(np.int32)\n",
    "    \n",
    "    # initialize the indicator matrix, \n",
    "    # N is the number of samples which dims is the number of classes\n",
    "    ind = np.zeros((N, dims))\n",
    "    for i in range(N):\n",
    "        ind[i, y[i]] = 1\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, Y, epochs=10000, learning_rate=0.001, reg=0.1, show_fig=False):\n",
    "        X, Y = shuffle(X, Y)\n",
    "        \n",
    "        N, D = X.shape\n",
    "        K = len(set(Y))\n",
    "        \n",
    "        # normally the split of data should be done before the fit method\n",
    "        Xvalid, Yvalid = X[-100:], Y[-100:]\n",
    "        X, Y = X[:-100], Y[:-100]\n",
    "        \n",
    "        T = y2indicator(Y, K)\n",
    "        Tvalid = y2indicator(Yvalid, K)\n",
    "        \n",
    "        self.W1 = np.random.randn(D, K) / np.sqrt(D + K)\n",
    "#         self.W1 = np.random.randn(D, K)\n",
    "        self.b1 = np.zeros(K)\n",
    "        \n",
    "        costs = []\n",
    "#         best_validation_error = 1\n",
    "        best_classification_rate = 0\n",
    "        for ep in range(epochs):\n",
    "            \n",
    "            Py = self.forward(X)\n",
    "            \n",
    "            Py_T = Py - T\n",
    "            self.W1 -= learning_rate * (X.T.dot(Py_T) + reg * self.W1)\n",
    "            self.b1 -= learning_rate * (Py_T.sum(axis=0) + reg * self.b1)\n",
    "            \n",
    "            \n",
    "            Pyvalid = self.forward(Xvalid)\n",
    "            c = softmax_cross_entropy(Tvalid, Pyvalid)\n",
    "            costs.append(c)\n",
    "                \n",
    "            if ep % 1000 == 0:\n",
    "#                 Pyvalid = self.forward(Xvalid)\n",
    "#                 c = softmax_cross_entropy(Tvalid, Pyvalid)\n",
    "#                 costs.append(c)\n",
    "                cl = classification_rate(Yvalid, np.argmax(Pyvalid, axis=1))\n",
    "                print(\"ep:\", ep, \"cost:\", c, \"classfication:\", cl)\n",
    "                if cl > best_classification_rate:\n",
    "                    best_classification_rate = cl\n",
    "                \n",
    "        print(\"best_classification_rate:\", best_classification_rate) \n",
    "        print(\"Final train classification rate:\", classification_rate(Y, self.predict(X))) \n",
    "        print(\"Final test classification rate:\", classification_rate(Yvalid, self.predict(Xvalid))) \n",
    "        \n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()              \n",
    "\n",
    "    def forward(self, X):\n",
    "        return softmax(X.dot(self.W1) + self.b1)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        Py = self.forward(X)\n",
    "        return np.argmax(Py, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0 cost: 0.315030951582 classfication: 0.47\n",
      "ep: 1000 cost: 0.100148279431 classfication: 0.87\n",
      "ep: 2000 cost: 0.0956286325591 classfication: 0.87\n",
      "ep: 3000 cost: 0.0940821449067 classfication: 0.87\n",
      "ep: 4000 cost: 0.0933683607299 classfication: 0.88\n",
      "ep: 5000 cost: 0.0929861021298 classfication: 0.88\n",
      "ep: 6000 cost: 0.0927641833303 classfication: 0.88\n",
      "ep: 7000 cost: 0.092629233259 classfication: 0.88\n",
      "ep: 8000 cost: 0.0925448806776 classfication: 0.88\n",
      "ep: 9000 cost: 0.0924912705111 classfication: 0.88\n",
      "best_classification_rate: 0.88\n",
      "Final train classification rate: 0.93\n",
      "Final test classification rate: 0.88\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109fb5400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, Y = get_data()\n",
    "\n",
    "# print(X.shape)\n",
    "model = SoftmaxRegression()\n",
    "model.fit(X, Y, learning_rate=0.001, epochs=10000, reg=0.1, show_fig=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
