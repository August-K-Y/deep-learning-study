{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Important hyperparameters for RNN\n",
    "\n",
    "* `batch_size`: The number of training examples to feed the network in one training pass. Typically this should be set as high as you can go without running out of memory.\n",
    "* `num_steps`: Number of characters in the sequence the network is trained on. Larger is better typically; the network will learn more long range dependencies. But it takes longer to train. \n",
    "* `lstm_units`: Number of units in the hidden layers in the LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
    "* `lstm_layers`: Number of LSTM layers in the network. I'd start with 1, then add more if I'm underfitting.\n",
    "* `learning_rate`: Learning rate\n",
    "* `keep_prob`: The dropout keep probability when training. If you're network is overfitting, try decreasing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word embedding\n",
    "\n",
    "* After preprocessing our data, we typically have training examples with shape `(batch_size, num_steps)` for each batch.\n",
    "    * For word-based application, we have a vocabulary with V words. Typically, each element in the training examples is a number in the range of [0, V] representing a word in the vocabulary. (In application like text generation, the range might be [1, V] and 0 may represent padding word).\n",
    "    * For character-based application, we have a vocabulary with C characters. Typically, each element in the training examples is a number in the range of [0, C] representing a character in the vocabulary. (In application like text generation, the range might be [1, V] and 0 may represent padding word).\n",
    "* We need to add an embedding layer to encode each word in the training example instead of one-hot encoding, which might work for character-based rnn problem. \n",
    "    * It is massively inefficient to one-hot encode a word since we may have a very big vocabulary base and one-hot would make the word vector extremely large. \n",
    "    * We can use an existing pre-trained word2vec representation. \n",
    "    \n",
    "* Create the embedding lookup matrix as a `tf.Variable`. Use that embedding matrix to get the embedded vectors for each word in the original training examples to pass to the LSTM cell with [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup). \n",
    "\n",
    "* [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup) takes the embedding matrix and an input tensor, typically training examples. Then, it'll return another tensor with the embedded vectors. So, if the embedding layer has 200 units, the function will return a tensor with size [batch_size, 200].\n",
    "\n",
    "```\n",
    "embedding_lookup(\n",
    "    params,\n",
    "    ids,\n",
    "    partition_strategy='mod',\n",
    "    name=None,\n",
    "    validate_indices=True,\n",
    "    max_norm=None\n",
    ")\n",
    "```\n",
    "\n",
    "* `params`: A single tensor representing the complete embedding tensor, or a list of P tensors all of same shape except for the first dimension, representing sharded embedding tensors. Alternatively, a PartitionedVariable, created by partitioning along dimension 0. Each element must be appropriately sized for the given partition_strategy.\n",
    "* `ids`: A Tensor with type int32 or int64 containing the ids to be looked up in params.\n",
    "* `partition_strategy`: A string specifying the partitioning strategy, relevant if len(params) > 1. Currently \"div\" and \"mod\" are supported. Default is \"mod\".\n",
    "\n",
    "returns a dense tensor:\n",
    "* with shape: shape(ids) + shape(params)[1:].\n",
    "* with type: the same as the tensors in params."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "embed_size = 300 \n",
    "with graph.as_default():\n",
    "    # create a embedding lookup matrix \n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_dim), -1, 1), name = \"embedding\")\n",
    "    # get embeded vector for each word in the inputs\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "```\n",
    "\n",
    "The code above creates a embedding lookup matrix with shape (n_words, embed_size), where `n_words` is the size of the word vocabulary and `embed_dim` is <b>embedding dimension</b> that is the dimension of the vector representing a word after embedding. \n",
    "\n",
    "After embedding lookup, the training examples would have shape `(batch_size, num_steps, embed_dim)` "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define type of cell for RNN\n",
    "\n",
    "Next, we'll create our cells (e.g., LSTM, GRU) to use in the [recurrent network](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn). Here we are just defining what the cells look like. This isn't actually building the graph, just defining the type of cells we want in our graph. There are many [types of cells](https://www.tensorflow.org/api_guides/python/contrib.rnn#Core_RNN_Cells_for_use_with_TensorFlow_s_core_RNN_methods):\n",
    "\n",
    "* tf.contrib.rnn.BasicRNNCell\n",
    "* tf.contrib.rnn.BasicLSTMCell\n",
    "* tf.contrib.rnn.GRUCell\n",
    "* tf.contrib.rnn.LSTMCell\n",
    "* tf.contrib.rnn.LayerNormBasicLSTMCell\n",
    "\n",
    "If we want to create a basic LSTM cell for the graph, we will want to use [tf.contrib.rnn.BasicLSTMCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell). Looking at the function documentation:\n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.BasicLSTMCell(num_units, forget_bias=1.0, state_is_tuple=True, activation=None, reuse=None)\n",
    "```\n",
    "\n",
    "* `num_units`: int, The number of units in the LSTM cell.\n",
    "* `forget_bias`: float, The bias added to forget gates (see above). Must set to 0.0 manually when restoring from CudnnLSTM-trained checkpoints.\n",
    "* `state_is_tuple`: If True, accepted and returned states are 2-tuples of the c_state and m_state. If False, they are concatenated along the column axis. The latter behavior will soon be deprecated.\n",
    "* `activation`: Activation function of the inner states. Default: tanh.\n",
    "* `reuse`: (optional) Python boolean describing whether to reuse variables in an existing scope. If not True, and the existing scope already has the given variables, an error is raised.\n",
    "\n",
    "you can see it takes a parameter called `num_units`, the number of units in the cell. So then, you can write something like \n",
    "\n",
    "```python\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "```\n",
    "\n",
    "to create an LSTM cell with `num_units`. \n",
    "\n",
    "Next, you can add dropout to the cell with [`tf.contrib.rnn.DropoutWrapper`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper). This just wraps the cell in another cell, but with dropout added to the inputs and/or outputs. It's a really convenient way to make your network better with almost no effort! So you'd do something like\n",
    "\n",
    "```python\n",
    "drop = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "```\n",
    "\n",
    "* `cell`: an RNNCell, a projection to output_size is added to it.\n",
    "* `input_keep_prob`: unit Tensor or float between 0 and 1, input keep probability; if it is constant and 1, no input  dropout will be added.\n",
    "* `output_keep_prob`: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added.\n",
    "* and many other arguments\n",
    "\n",
    "Most of the time, your network will have better performance with more layers. That's sort of the magic of deep learning, adding more layers allows the network to learn really complex relationships. Again, there is a simple way to create multiple layers of LSTM cells with [`tf.contrib.rnn.MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell):\n",
    "\n",
    "```python\n",
    "def build_cell(num_units, keep_prob):     \n",
    "      lstm = tf.contrib.rnn.BasicLSTMCell(num_units)     \n",
    "      drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)      \n",
    "      return drop  \n",
    "      \n",
    "cell = tf.contrib.rnn.MultiRNNCell([build_cell(num_units, keep_prob) for _ in range(num_layers)])\n",
    "\n",
    "```\n",
    "\n",
    "The `build_cell()` function build a cell we described above. The `MultiRNNCell` wrapper builds this into multiple layers of RNN cells, one for each cell in the list.\n",
    "\n",
    "So the final cell you're using in the network is actually multiple (or just one) cells with dropout. But it all works the same from an achitectural viewpoint, just a more complicated graph in the cell.\n",
    "\n",
    "We also need to create an initial cell state of all zeros:\n",
    "\n",
    "```python\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "```\n",
    "\n",
    "Here is [a tutorial on building RNNs](https://www.tensorflow.org/tutorials/recurrent) that will help you out."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RNN forward pass\n",
    "\n",
    "We need to actually run the data through the RNN nodes. You can use [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn![image.png](attachment:image.png)) to do this. \n",
    "\n",
    "You'd pass in the RNN `cell` you created (our multiple layered LSTM cell for instance), the `inputs` to the network and the `initial_state`:\n",
    "\n",
    "```python\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)\n",
    "```\n",
    "\n",
    "the `initial_state` is the cell state that is passed between the hidden layers in successive time steps. `tf.nn.dynamic_rnn` takes care of most of the work for us. We pass in our cell and the input to the cell. Then it does the unrolling and everything else for us. It returns `outputs` for each time step and the `final_state` of the hidden layer.\n",
    "\n",
    "\n",
    "* `cell`: An instance of RNNCell.\n",
    "* `inputs`: The RNN inputs. \n",
    "    * If time_major == False (default), this must be a Tensor of shape: [batch_size, max_time, ...], or a nested tuple of such elements. \n",
    "    * If time_major == True, this must be a Tensor of shape:[max_time, batch_size, ...], or a nested tuple of such elements. \n",
    "        * <b>The first two dimensions must match across all the inputs</b>, but otherwise the ranks and other shape components may differ. In this case, \n",
    "        * Input to cell at each time-step will replicate the structure of these tuples, except for the time dimension (from which the time is taken). <b>The input to cell <b style='color:red'>at each time step</b> will be a Tensor or (possibly nested) tuple of Tensors each with dimensions [batch_size, ...]<b>.\n",
    "\n",
    "* `initial_state`: (optional) An initial state for the RNN. If cell.state_size is an integer, this must be a Tensorof appropriate type and shape [batch_size, cell.state_size]. If cell.state_size is a tuple, this should be a tuple of tensors having shapes [batch_size, s] for s in cell.state_size.\n",
    "\n",
    "Returns:\n",
    "\n",
    "A pair (outputs, state) where:\n",
    "\n",
    "* `outputs`: The RNN output Tensor.\n",
    "    * If time_major == False (default), this will be a Tensor shaped: [batch_size, max_time, cell.output_size].\n",
    "    * If time_major == True, this will be a Tensor shaped: [max_time, batch_size, cell.output_size].\n",
    "\n",
    "* `state`: The final state. If cell.state_size is an int, this will be shaped [batch_size, cell.state_size]. If it is a TensorShape, this will be shaped [batch_size] + cell.state_size. If it is a (possibly nested) tuple of ints or TensorShape, this will be a tuple having the corresponding shapes. If cells are LSTMCells state will be a tuple containing a LSTMStateTuple for each cell.\n",
    "\n",
    "**Understand inputs and outputs**\n",
    "* The inputs to the tf.nn.dynamic_rnn is of shape `[batch_size, max_time, embed_dim]`\n",
    "* The inputs to cell <b style='color:red'>at each time step</b> will have shape of `[batch_size, embed_dim]`\n",
    "* The outputs from tf.nn.dynamic_rnn have shape of `[batch_size, max_time, cell.output_size]`\n",
    "    * the cell.output_size typically is the `lstm_units` of the cell in the last layer of LSTM network. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Output Layer of RNN\n",
    "\n",
    "We only care about the final output, we'll be using that as our sentiment prediction. So we need to grab the last output with outputs[:, -1], the calculate the cost from that and labels_.\n",
    "\n",
    "```python\n",
    "def build_output(last_layer_input, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        lstm_output: List of output tensors from the LSTM layer\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape seq_output to a 2D tensor with lstm_size columns\n",
    "    x = tf.reshape(lstm_output, [-1, in_size])\n",
    "\n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        # Create the weight and bias variables here\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.add(tf.matmul(x, softmax_w), softmax_b)\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits, name=\"predictions\")\n",
    "    \n",
    "    return out, logits\n",
    "    \n",
    "\n",
    "predictions, logits = build_output(last_layer_input, in_size, out_size) \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimizer for RNN\n",
    "\n",
    "Normal RNNs have have issues gradients exploding and disappearing. LSTMs fix the disappearance problem, but the gradients can still grow without bound. To fix this, we can clip the gradients above some threshold. That is, if a gradient is larger than that threshold, we set it to the threshold. This will ensure the gradients never grow overly large. Then we use an AdamOptimizer for the learning step.\n",
    "\n",
    "```python\n",
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        grad_clip:\n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    # grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    grads = tf.gradients(loss, tvars)\n",
    "    clip_grads, _ = tf.clip_by_global_norm(grads , grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(clip_grads, tvars))\n",
    "    \n",
    "    return optimizer\n",
    "```\n",
    "\n",
    "`tf.trainable_variables()` creates a list of all the variables we've defined in our graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. validation accuracy\n",
    "\n",
    "```python\n",
    "correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
