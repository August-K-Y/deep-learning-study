{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.nn.dynamic_rnn in depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article, we will discuss in depth what the output of `tf.nn.dynamic_rnn()` looks like in various scenaios.\n",
    "\n",
    "The original code is from [article](https://stats.stackexchange.com/questions/330176/what-is-the-output-of-a-tf-nn-dynamic-rnn) and [\n",
    "Learning TensorFlow book](http://shop.oreilly.com/product/0636920063698.do)\n",
    "\n",
    "I add more thorough investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Layer with Basic RNN cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X batch shape: (4, 2, 3)\n",
      "outputs with shape:  (4, 2, 5)\n",
      "[[[ 0.0982992   0.91937888  0.03790487 -0.54347765 -0.00692416]\n",
      "  [ 0.99998456  1.          0.99177212  0.99987334  0.99939644]]\n",
      "\n",
      " [[ 0.97107625  0.99995422  0.75952923  0.60435373  0.87907934]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.9994756   1.          0.96049631  0.96466023  0.99186504]\n",
      "  [ 0.97324771  0.99998188  0.81755435  0.99450946  0.97564179]]\n",
      "\n",
      " [[ 0.99778575  0.98422712  0.97891462  0.99996006  0.99962366]\n",
      "  [ 0.12011302  0.96819985  0.18604073  0.9268809   0.67240906]]]\n",
      "==================================================================\n",
      "h states with shape:  (4, 5)\n",
      "[[ 0.99998456  1.          0.99177212  0.99987334  0.99939644]\n",
      " [ 0.97107625  0.99995422  0.75952923  0.60435373  0.87907934]\n",
      " [ 0.97324771  0.99998188  0.81755435  0.99450946  0.97564179]\n",
      " [ 0.12011302  0.96819985  0.18604073  0.9268809   0.67240906]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "    \n",
    "input_dim = 3\n",
    "num_steps = 2\n",
    "num_units = 5\n",
    "\n",
    "# None represents the batch size. \n",
    "# We put None here, since the batch size can be different for each batch.\n",
    "X = tf.placeholder(tf.float32, [None, num_steps, input_dim], 'inputs')\n",
    "seq_length = tf.placeholder(tf.int32, [None], 'seq_length')\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units)\n",
    "\n",
    "initial_state = basic_cell.zero_state(4, tf.float32)\n",
    "\n",
    "outputs, final_states = tf.nn.dynamic_rnn(basic_cell, X, sequence_length=seq_length, \n",
    "                                          initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "# Create a batch of training examples\n",
    "# shape (4, 2, 3)\n",
    "X_batch = np.array([\n",
    "  # t = 0      t = 1\n",
    "  [[0, 1, 2], [9, 8, 7]], # instance 0\n",
    "  [[3, 4, 5], [5, 1, 9]], # instance 1\n",
    "  [[6, 7, 8], [6, 5, 4]], # instance 2\n",
    "  [[9, 0, 1], [3, 2, 1]], # instance 3 \n",
    "])\n",
    "\n",
    "print('X batch shape:', X_batch.shape)\n",
    "\n",
    "seq_length_batch = np.array([2, 1, 2, 2])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs_val, states_val = sess.run([outputs, final_states],\n",
    "                                      feed_dict={X:X_batch, seq_length:seq_length_batch})\n",
    "    \n",
    "    print('outputs with shape: ', outputs_val.shape) \n",
    "    print(outputs_val)\n",
    "    print('==================================================================')\n",
    "    print('h states with shape: ', states_val.shape) \n",
    "    print(states_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Point 1: From code above, we know that:**\n",
    "* input `X_batch` shape: `(batch_size, num_steps, input_dim)` => (4, 2, 3)\n",
    "* `outputs_val` with shape:  `(batch_size, num_steps, output_size)` => (4, 2, 5)\n",
    "* `states_val` with shape:  `(batch_size, state_size)` => (4, 5)\n",
    "\n",
    "**Point 2: More specifically, we observe that tf.nn.dynamic_rnn returns:**\n",
    "* `outputs_val` contains hidden states for each sample in a batch over <b>every time step</b>. In this particular example:\n",
    "    * 2 steps\n",
    "    * a batch of 4 examples, each has 5 dimensions.\n",
    "\n",
    "* `states_val` contains the hidden states from last time step\n",
    "    * final state only involves one time step (i.e., the last one)\n",
    "    * a batch of 4 states, each for an example. \n",
    "\n",
    "**Point 3: Moreover:**\n",
    "* `state_size` is determined by num_units\n",
    "* `output_size` is determined by num_units of the last RNN cell (last layer)\n",
    "    * In this particular example, there is only one cell (i.e., one layer), therefore output_size == state_size, which is 5.\n",
    "\n",
    "**Point 4: sequence_length of tf.nn.dynamic_rnn**\n",
    "\n",
    "`sequence_length`: (optional) An int32/int64 vector sized [batch_size]. Used to copy-through state and zero-out outputs when past a batch element's sequence length. So it's more for correctness than performance.\n",
    "\n",
    "In the above code, we define:\n",
    "```python\n",
    "seq_length_batch = np.array([2, 1, 2, 2])\n",
    "```\n",
    "which means that the `tf.nn.dynamic_rnn` only handles:\n",
    "* the first two time steps for the first example of the batch,  \n",
    "* the first time step for the second example\n",
    "* the first two time steps for the third example \n",
    "* the first two time steps for the fourth example \n",
    "\n",
    "> Therefore, we can see that values for the second time step of the second example are zeros in the `outputs`. More importantly, since the first time step of the second example is the last time step that has been handled, the final state for the second example contains the state from the first time step rather than the second time step\n",
    "\n",
    "the state is a convenient tensor that holds the last actual RNN state, ignoring the zeros. The output tensor holds the outputs of all cells, so it doesn't ignore the zeros. That's the reason for returning both of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Layer with LSTM RNN cell \n",
    "\n",
    "* We replace BasicRNNCell with BasicLSTMCell, which has memory states and hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X batch shape: (4, 2, 3)\n",
      "outputs with shape:  (4, 2, 5)\n",
      "[[[ -2.67375764e-02  -8.80622715e-02   1.61942363e-01  -1.06723178e-02\n",
      "     2.22548679e-01]\n",
      "  [  7.19351619e-02  -3.60482305e-01   3.10523093e-01   8.92076496e-05\n",
      "     6.73704088e-01]]\n",
      "\n",
      " [[  3.88698220e-01  -2.82720029e-01   3.20574433e-01   2.17292667e-03\n",
      "     4.33897674e-01]\n",
      "  [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "     0.00000000e+00]]\n",
      "\n",
      " [[  4.13269788e-01  -3.30032617e-01   3.21764827e-01   6.56747579e-05\n",
      "     5.19900203e-01]\n",
      "  [  3.43061060e-01  -4.45168853e-01   3.73977542e-01   3.55326873e-03\n",
      "     7.03240216e-01]]\n",
      "\n",
      " [[  3.24896909e-02  -8.82205814e-02   6.35052145e-01   1.20023526e-01\n",
      "     4.86850947e-01]\n",
      "  [  1.28061742e-01  -2.86177725e-01   3.25985193e-01   1.07136607e-01\n",
      "     5.41655004e-01]]]\n",
      "==================================================================\n",
      "c states with shape:  (4, 5)\n",
      "h states with shape:  (4, 5)\n",
      "c states:\n",
      "[[ 0.07384271 -0.38438019  1.06630743  0.08183971  0.82870448]\n",
      " [ 0.49419126 -0.35714117  0.8639577   0.10446893  0.51287788]\n",
      " [ 0.39674559 -0.52737463  1.30646801  0.24093775  0.94447958]\n",
      " [ 0.17746513 -0.41843548  1.05389285  0.74125063  0.98766631]]\n",
      "h states:\n",
      "[[  7.19351619e-02  -3.60482305e-01   3.10523093e-01   8.92076496e-05\n",
      "    6.73704088e-01]\n",
      " [  3.88698220e-01  -2.82720029e-01   3.20574433e-01   2.17292667e-03\n",
      "    4.33897674e-01]\n",
      " [  3.43061060e-01  -4.45168853e-01   3.73977542e-01   3.55326873e-03\n",
      "    7.03240216e-01]\n",
      " [  1.28061742e-01  -2.86177725e-01   3.25985193e-01   1.07136607e-01\n",
      "    5.41655004e-01]]\n",
      "------------------------------------------------------------------\n",
      "detail info:  LSTMStateTuple(c=array([[ 0.07384271, -0.38438019,  1.06630743,  0.08183971,  0.82870448],\n",
      "       [ 0.49419126, -0.35714117,  0.8639577 ,  0.10446893,  0.51287788],\n",
      "       [ 0.39674559, -0.52737463,  1.30646801,  0.24093775,  0.94447958],\n",
      "       [ 0.17746513, -0.41843548,  1.05389285,  0.74125063,  0.98766631]], dtype=float32), h=array([[  7.19351619e-02,  -3.60482305e-01,   3.10523093e-01,\n",
      "          8.92076496e-05,   6.73704088e-01],\n",
      "       [  3.88698220e-01,  -2.82720029e-01,   3.20574433e-01,\n",
      "          2.17292667e-03,   4.33897674e-01],\n",
      "       [  3.43061060e-01,  -4.45168853e-01,   3.73977542e-01,\n",
      "          3.55326873e-03,   7.03240216e-01],\n",
      "       [  1.28061742e-01,  -2.86177725e-01,   3.25985193e-01,\n",
      "          1.07136607e-01,   5.41655004e-01]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "    \n",
    "input_dim = 3\n",
    "num_steps = 2\n",
    "num_units = 5\n",
    "\n",
    "# None represents the batch size. \n",
    "# We put None here, since the batch size can be different for each batch.\n",
    "X = tf.placeholder(tf.float32, [None, num_steps, input_dim], 'inputs')\n",
    "seq_length = tf.placeholder(tf.int32, [None], 'seq_length')\n",
    "\n",
    "# NOTE: we are using BasicLSTMCell here instead of BasicRNNCell\n",
    "basic_cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "\n",
    "initial_state = basic_cell.zero_state(4, tf.float32)\n",
    "\n",
    "outputs, final_states = tf.nn.dynamic_rnn(basic_cell, X, sequence_length=seq_length, \n",
    "                                          initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "# Create a batch of training examples\n",
    "# shape (4, 2, 3)\n",
    "X_batch = np.array([\n",
    "  # t = 0      t = 1\n",
    "  [[0, 1, 2], [9, 8, 7]], # instance 0\n",
    "  [[3, 4, 5], [5, 1, 9]], # instance 1\n",
    "  [[6, 7, 8], [6, 5, 4]], # instance 2\n",
    "  [[9, 0, 1], [3, 2, 1]], # instance 3 \n",
    "])\n",
    "\n",
    "print('X batch shape:', X_batch.shape)\n",
    "\n",
    "seq_length_batch = np.array([2, 1, 2, 2])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs_val, states_val = sess.run([outputs, final_states],\n",
    "                                      feed_dict={X:X_batch, seq_length:seq_length_batch})\n",
    "    \n",
    "    print('outputs with shape: ', outputs_val.shape) \n",
    "    print(outputs_val)\n",
    "    print('==================================================================')\n",
    "    print('c states with shape: ', states_val[0].shape) \n",
    "    print('h states with shape: ', states_val[1].shape) \n",
    "    print('c states:') \n",
    "    print(states_val[0])\n",
    "    print('h states:') \n",
    "    print(states_val[1])\n",
    "    print('------------------------------------------------------------------')\n",
    "    print('detail info: ', states_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Point 1: From code above, we know that:**\n",
    "* input `X_batch` shape: `(batch_size, num_steps, input_dim)` => (4, 2, 3)\n",
    "* `outputs_val` with shape:  `(batch_size, num_steps, output_size)` => (4, 2, 5)\n",
    "* <b style=\"color:red\">NOTE</b> `states_val` is a `LSTMStateTuple` containing two elements:\n",
    "    * `states_val[0]` is the internal memory states with shape:  `(batch_size, state_size)` => (4, 5)\n",
    "    * `states_val[1]` is the last step hidden states with shape:  `(batch_size, state_size)` => (4, 5)\n",
    "\n",
    "**Point 2: More specifically, we observe that tf.nn.dynamic_rnn returns:**\n",
    "* `outputs_val` contains hidden states for each sample in a batch over <b>every time step</b>. In this particular example:\n",
    "    * 2 steps\n",
    "    * a batch of 4 examples, each has 5 dimensions.\n",
    "* <b style=\"color:red\">NOTE</b> `states_val[0]` contains the memory states from last time step (Normally we do not use this)\n",
    "* <b style=\"color:red\">NOTE</b> `states_val[1]` contains the hidden states from last time step\n",
    "    * final state only involves one time step (i.e., the last one)\n",
    "    * a batch of 4 states, each for an example. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Layer with LSTM RNN cell \n",
    "\n",
    "* We construct two layer RNN with LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cell(num_units):     \n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(num_units)     \n",
    "    return lstm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X batch shape: (4, 2, 3)\n",
      "outputs with shape:  (4, 2, 5)\n",
      "[[[ 0.0065024  -0.017124   -0.01359156  0.00859271  0.00505634]\n",
      "  [ 0.03826391 -0.09002061 -0.05047503  0.06010268  0.02036073]]\n",
      "\n",
      " [[ 0.02109389 -0.05707591 -0.03460117  0.03494828  0.02418501]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.02550699 -0.06566202 -0.03883148  0.04253773  0.02695743]\n",
      "  [ 0.04090386 -0.12728092 -0.07308862  0.08742567  0.04232676]]\n",
      "\n",
      " [[ 0.01300943 -0.0356161  -0.02754521  0.02426844  0.01893609]\n",
      "  [ 0.01473426 -0.0594526  -0.07691813  0.0522584   0.04810004]]]\n",
      "==================================================================\n",
      "First Layer c states with shape:  (4, 4)\n",
      "First Layer h states with shape:  (4, 4)\n",
      "First Layer c states:\n",
      "[[-0.72883856  0.19821936  0.94745511 -0.10293541]\n",
      " [-0.38816008  0.09462205  0.69282103 -0.0132799 ]\n",
      " [-0.91657662  0.17569107  1.29657149 -0.00464094]\n",
      " [-1.31906509 -0.35433936  0.99494326  0.00202722]]\n",
      "First Layer h states:\n",
      "[[-0.15535693  0.05520145  0.73585266 -0.06237983]\n",
      " [-0.21082501  0.04196973  0.58008361 -0.0040063 ]\n",
      " [-0.21965241  0.06713828  0.82501358 -0.00286249]\n",
      " [-0.29173365 -0.14853604  0.55137366  0.00131575]]\n",
      "------------------------------------------------------------------\n",
      "Second (last) Layer c states with shape:  (4, 5)\n",
      "Second (last) Layer h states with shape:  (4, 5)\n",
      "Last Layer c states:\n",
      "[[ 0.07081604 -0.18942599 -0.12090863  0.12152738  0.03759795]\n",
      " [ 0.04002866 -0.11722505 -0.07950205  0.07046109  0.0453432 ]\n",
      " [ 0.07501453 -0.27287588 -0.17870325  0.180516    0.07745019]\n",
      " [ 0.02784172 -0.12905261 -0.16975316  0.10132997  0.08979553]]\n",
      "Last Layer h states:\n",
      "[[ 0.03826391 -0.09002061 -0.05047503  0.06010268  0.02036073]\n",
      " [ 0.02109389 -0.05707591 -0.03460117  0.03494828  0.02418501]\n",
      " [ 0.04090386 -0.12728092 -0.07308862  0.08742567  0.04232676]\n",
      " [ 0.01473426 -0.0594526  -0.07691813  0.0522584   0.04810004]]\n",
      "------------------------------------------------------------------\n",
      "detail info:  (LSTMStateTuple(c=array([[-0.72883856,  0.19821936,  0.94745511, -0.10293541],\n",
      "       [-0.38816008,  0.09462205,  0.69282103, -0.0132799 ],\n",
      "       [-0.91657662,  0.17569107,  1.29657149, -0.00464094],\n",
      "       [-1.31906509, -0.35433936,  0.99494326,  0.00202722]], dtype=float32), h=array([[-0.15535693,  0.05520145,  0.73585266, -0.06237983],\n",
      "       [-0.21082501,  0.04196973,  0.58008361, -0.0040063 ],\n",
      "       [-0.21965241,  0.06713828,  0.82501358, -0.00286249],\n",
      "       [-0.29173365, -0.14853604,  0.55137366,  0.00131575]], dtype=float32)), LSTMStateTuple(c=array([[ 0.07081604, -0.18942599, -0.12090863,  0.12152738,  0.03759795],\n",
      "       [ 0.04002866, -0.11722505, -0.07950205,  0.07046109,  0.0453432 ],\n",
      "       [ 0.07501453, -0.27287588, -0.17870325,  0.180516  ,  0.07745019],\n",
      "       [ 0.02784172, -0.12905261, -0.16975316,  0.10132997,  0.08979553]], dtype=float32), h=array([[ 0.03826391, -0.09002061, -0.05047503,  0.06010268,  0.02036073],\n",
      "       [ 0.02109389, -0.05707591, -0.03460117,  0.03494828,  0.02418501],\n",
      "       [ 0.04090386, -0.12728092, -0.07308862,  0.08742567,  0.04232676],\n",
      "       [ 0.01473426, -0.0594526 , -0.07691813,  0.0522584 ,  0.04810004]], dtype=float32)))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "    \n",
    "input_dim = 3\n",
    "num_steps = 2\n",
    "num_units = [4, 5]\n",
    "num_layers = 2\n",
    "\n",
    "# None represents the batch size. \n",
    "# We put None here, since the batch size can be different for each batch.\n",
    "X = tf.placeholder(tf.float32, [None, num_steps, input_dim], 'inputs')\n",
    "seq_length = tf.placeholder(tf.int32, [None], 'seq_length')\n",
    "\n",
    "# We are using BasicLSTMCell here instead of BasicRNNCell\n",
    "basic_cell = tf.contrib.rnn.MultiRNNCell([build_cell(num_units[idx]) for idx in range(num_layers)])\n",
    "\n",
    "initial_state = basic_cell.zero_state(4, tf.float32)\n",
    "\n",
    "outputs, final_states = tf.nn.dynamic_rnn(basic_cell, X, sequence_length=seq_length, \n",
    "                                          initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "# Create a batch of training examples\n",
    "# shape (4, 2, 3)\n",
    "X_batch = np.array([\n",
    "  # t = 0      t = 1\n",
    "  [[0, 1, 2], [9, 8, 7]], # instance 0\n",
    "  [[3, 4, 5], [5, 1, 9]], # instance 1\n",
    "  [[6, 7, 8], [6, 5, 4]], # instance 2\n",
    "  [[9, 0, 1], [3, 2, 1]], # instance 3 \n",
    "])\n",
    "\n",
    "print('X batch shape:', X_batch.shape)\n",
    "\n",
    "seq_length_batch = np.array([2, 1, 2, 2])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs_val, states_val = sess.run([outputs, final_states],\n",
    "                                      feed_dict={X:X_batch, seq_length:seq_length_batch})\n",
    "    \n",
    "    print('outputs with shape: ', outputs_val.shape) \n",
    "    print(outputs_val)\n",
    "    print('==================================================================')\n",
    "    print('First Layer c states with shape: ', states_val[0][0].shape) \n",
    "    print('First Layer h states with shape: ', states_val[0][1].shape) \n",
    "    print('First Layer c states:') \n",
    "    print(states_val[0][0])\n",
    "    print('First Layer h states:') \n",
    "    print(states_val[0][1])\n",
    "    print('------------------------------------------------------------------')\n",
    "    print('Second (last) Layer c states with shape: ', states_val[1][0].shape) \n",
    "    print('Second (last) Layer h states with shape: ', states_val[1][1].shape) \n",
    "    print('Last Layer c states:') \n",
    "    print(states_val[1][0])\n",
    "    print('Last Layer h states:') \n",
    "    print(states_val[1][1])\n",
    "    print('------------------------------------------------------------------')\n",
    "    print('detail info: ', states_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Point 1: From code above, we know that:**\n",
    "* `outputs_val` is the same as the previous two scenarios and still has shape:  `(batch_size, num_steps, output_size)` => (4, 2, 5) (with different values, of course)\n",
    "* <b style=\"color:red\">NOTE</b> `states_val` is a tuple of tuples. The outer tuple contains elements representing layers (2 in this particular example), each of which contains a `LSTMStateTuple` that in turn contains two elements, one for memory states and the other for hidden states:\n",
    "    * `states_val[0]` is the first layer LSTMStateTuple\n",
    "        * `states_val[0][0]` is the first layer memory states with shape:  `(batch_size, state_size)` => (4, 4)\n",
    "        * `states_val[0][1]` is the first layer last step hidden states with shape:  `(batch_size, state_size)` => (4, 4)\n",
    "    * `states_val[1]` is the second layer LSTMStateTuple\n",
    "        * `states_val[1][0]` is the second layer memory states with shape:  `(batch_size, state_size)` => (4, 5)\n",
    "        * `states_val[1][1]` is the second layer last step hidden states with shape:  `(batch_size, state_size)` => (4, 5)\n",
    "        \n",
    "**Point 2: Moreover:**\n",
    "* `state_size` of memory states and hidden states in a layer are the same and is determined by `num_units` of LSTM cell in that layer.\n",
    "    * In the first layer, the state_size is 4, while \n",
    "    * In thw second layer, the state_size is 5\n",
    "* <b style=\"color:red\">NOTE</b> `output_size` is determined by `num_units` of the last LSTM cell (last layer)\n",
    "    * In this particular example, the `num_units` of last (second) layer is 5. therefor the `output_size` is 5\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `outputs` contains hidden states for each sample (in a batch) over <b>every time step</b> of the <b>last layer</b>\n",
    "* `states` contains hidden states (and memory states) for each sample (in a batch) over <b>every layers</b> at the <b>last time step</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
