{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Important hyperparameters for RNN\n",
    "\n",
    "* `batch_size`: The number of training examples to feed the network in one training pass. Typically this should be set as high as you can go without running out of memory.\n",
    "* `num_steps`: (or time steps) Number of characters (or words) in sequences of a batch the network is trained on. Larger is better typically; the network will learn more long range dependencies. But it takes longer to train. \n",
    "* `lstm_units`: Number of units in the hidden layers in the LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
    "* `lstm_layers`: Number of LSTM layers in the network. I'd start with 1, then add more if I'm underfitting.\n",
    "* `learning_rate`: Learning rate\n",
    "* `keep_prob`: The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "**Note: ** the above hyperparameters can be applied to any kind of RNN, not just LSTM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word embedding\n",
    "\n",
    "* After preprocessing our data, we typically have training examples with shape `(batch_size, num_steps)` for each batch.\n",
    "    * For word-based application, we have a vocabulary with V words. Typically, each element in the training examples is a number in the range of [0, V] representing a word in the vocabulary. (In application like text generation, the range might be [1, V] and 0 may represent padding word).\n",
    "    * For character-based application, we have a vocabulary with C characters. Typically, each element in the training examples is a number in the range of [0, C] representing a character in the vocabulary. (In application like text generation, the range might be [1, V] and 0 may represent padding word).\n",
    "* We need to add an embedding layer to encode each word in the training example instead of one-hot encoding, which might work for character-based rnn problem. \n",
    "    * It is massively inefficient to one-hot encode a word since we may have a very big vocabulary base and one-hot would make the word vector extremely large. \n",
    "    * We can use an existing pre-trained word2vec representation. \n",
    "    \n",
    "* Create the embedding lookup matrix as a `tf.Variable`. Use that embedding matrix to get the embedded vectors for each word in the original training examples to pass to the LSTM cell with [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup). \n",
    "\n",
    "* [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup) takes the embedding matrix and an input tensor, typically training examples. Then, it'll return another tensor with the embedded vectors. So, if the embedding layer has 200 units, the function will return a tensor with size [batch_size, 200].\n",
    "\n",
    "```\n",
    "embedding_lookup(\n",
    "    params,\n",
    "    ids,\n",
    "    partition_strategy='mod',\n",
    "    name=None,\n",
    "    validate_indices=True,\n",
    "    max_norm=None\n",
    ")\n",
    "```\n",
    "\n",
    "* `params`: Basically, it is <b>embedding lookup matrix</b>. More specifically, A single tensor representing the complete embedding tensor, or a list of P tensors all of same shape except for the first dimension, representing sharded embedding tensors. Alternatively, a PartitionedVariable, created by partitioning along dimension 0. Each element must be appropriately sized for the given partition_strategy.\n",
    "* `ids`: A Tensor with type int32 or int64 containing the ids (or indexes) to be looked up in params.\n",
    "* `partition_strategy`: A string specifying the partitioning strategy, relevant if len(params) > 1. Currently \"div\" and \"mod\" are supported. Default is \"mod\".\n",
    "\n",
    "returns a dense tensor:\n",
    "* with shape: `shape(ids) + shape(params)[1:]`.\n",
    "* with type: the same as the tensors in params."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "embed_size = 300 \n",
    "with graph.as_default():\n",
    "    # create a embedding lookup matrix \n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_dim), -1, 1), name = \"embedding\")\n",
    "    # get embeded vector for each word in the inputs\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "```\n",
    "\n",
    "The code above creates a embedding lookup matrix with shape (n_words, embed_size), where `n_words` is the size of the word vocabulary and `embed_dim` is <b>embedding dimension</b> that is the dimension of the vector representing a word after embedding. \n",
    "\n",
    "After embedding lookup, the training examples would have shape `(batch_size, num_steps, embed_dim)` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define type of cell for RNN\n",
    "\n",
    "Next, we'll create our cells (e.g., LSTM, GRU) to use in the [recurrent network](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn). Here we are just defining what the cells look like. This isn't actually building the graph, just defining the type of cells we want in our graph. There are many [types of cells](https://www.tensorflow.org/api_guides/python/contrib.rnn#Core_RNN_Cells_for_use_with_TensorFlow_s_core_RNN_methods):\n",
    "\n",
    "* tf.contrib.rnn.BasicRNNCell\n",
    "* tf.contrib.rnn.BasicLSTMCell\n",
    "* tf.contrib.rnn.GRUCell\n",
    "* tf.contrib.rnn.LSTMCell\n",
    "* tf.contrib.rnn.LayerNormBasicLSTMCell\n",
    "\n",
    "If we want to create a basic LSTM cell for the graph, we will want to use [tf.contrib.rnn.BasicLSTMCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell). Looking at the function documentation:\n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.BasicLSTMCell(num_units, forget_bias=1.0, state_is_tuple=True, activation=None, reuse=None)\n",
    "```\n",
    "\n",
    "* `num_units`: int, The number of units in the LSTM cell.\n",
    "* `forget_bias`: float, The bias added to forget gates (see above). Must set to 0.0 manually when restoring from CudnnLSTM-trained checkpoints.\n",
    "* `state_is_tuple`: If True, accepted and returned states are 2-tuples of the c_state and m_state. If False, they are concatenated along the column axis. The latter behavior will soon be deprecated.\n",
    "* `activation`: Activation function of the inner states. Default: tanh.\n",
    "* `reuse`: (optional) Python boolean describing whether to reuse variables in an existing scope. If not True, and the existing scope already has the given variables, an error is raised.\n",
    "\n",
    "you can see it takes a parameter called `num_units`, the number of units in the cell. So then, you can write something like \n",
    "\n",
    "```python\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "```\n",
    "\n",
    "to create an LSTM cell with `num_units`. \n",
    "\n",
    "Next, you can add dropout to the cell with [`tf.contrib.rnn.DropoutWrapper`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper). This just wraps the cell in another cell, but with dropout added to the inputs and/or outputs. It's a really convenient way to make your network better with almost no effort! So you'd do something like\n",
    "\n",
    "```python\n",
    "drop = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "```\n",
    "\n",
    "* `cell`: an RNNCell, a projection to output_size is added to it.\n",
    "* `input_keep_prob`: unit Tensor or float between 0 and 1, input keep probability; if it is constant and 1, no input  dropout will be added.\n",
    "* `output_keep_prob`: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added.\n",
    "* and many other arguments\n",
    "\n",
    "**Multiple layers of RNN network**\n",
    "\n",
    "Most of the time, your network will have better performance with more layers. That's sort of the magic of deep learning, adding more layers allows the network to learn really complex relationships. Again, there is a simple way to create multiple layers of LSTM cells with [`tf.contrib.rnn.MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell):\n",
    "\n",
    "```python\n",
    "def build_cell(num_units, keep_prob):     \n",
    "      lstm = tf.contrib.rnn.BasicLSTMCell(num_units)     \n",
    "      drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)      \n",
    "      return drop  \n",
    "      \n",
    "cell = tf.contrib.rnn.MultiRNNCell([build_cell(num_units, keep_prob) for _ in range(num_layers)])\n",
    "```\n",
    "\n",
    "The `build_cell()` function build a cell we described above. The `MultiRNNCell` wrapper builds this into multiple layers of RNN cells, one for each cell in the list.\n",
    "\n",
    "The final `MultiRNNCell` you're using in the network is actually multiple (or just one) cells with typically dropout. But it all works the same from an achitectural viewpoint, just a more complicated graph in the cell.\n",
    "\n",
    "> NOTE: each RNN cell represents a layer. It is just that when we unroll a RNN cell in terms of time steps, it is like we have multiple RNN cells per layer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RNN forward pass\n",
    "\n",
    "We need to actually run the data through the RNN nodes. You can use [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) to do this. \n",
    "\n",
    "You'd pass in the RNN `cell` you created (our multiple layered LSTM cell for instance), the `inputs` to the network and the `initial_state`:\n",
    "\n",
    "```python\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)\n",
    "```\n",
    "\n",
    "the `initial_state` is the initial values of the cell state that is passed between the hidden layers in successive time steps. `tf.nn.dynamic_rnn` takes care of most of the work for us. We pass in our cell and the input to the cell. Then it does the unrolling and everything else for us.\n",
    "\n",
    "* `cell`: An instance of RNNCell.\n",
    "* `inputs`: The RNN inputs. \n",
    "    * If time_major == False (default), this must be a Tensor of shape: [batch_size, max_time, ...], or a nested tuple of such elements. \n",
    "    * If time_major == True, this must be a Tensor of shape:[max_time, batch_size, ...], or a nested tuple of such elements. \n",
    "        * <b>The first two dimensions must match across all the inputs</b>, but otherwise the ranks and other shape components may differ. In this case, \n",
    "        * Input to cell at each time-step will replicate the structure of these tuples, except for the time dimension (from which the time is taken). <b>The input to cell <b style='color:red'>at each time step</b> will be a Tensor or (possibly nested) tuple of Tensors each with dimensions [batch_size, ...]<b>.\n",
    "\n",
    "* `initial_state`: (optional) An initial state for the RNN. If cell.state_size is an integer, this must be a Tensorof appropriate type and shape [batch_size, cell.state_size]. If cell.state_size is a tuple, this should be a tuple of tensors having shapes [batch_size, s] for s in cell.state_size.\n",
    "\n",
    "Create tensor for initial states with all zeros:\n",
    "\n",
    "```python\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "```\n",
    "\n",
    "`tf.nn.dynamic_rnn` returns `outputs` for each time step and the <b style='color:red'>final</b> `state` of the hidden layer.\n",
    "\n",
    "> Note that, unlike outputs, the final `state` doesn't contain information about every time step, but only about the last one (that is, the state after the last one)\n",
    "\n",
    "A pair `(outputs, state)` where:\n",
    "\n",
    "* `outputs`: The RNN output Tensor.\n",
    "    * If time_major == False (default), this will be a Tensor shaped: `[batch_size, max_time, cell.output_size]`.\n",
    "    * If time_major == True, this will be a Tensor shaped: `[max_time, batch_size, cell.output_size]`.\n",
    "\n",
    "* `state`: The final state. If cell.state_size is an int, this will be shaped [batch_size, cell.state_size]. If it is a TensorShape, this will be shaped [batch_size] + cell.state_size. If it is a (possibly nested) tuple of ints or TensorShape, this will be a tuple having the corresponding shapes. If cells are LSTMCells state will be a tuple containing a LSTMStateTuple for each cell.\n",
    "\n",
    "**Typeical scenario for inputs and outputs**\n",
    "* The inputs to the `tf.nn.dynamic_rnn` is of shape `[batch_size, max_time, embed_dim]`\n",
    "* The inputs to cell <b style='color:red'>at each time step</b> will have shape of `[batch_size, embed_dim]`\n",
    "* The outputs from `tf.nn.dynamic_rnn` have shape of `[batch_size, max_time, cell.output_size]`\n",
    "    * the cell.output_size typically is the `lstm_units` of the cell in the last layer of LSTM network if the network has multiple LSTM layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More investigation on tf.nn.dynamic_rnn** from [here](https://stats.stackexchange.com/questions/330176/what-is-the-output-of-a-tf-nn-dynamic-rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X batch shape: (4, 2, 3)\n",
      "outputs with shape:  (4, 2, 5)\n",
      "[[[ 0.93291515  0.50239432 -0.43230817  0.75407064 -0.0769805 ]\n",
      "  [ 0.99999219  0.99999034 -0.99999845 -0.99962378 -0.9878065 ]]\n",
      "\n",
      " [[ 0.99956155  0.99439365 -0.99177754  0.11601707 -0.69421184]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.99999726  0.99995226 -0.99991405 -0.63469619 -0.92676276]\n",
      "  [ 0.99834806  0.99900055 -0.99992067 -0.98323303 -0.97121006]]\n",
      "\n",
      " [[-0.95585638  0.99917287 -0.99539107 -0.99984741 -0.99911404]\n",
      "  [ 0.94049209  0.87342799 -0.97667748 -0.67169487 -0.92319804]]]\n",
      "----------------------\n",
      "states with shape:  (4, 5)\n",
      "[[ 0.99999219  0.99999034 -0.99999845 -0.99962378 -0.9878065 ]\n",
      " [ 0.99956155  0.99439365 -0.99177754  0.11601707 -0.69421184]\n",
      " [ 0.99834806  0.99900055 -0.99992067 -0.98323303 -0.97121006]\n",
      " [ 0.94049209  0.87342799 -0.97667748 -0.67169487 -0.92319804]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "    \n",
    "input_dim = 3\n",
    "num_steps = 2\n",
    "num_units = 5\n",
    "\n",
    "# None represents the batch size. \n",
    "# We put None here, since the batch size can be different for each batch.\n",
    "X = tf.placeholder(tf.float32, [None, num_steps, input_dim], 'inputs')\n",
    "seq_length = tf.placeholder(tf.int32, [None], 'seq_length')\n",
    "\n",
    "# basic_cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units)\n",
    "\n",
    "initial_state = basic_cell.zero_state(4, tf.float32)\n",
    "\n",
    "outputs, final_states = tf.nn.dynamic_rnn(basic_cell, X, sequence_length=seq_length, \n",
    "                                          initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "# Create a batch of training examples\n",
    "# shape (4, 2, 3)\n",
    "X_batch = np.array([\n",
    "  # t = 0      t = 1\n",
    "  [[0, 1, 2], [9, 8, 7]], # instance 0\n",
    "  [[3, 4, 5], [5, 1, 9]], # instance 1\n",
    "  [[6, 7, 8], [6, 5, 4]], # instance 2\n",
    "  [[9, 0, 1], [3, 2, 1]], # instance 3 \n",
    "])\n",
    "\n",
    "print('X batch shape:', X_batch.shape)\n",
    "\n",
    "seq_length_batch = np.array([2, 1, 2, 2])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs_val, states_val = sess.run([outputs, final_states],\n",
    "                                      feed_dict={X:X_batch, seq_length:seq_length_batch})\n",
    "    \n",
    "    print('outputs with shape: ', outputs_val.shape) \n",
    "    print(outputs_val)\n",
    "    print('----------------------')\n",
    "    print('states with shape: ', states_val.shape) \n",
    "    print(states_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From code above, we know that:**\n",
    "* input X batch shape: (batch_size, num_steps, input_dim) => (4, 2, 3)\n",
    "* outputs with shape:  (batch_size, num_steps, cell.output_size) => (4, 2, 5)\n",
    "* states with shape:  (batch_size, cell.state_size) => (4, 5)\n",
    "\n",
    "**More specifically, we observe that tf.nn.dynamic_rnn returns:**\n",
    "* `outputs` contains information about every time step. In this particular example:\n",
    "    * 2 steps\n",
    "    * a batch of four examples, each has 5 dimensions.\n",
    "\n",
    "* `state` contains the state from last time step\n",
    "    * final state only involves one time step (i.e., the last one)\n",
    "    * a batch of four states, each for an example. \n",
    "\n",
    "**Moreover:**\n",
    "* `cell.state_size` is determined by num_units\n",
    "* `cell.output_size` is determined by num_units of the last RNN cell (last layer)\n",
    "    * In this particular example, there is only one cell (i.e., one layer), therefore cell.output_size == cell.state_size, which is 5.\n",
    "\n",
    "**sequence_length of tf.nn.dynamic_rnn**\n",
    "\n",
    "`sequence_length`: (optional) An int32/int64 vector sized [batch_size]. Used to copy-through state and zero-out outputs when past a batch element's sequence length. So it's more for correctness than performance.\n",
    "\n",
    "In the above code, we define:\n",
    "```python\n",
    "seq_length_batch = np.array([2, 1, 2, 2])\n",
    "```\n",
    "which means that the `tf.nn.dynamic_rnn` only handles:\n",
    "* the first two time steps for the first example of the batch,  \n",
    "* the first time step for the second example\n",
    "* the first two time steps for the third example \n",
    "* the first two time steps for the fourth example \n",
    "\n",
    "> Therefore, we can see that values for the second time step of the second example are zeros in the `outputs`. More importantly, since the first time step of the second example is the last time step that has been handled, the final state for the second example contains the state from the first time step rather than the second time step\n",
    "\n",
    "the state is a convenient tensor that holds the last actual RNN state, ignoring the zeros. The output tensor holds the outputs of all cells, so it doesn't ignore the zeros. That's the reason for returning both of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Output Layer of RNN\n",
    "\n",
    "We only care about the final output, we'll be using that as our sentiment prediction. So we need to grab the last output with outputs[:, -1], the calculate the cost from that and labels_.\n",
    "\n",
    "```python\n",
    "def build_output(last_layer_input, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        lstm_output: List of output tensors from the LSTM layer\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape seq_output to a 2D tensor with lstm_size columns\n",
    "    x = tf.reshape(lstm_output, [-1, in_size])\n",
    "\n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        # Create the weight and bias variables here\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.add(tf.matmul(x, softmax_w), softmax_b)\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits, name=\"predictions\")\n",
    "    \n",
    "    return out, logits\n",
    "    \n",
    "\n",
    "predictions, logits = build_output(last_layer_input, in_size, out_size) \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimizer for RNN\n",
    "\n",
    "Normal RNNs have have issues gradients exploding and vanishing. LSTMs fix the gradients vanishing problem, but the gradients can still grow without bound. To fix this, we can clip the gradients above some threshold. That is, if a gradient is larger than that threshold, we set it to the threshold. This will ensure the gradients never grow overly large. Then we use an AdamOptimizer for the learning step.\n",
    "\n",
    "```python\n",
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        grad_clip:\n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    # grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    grads = tf.gradients(loss, tvars)\n",
    "    clip_grads, _ = tf.clip_by_global_norm(grads , grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(clip_grads, tvars))\n",
    "    \n",
    "    return optimizer\n",
    "```\n",
    "\n",
    "`tf.trainable_variables()` creates a list of all the variables we've defined in our graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. validation accuracy\n",
    "\n",
    "```python\n",
    "correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batching\n",
    "\n",
    "This is a simple function for returning batches from data. \n",
    "* First it removes data such that we only have full batches. \n",
    "* Then it iterates through the `x` and `y` arrays and returns slices out of those arrays with size `[batch_size]`.\n",
    "\n",
    "```python\n",
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    \n",
    "    # Only get full batches\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    \n",
    "    # returns slices with size [batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "\n",
    "** Below is the typical training pseudo code: **\n",
    "\n",
    "```\n",
    "create a session for the specified graph\n",
    "{\n",
    "    initialize all global variables\n",
    "\n",
    "    iterate epochs\n",
    "    {\n",
    "            iterate batches\n",
    "            {\n",
    "                create graph input map for current run\n",
    "\n",
    "                evalute the graph for current run with input map, and \n",
    "                return results for specified tensors or operations\n",
    "\n",
    "                (for certain # of iterations) print training process information, such as loss and accuracy\n",
    "\n",
    "                (for certain # of iterations) validate the graph while training\n",
    "            }\n",
    "    } \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** python code: **\n",
    "\n",
    "```python\n",
    "epochs = 10\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "# create a session for the specified graph\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    \n",
    "    # initialize all global variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    \n",
    "    # iterate epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # evaluates the initial_state tensor and returns the results.\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        # iterate batches\n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            \n",
    "            # create graph input map\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y[:, None],   # change ths shape of y to [batch_size, 1]\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            \n",
    "            # evalute the graph for one run with input map, and\n",
    "            # return results for specified tensors or operations\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            # print training process information\n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            # print validation information while training\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y[:, None],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "                        \n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about the initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "The logic of testing process is quite similar to training except that \n",
    "* No need to run epochs and validate the graph\n",
    "* Get evaluating results of different tensors or operations for each graph run \n",
    "\n",
    "\n",
    "** python code: **\n",
    "\n",
    "```python\n",
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    \n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    # any difference with state = sess.run(initial_state) ???\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    \n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        \n",
    "        feed = {inputs_: x,\n",
    "                labels_: y[:, None],\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        \n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        \n",
    "        test_acc.append(batch_acc)\n",
    "        \n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is [a tutorial on building RNNs](https://www.tensorflow.org/tutorials/recurrent) that will help you out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
