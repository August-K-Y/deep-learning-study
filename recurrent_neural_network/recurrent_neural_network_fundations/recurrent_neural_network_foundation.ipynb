{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Recurrent Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RNN works on data that are sequences\n",
    "\n",
    "> Sequence involves a factor of time. Elements in a sequence appear or occur in chronological order\n",
    "\n",
    "* and exploits\n",
    "\n",
    "> States of elements in sequence. States is defined as the output of hidden layer neurons, which will serve as additional input to the network during next training (time) step.\n",
    "\n",
    "* Suppose a sequence is of length T (T observations/elements). Then, this sequence would be: \n",
    "\n",
    "$$ X(1), X(2),...,X(t) $$\n",
    "\n",
    "* If each element in this sequence has D features, this sequence is a $T \\times D $ matrix\n",
    "* All sequeces would have the shape $ N \\times T \\times D $\n",
    "* Sequences may have arbitrary length (e.g., sound, music or sentences)\n",
    "    * We can store elements of each sequence in a list \n",
    "    * The nth sequence would be a list E(n) containing elements of the nth sequence and each element has D features\n",
    "    * The nth sequence is of shape $ len(E(n)) \\times D $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Regular feedforward unit:**\n",
    "    \n",
    "<img src=\"images/forward.png\" alt=\"forward\" style=\"width:50%;height:50%\"/>\n",
    "    \n",
    "** Recurrent unit: **\n",
    "    \n",
    "* The recurrent unit has a feedback connection from hidden layer to itself. The feedback loop implies there is a delay of one time unit. \n",
    "\n",
    "<img src=\"images/simple_recurrent_unit.png\" alt=\"forward\" style=\"width:50%;height:50%\"/>\n",
    "\n",
    "\n",
    "* Recurrent Unit Internals\n",
    "\n",
    "Let’s take a closer look at what is going on inside a recurrent unit.\n",
    "<img src=\"images/recurrent_unit_internal.png\" alt=\"forward\" style=\"width:90%;height:100%\"/>\n",
    "\n",
    "* The input word vector $x_t$ is first multiplied by the weight matrix: $W_x$\n",
    "* Then bias values are added to produce our first intermediate result: $x_t W_x + b$ \n",
    "* Meanwhile, the state vector from the previous time step $h_{t-1}$ is multiplied with another weight matrix $ W_h $ to produce our second intermediate result: $h_{t-1} W_h$ \n",
    "* These two are then added together, and passed through an activation function such as ReLU, sigmoid or tanh to produce the state for the current time step: $h_t$\n",
    "* This state vector $h_t$ is passed on as input to the next fully-connected layer, that applies another weight matrix, bias and activation to produce the output: $y_t$\n",
    "\n",
    "> The key thing to note here is that the RNN’s state $h_t$ is used to produce the output $y_t$ as well as looped back to produce the next state.\n",
    "\n",
    "* In Math\n",
    "\n",
    "$$ h(t) = f(W^T_h h(t-1) + W^T_x x(t) + b_h) $$\n",
    "\n",
    "$$ y(t) = softmax(W^T_o h(t) + b_o) $$\n",
    "\n",
    "where $f()$ certain non-linear activation function such as sigmoid, tanh or relu\n",
    "\n",
    "* How big is $ W_h $ ?\n",
    "    * if the hidden layer contains $M$ neuros, each neuro would be connected to all other neuros in the hidden layer:\n",
    "        * first neuro connects back to all $M$ neuros, second neuro connects back to all $M$ neuros, ..., $M$th neuro connects back to all $M$ neuros. Therefore, $ W_h $ is $ M \\times M $\n",
    "\n",
    "> Let's say you've decided to use a word vector ($x_t$) of length 200, and a state vector ($h_t$) of length 300. Treating these as single-row matrices, we can write the sizes as 1x200 and 1x300 respectively. Then, the input weight matrix ($W_x$) has shape 200x300, the recurrent weight matrix ($W_h$) has shape 300x300 and bias vector $b$ has shape 1x300.\n",
    "\n",
    "* Note that this is not Markov, since $ h(t) $ is defined by $ h(t-1) $, which in turns is defined by $ h(t-2) $, all the way down to $ h(0) $, which is different from Markov property that each state only depends on its previous states. \n",
    "\n",
    "\n",
    "$$ h(t) = f(W^T_h h(t-1) + W^T_x x(t) + b_h) $$\n",
    " \n",
    "$$ h(t - 1) = f(W^T_h h(t-2) + W^T_x x(t-1) + b_h) $$\n",
    "   \n",
    "$$  h(0) = ... $$\n",
    "\n",
    "* $ h(0) $ is a hyperparameter. We can initialize it to 0\n",
    "\n",
    "* We can add more than one recurrent unit to neural network\n",
    "\n",
    "<img src=\"images/two_recurrent_units.png\" alt=\"forward\" style=\"width:50%;height:50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Intuition by unfolding recurrent unit**\n",
    "\n",
    "* We visualize a recurrent neural network by unfolding it in time to a feedforward neural network, as shown below.\n",
    "* We have a sequence of elements, each is represented by a hidden layer in the feedforward neural network. At each hidden layer, we want to make a decision about what happened so far in this sequence. \n",
    "* If your sequence is reasonably stationary, you can use the same classifier at each hidden layer in time.\n",
    "* Because this is a sequence, you also want to take into account the past - everything that happened before each hidden layer.  \n",
    "\n",
    "> One natural thing to do is to use the states of the previous hidden layer as a summary of what happened before recursively. We use the same model $h$ to model the states of each hidden layer. As shown below, at each hidden layer, it takes into account the previous hidden layer's states multiplying the weight matrix between the two hidden layers.\n",
    "\n",
    "<img src=\"images/unfold_rnn_1.png\" alt=\"forward\" style=\"width:65%;height:65%\"/>\n",
    "\n",
    "* Since at each hidden layer, we take in previous hidden layer's states multiplying a distinct weight matrix $ W_1, W_2, ..., W_3, or ... $, we would need a very deep neural network to remember far in the past since a sequence may involve hundreds or even thousands steps.\n",
    "\n",
    "> A more compact way is to use a <b style=\"color:red\">shared weights</b> $ W_h $ between each pair of adjacent layers in the sequence. Similarly, we use <b style=\"color:red\">shared weights</b> $ W_x $ for all the inputs $ x(1), x(2),..., x(t),...$ into the hidden layers and <b style=\"color:red\">shared weights</b> $ W_o $ for outputs of all layers. We would end up with a network with a relatively simple repeating pattern.\n",
    "\n",
    "<img src=\"images/unfold_rnn_2.png\" alt=\"forward\" style=\"width:65%;height:65%\"/>\n",
    "\n",
    "\n",
    "<img src=\"images/rnn_3.png\" alt=\"forward\" style=\"width:40%;height:40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take Away**\n",
    "* The simple recurrent unit is the foundation for other variations on recurrent neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Through Time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rated Recurrent Unit\n",
    "\n",
    "* The rated recurrent unit weights 2 things:\n",
    "    * $ h(t-1) $, the previous value of hidden state\n",
    "    * $ f(x(t), h(t-1)) $, output that we would have gotten from a regular recurrent unit \n",
    "\n",
    "* We use a rate/update gate z, which is a matrix, to do the weighting\n",
    "* The rated recurrent unit architecture:\n",
    "\n",
    "<img src=\"images/rrnn.png\" alt=\"forward\" style=\"width:65%;height:65%\"/>\n",
    "\n",
    "* The mathematical formula for calculating current hidden state is as:\n",
    "\n",
    "$$ h(t) = (1-z(t)) \\circ h(t-1) + z(t) \\circ f(x(t), h(t-1)) $$\n",
    "\n",
    "where $ \\circ $ indicates element-by-element multiplication\n",
    "\n",
    "* we define <b style=\"color:red\">$ \\hat{h}(t) = f(x(t), h(t-1)) $</b>, which can be considered as candidate hidden state that will go through the rating gate z. Then we rewrite the mathematical formula for calculating current hidden state:\n",
    "\n",
    "<b style=\"color:red\">$$ h(t) = (1-z(t)) \\circ h(t-1) + z(t) \\circ \\hat{h}(t) $$</b>\n",
    "\n",
    "> Intuitively, the rating gate z is choosing between the old state value $ h(t-1) $  and the new state value $ \\hat{h} $\n",
    "\n",
    "** Calculating candidate hidden value $\\hat{h}$**\n",
    "$$ \\hat{h}(t) = f(x(t), h(t-1)) = f(x(t)W_{xh} + h(t-1)W_{hh} + b_h)$$\n",
    "\n",
    "** Calculating rating gate z**\n",
    "* Many options for how z can be calculated: as a weight parameer, as a function of x, etc...\n",
    "* One common way is calculating z as a function of $ x(t) $ and $ h(t-1) $ \n",
    "\n",
    "$$ z(t) = \\sigma(x(t)W_{xz} + h(t-1)W_{hz} + b_z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit\n",
    "\n",
    "* The Gate Recurrent Unit adds one more gate called reseting gate to the Rated Recurrent Unit.\n",
    "\n",
    "<img src=\"images/grn.png\" alt=\"forward\" style=\"width:65%;height:65%\"/>\n",
    "\n",
    "* We calculate the current hidden state $ h(t) $ the same way as we calculate it for Rated Recurrent Unit:\n",
    "\n",
    "$$ h(t) = (1-z(t)) \\circ h(t-1) + z(t) \\circ \\hat{h}(t) $$\n",
    "\n",
    "* $ \\hat{h} $ is calculated by multiplying reset gate r with the previous hidden state $ h(t-1) $. The gate g controls how much of the previous hidden state we will consider when we create the candidate hidden state. In other words, it has the ability to reset the hidden state:\n",
    "\n",
    "<b style=\"color:red\">$$ \\hat{h}(t) = f(x(t), r(t) \\circ h(t-1)) $$</b> \n",
    "\n",
    "** Calculating candidate hidden value $\\hat{h}$**\n",
    "$$ \\hat{h}(t) = f(x(t), h(t-1)) = f(x(t)W_{xh} + (r(t) \\circ h(t-1))W_{hh} + b_h)$$\n",
    "\n",
    "** Calculating rating gate z**\n",
    "$$ z(t) = \\sigma(x(t)W_{xz} + h(t-1)W_{hz} + b_z)$$\n",
    "\n",
    "** Calculating reset gate r**\n",
    "$$ r(t) = \\sigma(x(t)W_{xr} + h(t-1)W_{hr} + b_r)$$\n",
    "\n",
    "### Take Away\n",
    "* Compared with Rated Recurrent Units\n",
    "    * GRU adds a reset gate r\n",
    "    * GRU calculates $ \\hat{h} $ considering the r\n",
    "    * All other components are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory (LSTM)\n",
    "\n",
    "* The concepts of LSTM are quite similar to those of GRU\n",
    "* LSTM has superior performance and it does not easily succumb to the vanishing gradient problem\n",
    "* Compared with GRU, LSTM adds complexity in terms of more components\n",
    "* Basically, LSTM have:\n",
    "    * Three gates: input gate $ i(t) $, output gate $ o(t) $ and forget gate $ f(t) $\n",
    "    * A memory cell $ c(t) $ \n",
    "    * a candidate memory cell $ \\hat{c}(t) $, which take place of $ \\hat{h}(t) $ \n",
    "* The LSTM architecture:\n",
    "   \n",
    "<img src=\"images/lstm.png\" alt=\"forward\" style=\"width:65%;height:65%\"/>\n",
    "\n",
    "> The input gate controls how much of the new memory cell value goes into current memory cell; \n",
    "\n",
    "> The forget gate controls how much of the previous memory cell value goes into current memory cell; \n",
    "\n",
    "> The output gate control how much of current memory cell value goes into the hidden state\n",
    "\n",
    "* The mathematical formula for calculating current hidden state is:\n",
    "\n",
    "$$ h(t) = o(t)g(c(t))$$\n",
    "\n",
    "* The mathematical formula caculating the memory cell is:\n",
    "\n",
    "$$ c(t) = f(t)c(t-1) + i(t)\\hat{c}(t)$$\n",
    "\n",
    "* where the candidate memory cell $ \\hat{c}(t) $ is calculated by:\n",
    "\n",
    "$$ \\hat{c}(t) = g(x(t)W_{xc} + h(t-1)W_{hc} + b_c)$$\n",
    "\n",
    "** Calculating input gate $i$**\n",
    "$$ i(t) = \\sigma(x(t)W_{xi} + h(t-1)W_{hi} + c(t-1)W_{ci} + b_i)$$\n",
    "\n",
    "** Calculating forget gate $f$**\n",
    "$$ f(t) = \\sigma(x(t)W_{xf} + h(t-1)W_{hf} + c(t-1)W_{cf} + b_f)$$\n",
    "\n",
    "** Calculating output gate $o$**\n",
    "$$ o(t) = \\sigma(x(t)W_{xo} + h(t-1)W_{ho} + c(t)W_{co} + b_o)$$\n",
    "\n",
    "* Notice that: different from $i(t)$ and $f(t)$, the output gate $o(t)$ depends on $c(t)$, not $c(t-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take Away\n",
    "\n",
    "**Modularization**\n",
    "* Rated recurrent unit, gated recurrent unit and LSTM are different ways to architect the recurrent net unit. We can treat the recurrent unit as a black box and modularize it since the specific way of how recurrent unit works is internal detail. Thus, we can plug-in different recurrent unit as needed.\n",
    "\n",
    "**Simple, but not Simpler**\n",
    "* You may tend to use more complex model such as GRUs and LSTMs\n",
    "* But if a simpler model solves the problem completely, we should stop there\n",
    "* Do not add more training time + possibility of overfitting\n",
    "* [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, Chung 2014](https://arxiv.org/pdf/1412.3555.pdf)\n",
    "* Interesting points:\n",
    "    * Deep learning is not that common in Kaggle\n",
    "    * Many contestants do feature engineering + simpler models\n",
    "    * Deep learning is interesting because it touches a lot different but related fields including computational neuroscience, machine learning and AI\n",
    "    * For basic tasks, a very simple and fast model may be the best choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
