{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated_Recurrent_Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compared with Rated Recurrent Unit (RRU), Gated Recurrent Unit (GRU) simple adds one more gate and more parameters, which make the model more expressive than that of RRU.\n",
    "* In terms of code, writing GRU should be relatively trivial if you already understand the simple recurrent unit and RRU. We just need to add more weights and modify the recurrence function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularization\n",
    "\n",
    "* Since Rated recurrent unit, gated recurrent unit and LSTM are different ways to architect the recurrent net unit, we can treat the recurrent unit as a black box and modularize it. Thus, we can plug-in different recurrent unit as needed.\n",
    "* In this notebook, we will create a Gated Recurrent Unit class that can be reused by recurrent net work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import nltk\n",
    "import operator\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/yankang/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight(Mi, Mo):\n",
    "    return np.random.randn(Mi, Mo) / np.sqrt(Mi + Mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(object):\n",
    "    def __init__(self, Mi, Mo, activation):\n",
    "        self.Mi = Mi\n",
    "        self.Mo = Mo\n",
    "        self.f = activation\n",
    "        \n",
    "        W_xr = init_weight(Mi, Mo)\n",
    "        W_hr = init_weight(Mo, Mo)\n",
    "        br = np.zeros(Mo)\n",
    "        \n",
    "        W_xz = init_weight(Mi, Mo)\n",
    "        W_hz = init_weight(Mo, Mo)\n",
    "        bz = np.zeros(Mo)\n",
    "        \n",
    "        W_xh = init_weight(Mi, Mo)\n",
    "        W_hh = init_weight(Mo, Mo)\n",
    "        bh = np.zeros(Mo)\n",
    "        h0 = np.zeros(Mo)\n",
    "        \n",
    "        self.W_xr = theano.shared(W_xr)\n",
    "        self.W_hr = theano.shared(W_hr)\n",
    "        self.br = theano.shared(br)\n",
    "        \n",
    "        self.W_xz = theano.shared(W_xz)\n",
    "        self.W_hz = theano.shared(W_hz)\n",
    "        self.bz = theano.shared(bz)\n",
    "        \n",
    "        self.W_xh = theano.shared(W_xh)\n",
    "        self.W_hh = theano.shared(W_hh)\n",
    "        self.bh = theano.shared(bh)\n",
    "        self.h0 = theano.shared(h0)\n",
    "        \n",
    "        self.params = [self.W_xr, self.W_hr, self.br, self.W_xz, self.W_hz, self.bz, self.W_xh, \n",
    "                       self.W_hh, self.bh, self.h0]\n",
    "        \n",
    "    def recurrence(self, x_t, h_t1):\n",
    "        z_t = T.nnet.sigmoid(x_t.dot(self.W_xz) + h_t1.dot(self.W_hz) + self.bz)\n",
    "        r_t = T.nnet.sigmoid(x_t.dot(self.W_xr) + h_t1.dot(self.W_hr) + self.br)\n",
    "        h_hat_t = self.f(x_t.dot(self.W_xh) + (r_t * h_t1).dot(self.W_hh) + self.bh)\n",
    "        h_t = (1 - z_t) * h_t1 + z_t * h_hat_t\n",
    "        return h_t\n",
    "    \n",
    "    def output(self, X):\n",
    "        h, _ = theano.scan(\n",
    "                fn=self.recurrence,\n",
    "                sequences=X,\n",
    "                outputs_info=[self.h0],\n",
    "                n_steps=X.shape[0],\n",
    "        )\n",
    "        return h\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "    def __init__(self, D, hidden_layer_sizes, V):\n",
    "        self.V = V\n",
    "        self.D = D\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        \n",
    "    def fit(self, X, learning_rate=1e-5, mu=0.99, epochs=10, activation=T.nnet.relu, show_fig=True, \n",
    "            RecurrentUnit=GRU, normalize=True):\n",
    "        \n",
    "        V = self.V\n",
    "        D = self.D\n",
    "        N = len(X)\n",
    "        \n",
    "        ### initialize hidden layers (i.e., recurrent units)\n",
    "            \n",
    "        self.hidden_layers = []\n",
    "        Mi = D\n",
    "        for Mo in self.hidden_layer_sizes:\n",
    "            ru = RecurrentUnit(Mi, Mo, activation)\n",
    "            self.hidden_layers.append(ru)\n",
    "            Mi = Mo\n",
    "        \n",
    "        ### initialize weights for word embedding layer and output layer\n",
    "                \n",
    "        We = init_weight(V, D)\n",
    "        Wo = init_weight(Mi, V)\n",
    "        bo = np.zeros(V)\n",
    "        \n",
    "        self.We = theano.shared(We)\n",
    "        self.Wo = theano.shared(Wo)\n",
    "        self.bo = theano.shared(bo)\n",
    "        self.params = [self.Wo, self.bo]\n",
    "        for ru in self.hidden_layers:\n",
    "            self.params += ru.params\n",
    "        \n",
    "        ### create input training vectors\n",
    "        \n",
    "        thx = T.ivector('X')\n",
    "        thy = T.ivector('Y')\n",
    "        \n",
    "        ### forward propagation\n",
    "        \n",
    "        Z = self.We[thx]\n",
    "        for ru in self.hidden_layers:\n",
    "            Z = ru.output(Z)\n",
    "            \n",
    "        py_x = T.nnet.softmax(Z.dot(self.Wo) + self.bo)\n",
    "        \n",
    "        prediction = T.argmax(py_x, axis=1)\n",
    "        self.prediction_op = theano.function(\n",
    "            inputs=[thx],\n",
    "            outputs=[py_x, prediction],\n",
    "            allow_input_downcast=True,\n",
    "        )\n",
    "        \n",
    "        ### back propagation\n",
    "        \n",
    "        cost = -T.mean(T.log(py_x[T.arange(thy.shape[0]), thy]))\n",
    "        grads = T.grad(cost, self.params)\n",
    "        dparams = [theano.shared(p.get_value()*0) for p in self.params]\n",
    "        \n",
    "        gWe = T.grad(cost, self.We)\n",
    "        dWe = theano.shared(self.We.get_value()*0)\n",
    "        dWe_update = mu*dWe - learning_rate*gWe\n",
    "        We_update = self.We + dWe_update\n",
    "        \n",
    "        # normalize other parameters???\n",
    "        if normalize:\n",
    "            We_update /= We_update.norm(2)\n",
    "        \n",
    "        updates = [\n",
    "            (p, p + mu*dp - learning_rate*g) for p, dp, g in zip(self.params, dparams, grads)\n",
    "        ] + [\n",
    "            (dp, mu*dp - learning_rate*g) for dp, g in zip(self.dparams, grads)\n",
    "        ] + [\n",
    "            (self.We, We_update), (dWe, dWe_update)\n",
    "        ]\n",
    "        \n",
    "        self.train_op = theano.function(\n",
    "            inputs=[thx, thy],\n",
    "            outputs=[cost, prediction],\n",
    "            updates=updates,\n",
    "        )\n",
    "        \n",
    "        ### training\n",
    "        costs=[]\n",
    "        for i in range(epochs):\n",
    "            t0 = datetime.now()\n",
    "            X = shuffle(X)\n",
    "            cost=0\n",
    "            n_correct=0\n",
    "            n_total=0\n",
    "            \n",
    "            for j in range(N):\n",
    "                if np.random.random() < 0.01 or len(X[j]) <=1:\n",
    "                    input_sequence = [0] + X[j]\n",
    "                    output_sequence = X[j] + [1]\n",
    "                else:\n",
    "                    input_sequence = [0] + X[j][:-1]\n",
    "                    output_sequence = X[j]\n",
    "                n_total += len(output_sequence)\n",
    "                \n",
    "                try:\n",
    "                    c, p = self.train_op(input_sequence, output_sequence)\n",
    "                except Exception as e:\n",
    "                    py_x, pred = self.prediction_op(input_sequence)\n",
    "                    print(\"input_sequence len:\", len(input_sequence))\n",
    "                    print(\"py_x.shape\", py_x.shape)\n",
    "                    print(\"pred.shape\", pred.shape)\n",
    "                    raise e\n",
    "                cost+=c\n",
    "                for pj, xj in zip(p, output_sequence):\n",
    "                    if pj == xj:\n",
    "                        n_correct+=1\n",
    "                \n",
    "                if j % 200 == 0:\n",
    "                    sys.stdout.write(\"j/N: %d/%d correct rate so far: %f\\r\" % (j, N, float(n_correct)/n_total))\n",
    "                    sys.stdout.flush()\n",
    "            print(\"i:\", i, \"cost:\", cost, \"correct rate:\", (float(n_correct)/n_total), \"time for epoch:\", (datetime.now() - t0))\n",
    "            costs.append(cost)\n",
    "        \n",
    "        \n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_WORDS = set([\n",
    "  'king', 'man', 'queen', 'woman',\n",
    "  'italy', 'rome', 'france', 'paris',\n",
    "  'london', 'britain', 'england',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_with_word2idx_limit_vocab(n_vocab=2000, keep_words=KEEP_WORDS):\n",
    "    sentences = brown.sents()\n",
    "    indexed_sentences = []\n",
    "\n",
    "    i = 2\n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    idx2word = ['START', 'END']\n",
    "\n",
    "    word_idx_count = {\n",
    "        0: float('inf'),\n",
    "        1: float('inf'),\n",
    "    }\n",
    "\n",
    "    for sentence in sentences:\n",
    "        indexed_sentence = []\n",
    "        for token in sentence:\n",
    "            token = token.lower()\n",
    "            if token not in word2idx:\n",
    "                idx2word.append(token)\n",
    "                word2idx[token] = i\n",
    "                i += 1\n",
    "\n",
    "            # keep track of counts for later sorting\n",
    "            idx = word2idx[token]\n",
    "            word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
    "            indexed_sentence.append(idx)\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "\n",
    "    # restrict vocab size\n",
    "\n",
    "    # set all the words I want to keep to infinity\n",
    "    # so that they are included when I pick the most\n",
    "    # common words\n",
    "    for word in keep_words:\n",
    "        word_idx_count[word2idx[word]] = float('inf')\n",
    "\n",
    "    sorted_word_idx_count = sorted(word_idx_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    word2idx_small = {}\n",
    "    new_idx = 0\n",
    "    idx_new_idx_map = {}\n",
    "    for idx, count in sorted_word_idx_count[:n_vocab]:\n",
    "        word = idx2word[idx]\n",
    "#         print(word, count)\n",
    "        word2idx_small[word] = new_idx\n",
    "        idx_new_idx_map[idx] = new_idx\n",
    "        new_idx += 1\n",
    "    # let 'unknown' be the last token\n",
    "    word2idx_small['UNKNOWN'] = new_idx \n",
    "    unknown_idx = new_idx\n",
    "\n",
    "    assert('START' in word2idx_small)\n",
    "    assert('END' in word2idx_small)\n",
    "    for word in keep_words:\n",
    "        assert(word in word2idx_small)\n",
    "\n",
    "    # map old idx to new idx\n",
    "    sentences_small = []\n",
    "    for sentence in indexed_sentences:\n",
    "        if len(sentence) > 1:\n",
    "            new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown_idx for idx in sentence]\n",
    "            sentences_small.append(new_sentence)\n",
    "\n",
    "    return sentences_small, word2idx_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_with_word2idx():\n",
    "    sentences = brown.sents()\n",
    "    indexed_sentences = []\n",
    "\n",
    "    i = 2\n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    for sentence in sentences:\n",
    "        indexed_sentence = []\n",
    "        for token in sentence:\n",
    "            token = token.lower()\n",
    "            if token not in word2idx:\n",
    "                word2idx[token] = i\n",
    "                i += 1\n",
    "            indexed_sentence.append(word2idx[token])\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "\n",
    "    return indexed_sentences, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished retrieving data\n",
      "vocab size 2001 number of sentences: 57013\n"
     ]
    }
   ],
   "source": [
    "# sentences, word2idx = get_sentences_with_word2idx()\n",
    "sentences, word2idx = get_sentences_with_word2idx_limit_vocab()\n",
    "print(\"finished retrieving data\")\n",
    "print(\"vocab size\", len(word2idx), \"number of sentences:\", len(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 cost: 324226.92534 correct rate: 0.2080931524446576 time for epoch: 0:05:54.758894\n",
      "i: 1 cost: 280406.053249 correct rate: 0.2081783554435123 time for epoch: 0:05:27.822977\n",
      "i: 2 cost: 277492.032165 correct rate: 0.20818283651805403 time for epoch: 0:05:47.327973\n",
      "i: 3 cost: 275133.250279 correct rate: 0.20492449746775063 time for epoch: 0:05:49.119582\n",
      "i: 4 cost: 272193.090854 correct rate: 0.20325882940437037 time for epoch: 0:05:49.083675\n",
      "i: 5 cost: 269022.677996 correct rate: 0.20658043829407652 time for epoch: 0:05:34.564550\n",
      "i: 6 cost: 266469.985048 correct rate: 0.20977401917987026 time for epoch: 5:52:40.614863\n",
      "i: 7 cost: 264251.408675 correct rate: 0.2121279437101403 time for epoch: 0:09:43.934519\n",
      "i: 8 cost: 262708.943625 correct rate: 0.21398880041877893 time for epoch: 0:05:28.583144\n",
      "i: 9 cost: 261598.872736 correct rate: 0.21477322528346302 time for epoch: 0:07:04.629615\n",
      "i: 10 cost: 260789.48436 correct rate: 0.21581127039476516 time for epoch: 0:06:15.671608\n",
      "i: 11 cost: 260116.715681 correct rate: 0.21598618955779034 time for epoch: 0:06:07.101087\n",
      "i: 12 cost: 259521.475603 correct rate: 0.21622838702011457 time for epoch: 0:05:35.972903\n",
      "i: 13 cost: 259131.134218 correct rate: 0.2170063687138332 time for epoch: 0:05:20.668351\n",
      "i: 14 cost: 258541.267337 correct rate: 0.2175317574979035 time for epoch: 0:22:25.761990\n",
      "i: 15 cost: 258184.931771 correct rate: 0.2179994610218028 time for epoch: 0:05:24.997401\n",
      "i: 16 cost: 257790.136973 correct rate: 0.2184620921718346 time for epoch: 0:05:17.027338\n",
      "i: 17 cost: 257471.966335 correct rate: 0.2185046370080686 time for epoch: 0:07:19.134961\n",
      "i: 18 cost: 257180.246641 correct rate: 0.2186219585377046 time for epoch: 0:07:16.582015\n",
      "i: 19 cost: 256953.934438 correct rate: 0.218822569104515 time for epoch: 0:07:05.066978\n",
      "i: 20 cost: 256532.604907 correct rate: 0.21914795789212657 time for epoch: 0:06:33.084555\n",
      "i: 21 cost: 256287.746373 correct rate: 0.21942420647152114 time for epoch: 0:06:03.773888\n",
      "i: 22 cost: 256005.270179 correct rate: 0.21959437061145767 time for epoch: 0:06:00.886194\n",
      "i: 23 cost: 255656.926588 correct rate: 0.21994591388064033 time for epoch: 0:06:03.851213\n",
      "i: 24 cost: 255345.332452 correct rate: 0.21998329702876526 time for epoch: 0:05:53.602036\n",
      "i: 25 cost: 255107.738526 correct rate: 0.22026951745681322 time for epoch: 0:06:03.781584\n",
      "i: 26 cost: 254809.821068 correct rate: 0.22049631150081622 time for epoch: 0:05:55.409786\n",
      "i: 27 cost: 254494.524182 correct rate: 0.221335858966356 time for epoch: 0:06:01.512949\n",
      "i: 28 cost: 254240.986493 correct rate: 0.221730127639424 time for epoch: 5:41:20.695150\n",
      "i: 29 cost: 253854.649581 correct rate: 0.22187763409734892 time for epoch: 3:30:41.589054\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIhIxhQaIiKSMYWGiIhkTKEhIiIZ+/9+6LRhQ5m8iwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c2315c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "V = len(word2idx)\n",
    "rnn = RNN(50, [50], V)\n",
    "rnn.fit(sentences, learning_rate=1e-5, show_fig=True, activation=T.nnet.relu, epochs=30)  \n",
    "\n",
    "we_file = \"word_embeddings.npy\"\n",
    "w2i_file = \"wikipedia_word2idx.json\"\n",
    "np.save(we_file, rnn.We.get_value())\n",
    "with open(w2i_file, 'w') as f:\n",
    "    json.dump(word2idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogies(w1, w2, w3):\n",
    "    We = np.load(we_file)\n",
    "    with open(w2i_file) as f:\n",
    "        word2idx = json.load(f)\n",
    "\n",
    "    kind = We[word2idx[w1]]\n",
    "    man = We[word2idx[w2]]\n",
    "    woman = We[word2idx[w3]]\n",
    "\n",
    "    v0 = king - man + woman\n",
    "    \n",
    "    def dist1(x, y):\n",
    "        return np.linalg.norm(a-b)\n",
    "    \n",
    "    def dist2(x, y):\n",
    "        return 1 - a.dot(b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    \n",
    "    for dist, name in [(dist1, \"Euclidean\"), (dist2, 'cosine')]:\n",
    "        min_dist = float('inf')\n",
    "        best_word = ''\n",
    "        for word, idx in word2idx.iterms():\n",
    "            if word not in (w1, w2, w3):\n",
    "                v1 = We[idx]\n",
    "                d = dist(v0, v1)\n",
    "                if d < min_dist:\n",
    "                    min_dist = d\n",
    "                    best_word = word\n",
    "        print(\"the closet match by\", name, \"distance:\", best_word)\n",
    "        print(w1, \"-\", w2, \"=\", best_word, \"-\", w3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_analogies(\"king\", \"man\", \"woman\")\n",
    "find_analogies(\"france\", \"paris\", \"london\")\n",
    "find_analogies(\"paris\", \"france\", \"italy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
