{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences_with_word2index():\n",
    "    # Brown has 574340 sentences\n",
    "    # Eacg sentence is represented as a list of individual string tokens\n",
    "    sentences = brown.sents()\n",
    "    word2index = {'START':0, 'END':1}\n",
    "    indexed_sentences = []\n",
    "            \n",
    "    index = 2\n",
    "    for sentence in sentences:\n",
    "        indexed_sentence = []\n",
    "        for token in sentence:\n",
    "            token = token.lower()\n",
    "            if token not in word2index:\n",
    "                word2index[token] = index\n",
    "                index+=1\n",
    "            indexed_sentence.append(word2index[token])\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "    print(\"vocb size:\", index)\n",
    "    return indexed_sentences, word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weight(Mi, Mo):\n",
    "    return np.random.randn(Mi, Mo) / np.sqrt(Mi + Mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(object):\n",
    "    def __init__(self, Mi, Mo, activation):\n",
    "        self.Mi = Mi\n",
    "        self.Mo = Mo\n",
    "        self.activation = activation\n",
    "        \n",
    "        W_xi = init_weight(Mi, Mo)\n",
    "        W_ci = init_weight(Mo, Mo)\n",
    "        W_hi = init_weight(Mo, Mo)\n",
    "        bi = np.zeros(Mo)\n",
    "        \n",
    "        W_xf = init_weight(Mi, Mo)\n",
    "        W_cf = init_weight(Mo, Mo)\n",
    "        W_hf = init_weight(Mo, Mo)\n",
    "        bf = np.zeros(Mo)\n",
    "        \n",
    "        W_xo = init_weight(Mi, Mo)\n",
    "        W_co = init_weight(Mo, Mo)\n",
    "        W_ho = init_weight(Mo, Mo)\n",
    "        bo = np.zeros(Mo)\n",
    "        \n",
    "        W_xc = init_weight(Mi, Mo)\n",
    "        W_hc = init_weight(Mo, Mo)\n",
    "        bc = np.zeros(Mo)\n",
    "        \n",
    "        c0 = np.zeros(Mo)\n",
    "        h0 = np.zeros(Mo)\n",
    "        \n",
    "        self.W_xi = theano.shared(W_xi)\n",
    "        self.W_ci = theano.shared(W_ci)\n",
    "        self.W_hi = theano.shared(W_hi)\n",
    "        self.bi = theano.shared(bi)\n",
    "        \n",
    "        self.W_xf = theano.shared(W_xf)\n",
    "        self.W_cf = theano.shared(W_cf)\n",
    "        self.W_hf = theano.shared(W_hf)\n",
    "        self.bf = theano.shared(bf)\n",
    "        \n",
    "        self.W_xo = theano.shared(W_xo)\n",
    "        self.W_co = theano.shared(W_co)\n",
    "        self.W_ho = theano.shared(W_ho)\n",
    "        self.bo = theano.shared(bo)\n",
    "        \n",
    "        self.W_xc = theano.shared(W_xc)\n",
    "        self.W_hc = theano.shared(W_hc)\n",
    "        self.bc = theano.shared(bc)\n",
    "        \n",
    "        self.c0 = theano.shared(c0)\n",
    "        self.h0 = theano.shared(h0)\n",
    "        \n",
    "        self.params = [self.W_xi, self.W_ci, self.W_hi, self.bi,\n",
    "                       self.W_xf, self.W_cf, self.W_hf, self.bf,\n",
    "                       self.W_xo, self.W_co, self.W_ho, self.bo,\n",
    "                       self.W_xc, self.W_hc, self.bc, self.c0, self.h0]\n",
    "        \n",
    "    def recurrence(self, x_t, h_t1, c_t1):\n",
    "        \n",
    "        i_t = T.nnet.sigmoid(x_t.dot(self.W_xi) + h_t1.dot(self.W_hi) + c_t1.dot(self.W_ci) + self.bi)\n",
    "        f_t = T.nnet.sigmoid(x_t.dot(self.W_xf) + h_t1.dot(self.W_hf) + c_t1.dot(self.W_cf) + self.bf)\n",
    "        \n",
    "        c_hat_t = self.f(x_t.dot(self.W_xc) + h_t1.dot(self.W_hc) + self.bc)\n",
    "        \n",
    "        c_t = f_t * c_t1 + i_t * c_hat_t\n",
    "        \n",
    "        # note o_t depends on c_t, not c_t1\n",
    "        o_t = T.nnet.sigmoid(x_t.dot(self.W_xo) + h_t1.dot(sefl.W_ho) + c_t.dot(self.W_co) + self.bo)\n",
    "        \n",
    "        h_t = o_t * self.f(c_t)\n",
    "        \n",
    "        return h_t, c_t\n",
    "    \n",
    "    def output(self, x):\n",
    "        [h, c], _ = theano.scan(\n",
    "            fn=self.recurrence,\n",
    "            sequence=x,\n",
    "            outputs_info=[self.h0, self.c0],\n",
    "            n_steps=x.shape[0]\n",
    "        )\n",
    "        return h\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-b242e266a7e1>, line 120)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-b242e266a7e1>\"\u001b[0;36m, line \u001b[0;32m120\u001b[0m\n\u001b[0;31m    costs.append(cost)\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class RNN(object):\n",
    "    def __init__(self, V, D, hidden_layer_sizes):\n",
    "        self.V = V;\n",
    "        self.D = D\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        \n",
    "    def fit(self, X, learning_rate=1e-5, mu=0.99, epochs=10, show_fig=True, activation=T.nnet.relu, \n",
    "            RecurrentUnit=LSTM, normalize=True):\n",
    "        \n",
    "        D = self.D\n",
    "        V = self.V\n",
    "        N = len(X)\n",
    "        \n",
    "        ### initialize hidden layers (i.e., recurrent units)\n",
    "        \n",
    "        self.hidden_layers = []\n",
    "        Mi = D\n",
    "        for Mo in self.hidden_layer_sizes:\n",
    "            ru = RecurrentUnit(Mi, Mo)\n",
    "            self.hidden_layers.append(ru)\n",
    "            Mi = Mo\n",
    "            \n",
    "        ### initialize weights for word embedding layer and output layer\n",
    "        \n",
    "        We = init_weight(V, D)\n",
    "        Wo = init_weight(Mi, V) \n",
    "        bo = np.zeros(V)\n",
    "        \n",
    "        self.We = theano.shared(We)\n",
    "        self.Wo = theano.shared(Wo)\n",
    "        self.bo = theano.shared(bo)\n",
    "        \n",
    "        # Note we do collect self.We here\n",
    "        self.params = [self.Wo, self.bo]\n",
    "        \n",
    "        ### create training vector\n",
    "        \n",
    "        thx = theano.ivector('X')\n",
    "        thy = theano.ivector('Y')\n",
    "        \n",
    "        ### forward propagation\n",
    "        \n",
    "        # get sequence of word embedding from the input sequence of word indexed\n",
    "        Z = self.We[thx]\n",
    "        for ru in self.hidden_layers:\n",
    "            Z = ru.output(Z)\n",
    "        py_x = T.nnet.softmax(Z.dot(self.Wo) + self.bo)\n",
    "        \n",
    "        prediction = T.argmax(py_x, axis=1)\n",
    "        prediction_op = theano.function(\n",
    "            inputs=[thx],\n",
    "            outputs=[py_x, prediction],  \n",
    "            allow_input_downcast=True,\n",
    "        )\n",
    "        \n",
    "        ### create symbolic expressions for gradient descent\n",
    "            \n",
    "        cost = -T.mean(T.log(py_x[T.arange(thy.shape[0]), thy]))\n",
    "        grads = T.grad(cost, self.params)\n",
    "        dparams = [theano.shared(p.get_value()*0) for p in self.params]\n",
    "        \n",
    "        gWd = T.grad(cost, self.We)\n",
    "        dWe = theano.shared(self.We.get_value()*0)\n",
    "        dWe_update = mu*dWe - learning_rate*gWd\n",
    "        We_update = self.We + dWe_update\n",
    "        \n",
    "        # Why we normalize We here not all of them or none of them ???\n",
    "        if normalize:\n",
    "            We_update /= We_update.norm(2)\n",
    "            \n",
    "        updates = [\n",
    "            (p, p + mu*dp - learning_rate*g) for p, dp, g in zip(self.params, dparams, grads)\n",
    "        ] + [\n",
    "            (dp, mu*dp - learning_rate *g) for dp, g in zip(dparams, grads)\n",
    "        ] + [\n",
    "            (self.We, We_update),(dWe, dWe_update)\n",
    "        ]\n",
    "        \n",
    "        self.train_op = theano.function(\n",
    "            inputs=[thx, thy],\n",
    "            outputs=[cost, prediction],\n",
    "            updates=updates,\n",
    "        )\n",
    "        \n",
    "        ### training\n",
    "        costs=[]\n",
    "        for i in range(epochs):\n",
    "            t0 = datetime.now()\n",
    "            X = shuffle(X)\n",
    "            cost=0\n",
    "            n_correct=0\n",
    "            n_total=0\n",
    "            \n",
    "            for j in N:\n",
    "                if np.random.random() < 0.01 or len(X[j]) <=1:\n",
    "                    input_sequence = [0] + X[j]\n",
    "                    output_sequence = X[j] + [1]\n",
    "                else:\n",
    "                    input_sequence = [0] + X[j][:-1]\n",
    "                    output_sequence = X[j]\n",
    "                n_total += len(output_sequence)\n",
    "                \n",
    "                try:\n",
    "                    c, p = self.train_op(input_sequence, output_sequence)\n",
    "                except Exception as e:\n",
    "                    py_x, pred = self.predict_op(input_sequence)\n",
    "                    print(\"input_sequence len:\", len(input_sequence))\n",
    "                    print(\"py_x.shape\", py_x.shape)\n",
    "                    print(\"pred.shape\", pred.shape)\n",
    "                    raise e\n",
    "                cost+=c\n",
    "                for pj, xj in zip(p, output_sequence):\n",
    "                    if pj == xj:\n",
    "                        n_correct+=1\n",
    "                \n",
    "                if j % 200 == 0:\n",
    "                    sys.stdout.write(\"j/N: %d/%d correct rate so far: %f\\r\" % (j, N, float(n_correct)/n_total))\n",
    "                    sys.stdout.flush()\n",
    "            print(\"i:\", i, \"cost:\", cost, \"correct rate:\", (float(n_correct)/n_total), \n",
    "                  \"time for epoch:\", (datetime.now() - t0)\n",
    "            costs.append(cost)\n",
    "        \n",
    "        \n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
