{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    return s.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_robert_frost():\n",
    "    word2idx = {'START':0, 'END':1}\n",
    "    current_idx = 2\n",
    "    sentences = []\n",
    "    for line in open(\"../data/hmm/robert_frost.txt\"):\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            tokens = remove_punctuation(line.lower()).split()\n",
    "            sentence=[]\n",
    "            for t in tokens:\n",
    "                if t not in word2idx:\n",
    "                    word2idx[t] = current_idx\n",
    "                    current_idx += 1\n",
    "                sentence.append(word2idx[t])\n",
    "            sentences.append(sentence)\n",
    "    return sentences, word2idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weight(Mi, Mo):\n",
    "    return np.random.randn(Mi, Mo) / np.sqrt(Mi + Mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    def __init__(self, D, M, V):\n",
    "        self.D = D  # dimensionality of word embedding\n",
    "        self.M = M  # hidden layer size\n",
    "        self.V = V  # vocabulary size\n",
    "        \n",
    "    def fit(self, X, learning_rate=10e-1, mu=0.99, reg=1.0, activation=T.tanh, epochs=500, show_fig=False):\n",
    "        \n",
    "        N = len(X)\n",
    "        D = self.D\n",
    "        M = self.M\n",
    "        V = self.V\n",
    "        \n",
    "        # initialize weights\n",
    "        We = init_weight(V, D)\n",
    "        Wx = init_weight(D, M)\n",
    "        Wh = init_weight(M, M)\n",
    "        bh = np.zeros(M)\n",
    "        h0 = np.zeros(M)\n",
    "        Wo = init_weight(M, V)\n",
    "        bo = np.zeros(V)\n",
    "        \n",
    "        W_xz = init_weight(D, M)\n",
    "        W_hz = init_weight(M, M)\n",
    "        bz = np.zeros(M)\n",
    "        \n",
    "        ################################################\n",
    "        ###  Prediction through forward propagation  ###\n",
    "        ################################################\n",
    "        \n",
    "        thX, thY, py_x, prediction = self.set_forward(We, Wx, Wh, Wo, bh, h0, bo, W_xz, W_hz, bz, activation)\n",
    "        \n",
    "        lr = T.scalar('learning_rate')\n",
    "            \n",
    "        ############################################\n",
    "        ###  Define Cross Entropy and Optimizer  ###\n",
    "        ############################################\n",
    "        \n",
    "        # Symbolic expression of the cross entropy\n",
    "        # Note, here utilizing matrix indexing to calculate the cross entropy\n",
    "        cost = -T.mean(T.log(py_x[T.arange(thY.shape[0]), thY]))\n",
    "        \n",
    "        # Symbolic expression of the gradient descent\n",
    "        grads = T.grad(cost, self.params)\n",
    "        \n",
    "        # Initialize momentum for all shared parameters\n",
    "        dparams = [theano.shared(p.get_value()*0) for p in self.params]\n",
    "        \n",
    "        # Define rules for updating gradients and momentum\n",
    "        updates = [\n",
    "            (p, p + mu*dp - lr*g) for p, dp, g in zip(self.params, dparams, grads)\n",
    "        ] + [\n",
    "            (dp, mu*dp - lr*g) for dp, g in zip(dparams, grads)\n",
    "        ]\n",
    "        \n",
    "        self.train_op = theano.function(\n",
    "            inputs=[thX, thY, lr],\n",
    "            outputs=[cost, prediction, py_x],\n",
    "            updates=updates,\n",
    "        )\n",
    "        \n",
    "        ################################################\n",
    "        ###  Start Training through Backpropagation  ###\n",
    "        ################################################\n",
    "        \n",
    "        costs = []\n",
    "        \n",
    "        # Calculate the total number of words in all sentences\n",
    "        n_total = sum((len(sentence) + 1) for sentence in X)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            X = shuffle(X)\n",
    "            n_correct = 0\n",
    "            n_total = 0\n",
    "            cost = 0\n",
    "            for j in range(N):\n",
    "                # Using stochastic gradient descent\n",
    "                if np.random.random() < 0.1:\n",
    "                    input_sequence = [0] + X[j]\n",
    "                    output_sequence = X[j] + [1]\n",
    "                else:\n",
    "                    input_sequence = [0] + X[j][:-1]\n",
    "                    output_sequence = X[j]\n",
    "                n_total += len(output_sequence) \n",
    "                \n",
    "                c, p, py_x_ = self.train_op(input_sequence, output_sequence, learning_rate)\n",
    "                cost += c\n",
    "                for pj, xj in zip(p, output_sequence):\n",
    "                    if pj == xj:\n",
    "                        n_correct += 1\n",
    "                        \n",
    "            if i % 20 == 0:\n",
    "                print(\"i:\", i, \"learning rate:\", learning_rate, \"cost:\", cost, \"correct rate:\", (float(n_correct)/ n_total))\n",
    "            \n",
    "            # half the learning rate every 500 epochs\n",
    "            if (i + 1) % 500 == 0:\n",
    "                learning_rate /= 2\n",
    "            costs.append(cost)\n",
    "        \n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "            \n",
    "   \n",
    "    def set_forward(self, We, Wx, Wh, Wo, bh, h0, bo, W_xz, W_hz, bz, activation):\n",
    "        \n",
    "        self.f = activation\n",
    "        \n",
    "        # Define all shared parameters that would be updated during the training\n",
    "        self.We = theano.shared(We)\n",
    "        self.Wx = theano.shared(Wx)\n",
    "        self.Wh = theano.shared(Wh)\n",
    "        self.Wo = theano.shared(Wo)\n",
    "        self.bh = theano.shared(bh)\n",
    "        self.bo = theano.shared(bo)\n",
    "        self.h0 = theano.shared(h0)\n",
    "        \n",
    "        self.W_xz = theano.shared(W_xz)\n",
    "        self.W_hz = theano.shared(W_hz)\n",
    "        self.bz = theano.shared(bz)\n",
    "        \n",
    "        # Collect all the parameters to make it easy to do gradient descent\n",
    "        self.params = [self.We, self.Wx, self.Wh, self.Wo, self.bh, self.h0, self.bo, self.W_xz, self.W_hz, self.bz]\n",
    "        \n",
    "        # a sequence of indices\n",
    "        thX = T.ivector('X')\n",
    "        \n",
    "        # Get the word embeddings indexed by the index sequence, i.e., thX\n",
    "        # Ei has shape T x D. Ei can also be think of as a sequence of T timesteps \n",
    "        # with D features per timestep\n",
    "        Ei = self.We[thX] \n",
    "        \n",
    "        thY = T.ivector('Y')\n",
    "        \n",
    "        # This is how we define simple recurrent unit, I left it here for comparison\n",
    "        #\n",
    "        # def recurrence(x_t, h_t1):\n",
    "        # h_t = self.f(x_t.dot(self.Wx) + h_t1.dot(self.Wh) + self.bh)\n",
    "        # y_t = T.nnet.softmax(h_t.dot(self.Wo) + self.bo)\n",
    "        # return h_t, y_t\n",
    "            \n",
    "        # Rated Recurrent Unit\n",
    "        def recurrence(x_t, h_t1):\n",
    "            z_t = T.nnet.sigmoid(x_t.dot(self.W_xz) + h_t1.dot(self.W_hz) + self.bz)\n",
    "            h_hat_t = self.f(x_t.dot(self.Wx) + h_t1.dot(self.Wh) + self.bh)\n",
    "            h_t = (1 - z_t) * h_t1 + z_t * h_hat_t\n",
    "            y_t = T.nnet.softmax(h_t.dot(self.Wo) + self.bo)\n",
    "            return h_t, y_t\n",
    "\n",
    "        [h, y], _ = theano.scan(\n",
    "            fn=recurrence,\n",
    "            sequences=Ei,\n",
    "            outputs_info=[self.h0, None],\n",
    "            n_steps=Ei.shape[0],\n",
    "        )\n",
    "\n",
    "        py_x = y[:, 0, :]\n",
    "        \n",
    "        prediction = T.argmax(py_x, axis=1)\n",
    "        self.prediction_op = theano.function(\n",
    "            inputs=[thX], \n",
    "            outputs=[py_x, prediction],\n",
    "            allow_input_downcast=True,\n",
    "        )\n",
    "        \n",
    "        return thX, thY, py_x, prediction\n",
    "    \n",
    "    \n",
    "    def save(self, filename):\n",
    "        np.savez(filename, *[p.get_value() for p in self.params])\n",
    "\n",
    "    def predict(self, X):\n",
    "        py_x, _ = self.prediction_op(X)\n",
    "        return py_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 learning rate: 0.0001 cost: 10984.2751822 correct rate: 0.04897096655641308\n",
      "i: 20 learning rate: 0.0001 cost: 8600.78732045 correct rate: 0.053882287924841114\n",
      "i: 40 learning rate: 0.0001 cost: 8086.49118423 correct rate: 0.05770823745973309\n",
      "i: 60 learning rate: 0.0001 cost: 7420.95419312 correct rate: 0.062080382599098684\n",
      "i: 80 learning rate: 0.0001 cost: 6761.98356388 correct rate: 0.07663964627855564\n",
      "i: 100 learning rate: 0.0001 cost: 6174.31246704 correct rate: 0.11643330876934414\n",
      "i: 120 learning rate: 0.0001 cost: 5704.8362512 correct rate: 0.17016961651917403\n",
      "i: 140 learning rate: 0.0001 cost: 5392.24284528 correct rate: 0.19793871353639458\n",
      "i: 160 learning rate: 0.0001 cost: 5079.96838567 correct rate: 0.23521827224166514\n",
      "i: 180 learning rate: 0.0001 cost: 4849.10416392 correct rate: 0.26300285372364907\n",
      "i: 200 learning rate: 0.0001 cost: 4647.98528944 correct rate: 0.28311759845772516\n",
      "i: 220 learning rate: 0.0001 cost: 4483.31414403 correct rate: 0.30518941837957414\n",
      "i: 240 learning rate: 0.0001 cost: 4412.39763115 correct rate: 0.3136317676257009\n",
      "i: 260 learning rate: 0.0001 cost: 4129.77487067 correct rate: 0.35829025110782864\n",
      "i: 280 learning rate: 0.0001 cost: 3982.90406159 correct rate: 0.36709908915263595\n",
      "i: 300 learning rate: 0.0001 cost: 3848.7441692 correct rate: 0.38891442228145967\n",
      "i: 320 learning rate: 0.0001 cost: 4017.52043898 correct rate: 0.3556985294117647\n",
      "i: 340 learning rate: 0.0001 cost: 3576.97476985 correct rate: 0.42339601769911506\n",
      "i: 360 learning rate: 0.0001 cost: 3599.81460075 correct rate: 0.4172635445362718\n",
      "i: 380 learning rate: 0.0001 cost: 3429.6020264 correct rate: 0.44366715758468334\n",
      "i: 400 learning rate: 0.0001 cost: 3468.71932458 correct rate: 0.4275582573454914\n",
      "i: 420 learning rate: 0.0001 cost: 3479.87622863 correct rate: 0.42953205600589534\n",
      "i: 440 learning rate: 0.0001 cost: 3502.63526616 correct rate: 0.4257334682240412\n",
      "i: 460 learning rate: 0.0001 cost: 3328.91167385 correct rate: 0.45722713864306785\n",
      "i: 480 learning rate: 0.0001 cost: 2971.0955529 correct rate: 0.5190199871050934\n",
      "i: 500 learning rate: 5e-05 cost: 2729.00382532 correct rate: 0.5653495440729484\n",
      "i: 520 learning rate: 5e-05 cost: 2402.53444914 correct rate: 0.6318600368324125\n",
      "i: 540 learning rate: 5e-05 cost: 2372.62773829 correct rate: 0.6374355195283714\n",
      "i: 560 learning rate: 5e-05 cost: 2312.84950957 correct rate: 0.6492083946980854\n",
      "i: 580 learning rate: 5e-05 cost: 2900.83992691 correct rate: 0.5353395913859746\n",
      "i: 600 learning rate: 5e-05 cost: 2791.17857151 correct rate: 0.5559240832872674\n",
      "i: 620 learning rate: 5e-05 cost: 2270.4386206 correct rate: 0.6549055734684477\n",
      "i: 640 learning rate: 5e-05 cost: 2532.66216418 correct rate: 0.5982151071855737\n",
      "i: 660 learning rate: 5e-05 cost: 2243.98534017 correct rate: 0.6590929997240365\n",
      "i: 680 learning rate: 5e-05 cost: 4093.80382904 correct rate: 0.34955303658648973\n",
      "i: 700 learning rate: 5e-05 cost: 2302.43298241 correct rate: 0.646197753636531\n",
      "i: 720 learning rate: 5e-05 cost: 2214.78955569 correct rate: 0.6619135234590616\n",
      "i: 740 learning rate: 5e-05 cost: 2134.81358593 correct rate: 0.6763947707604493\n",
      "i: 760 learning rate: 5e-05 cost: 2198.39587611 correct rate: 0.660375621432517\n",
      "i: 780 learning rate: 5e-05 cost: 2139.99651031 correct rate: 0.6773271889400921\n",
      "i: 800 learning rate: 5e-05 cost: 2177.98373919 correct rate: 0.6635978641134229\n",
      "i: 820 learning rate: 5e-05 cost: 2121.32466804 correct rate: 0.6764353515804995\n",
      "i: 840 learning rate: 5e-05 cost: 2056.27086822 correct rate: 0.6883152673158185\n",
      "i: 860 learning rate: 5e-05 cost: 2045.75213993 correct rate: 0.6918978504501194\n",
      "i: 880 learning rate: 5e-05 cost: 2108.2158311 correct rate: 0.6741935483870968\n",
      "i: 900 learning rate: 5e-05 cost: 2653.25759627 correct rate: 0.5700754092330329\n",
      "i: 920 learning rate: 5e-05 cost: 2935.07876363 correct rate: 0.5195366369403328\n",
      "i: 940 learning rate: 5e-05 cost: 2054.29104868 correct rate: 0.6865218191861535\n",
      "i: 960 learning rate: 5e-05 cost: 2121.62289734 correct rate: 0.6713956701980655\n",
      "i: 980 learning rate: 5e-05 cost: 2209.74906457 correct rate: 0.6482282558674644\n",
      "i: 1000 learning rate: 2.5e-05 cost: 1851.73660241 correct rate: 0.7282848730217151\n",
      "i: 1020 learning rate: 2.5e-05 cost: 1812.57364781 correct rate: 0.7362110311750599\n",
      "i: 1040 learning rate: 2.5e-05 cost: 1799.11199058 correct rate: 0.7353401454478505\n",
      "i: 1060 learning rate: 2.5e-05 cost: 1768.79101303 correct rate: 0.7404615243173669\n",
      "i: 1080 learning rate: 2.5e-05 cost: 1779.82447859 correct rate: 0.7374908020603385\n",
      "i: 1100 learning rate: 2.5e-05 cost: 2700.01013895 correct rate: 0.5578782576664518\n",
      "i: 1120 learning rate: 2.5e-05 cost: 2047.94258028 correct rate: 0.687091728769896\n",
      "i: 1140 learning rate: 2.5e-05 cost: 1932.96045883 correct rate: 0.7155077262693157\n",
      "i: 1160 learning rate: 2.5e-05 cost: 1902.76302681 correct rate: 0.7166943062465451\n",
      "i: 1180 learning rate: 2.5e-05 cost: 1945.40000522 correct rate: 0.7088665868704539\n",
      "i: 1200 learning rate: 2.5e-05 cost: 1905.68456536 correct rate: 0.7139434153534236\n",
      "i: 1220 learning rate: 2.5e-05 cost: 1825.90409608 correct rate: 0.734340456890199\n",
      "i: 1240 learning rate: 2.5e-05 cost: 1804.08333813 correct rate: 0.7379132516806336\n",
      "i: 1260 learning rate: 2.5e-05 cost: 1838.05811658 correct rate: 0.7297745052922228\n",
      "i: 1280 learning rate: 2.5e-05 cost: 2026.72984302 correct rate: 0.6811339920583618\n",
      "i: 1300 learning rate: 2.5e-05 cost: 2571.49766734 correct rate: 0.5860859937257796\n",
      "i: 1320 learning rate: 2.5e-05 cost: 2032.72917057 correct rate: 0.6797162859248342\n",
      "i: 1340 learning rate: 2.5e-05 cost: 1775.56081013 correct rate: 0.7386499677686712\n",
      "i: 1360 learning rate: 2.5e-05 cost: 1754.3055627 correct rate: 0.7420722713864307\n",
      "i: 1380 learning rate: 2.5e-05 cost: 1827.74301013 correct rate: 0.7248735632183908\n",
      "i: 1400 learning rate: 2.5e-05 cost: 1749.61229169 correct rate: 0.741263254956201\n",
      "i: 1420 learning rate: 2.5e-05 cost: 2180.27983804 correct rate: 0.6513448784082535\n",
      "i: 1440 learning rate: 2.5e-05 cost: 1746.01645144 correct rate: 0.7435590725064409\n",
      "i: 1460 learning rate: 2.5e-05 cost: 1778.08255346 correct rate: 0.7353293413173653\n",
      "i: 1480 learning rate: 2.5e-05 cost: 1730.03645815 correct rate: 0.7455583172236031\n",
      "i: 1500 learning rate: 1.25e-05 cost: 1820.22847622 correct rate: 0.725739563173901\n",
      "i: 1520 learning rate: 1.25e-05 cost: 1689.29950959 correct rate: 0.7515870825282914\n",
      "i: 1540 learning rate: 1.25e-05 cost: 1688.27061869 correct rate: 0.7507355645457889\n",
      "i: 1560 learning rate: 1.25e-05 cost: 1678.8221903 correct rate: 0.7523282618718303\n",
      "i: 1580 learning rate: 1.25e-05 cost: 1672.99067517 correct rate: 0.7514945277292375\n",
      "i: 1600 learning rate: 1.25e-05 cost: 1668.63940839 correct rate: 0.7525564256103179\n",
      "i: 1620 learning rate: 1.25e-05 cost: 1670.95189199 correct rate: 0.7521643028181986\n",
      "i: 1640 learning rate: 1.25e-05 cost: 1704.87351717 correct rate: 0.7473723031532362\n",
      "i: 1660 learning rate: 1.25e-05 cost: 1667.67009808 correct rate: 0.7526465985455215\n",
      "i: 1680 learning rate: 1.25e-05 cost: 1663.08936144 correct rate: 0.7531137558815388\n",
      "i: 1700 learning rate: 1.25e-05 cost: 1668.40349027 correct rate: 0.7524151255865305\n",
      "i: 1720 learning rate: 1.25e-05 cost: 1661.58502766 correct rate: 0.7534789420329924\n",
      "i: 1740 learning rate: 1.25e-05 cost: 1666.28552354 correct rate: 0.7523467697404749\n",
      "i: 1760 learning rate: 1.25e-05 cost: 1649.12967838 correct rate: 0.7544166359955834\n",
      "i: 1780 learning rate: 1.25e-05 cost: 1648.41375352 correct rate: 0.7545554942020983\n",
      "i: 1800 learning rate: 1.25e-05 cost: 1643.44962638 correct rate: 0.7542341678939617\n",
      "i: 1820 learning rate: 1.25e-05 cost: 1643.64461776 correct rate: 0.7550832643297452\n",
      "i: 1840 learning rate: 1.25e-05 cost: 1668.79190347 correct rate: 0.7523502304147466\n",
      "i: 1860 learning rate: 1.25e-05 cost: 1659.56931784 correct rate: 0.7529998153959756\n",
      "i: 1880 learning rate: 1.25e-05 cost: 1656.06174347 correct rate: 0.7536859565057132\n",
      "i: 1900 learning rate: 1.25e-05 cost: 1684.36956261 correct rate: 0.7497238586156112\n",
      "i: 1920 learning rate: 1.25e-05 cost: 1644.66376644 correct rate: 0.7533425541724297\n",
      "i: 1940 learning rate: 1.25e-05 cost: 1641.36215425 correct rate: 0.7541436464088398\n",
      "i: 1960 learning rate: 1.25e-05 cost: 1641.74283015 correct rate: 0.7546232404085013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 1980 learning rate: 1.25e-05 cost: 1636.77005368 correct rate: 0.7542022595756407\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCvL/AUvZwwIHEgSVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1113fc5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences, word2idx = get_robert_frost()\n",
    "V = len(word2idx)\n",
    "rnn = SimpleRNN(50, 50, V)\n",
    "rnn.fit(sentences, learning_rate=10e-5, show_fig=True, activation=T.nnet.relu, epochs=2000) \n",
    "#vlr stands for variable learning rate\n",
    "rnn.save(\"generating_poetry_rrnn_D50_M50_epochs2000_relu_vlr.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load(filename, activation):\n",
    "    npz = np.load(filename)\n",
    "    We = npz['arr_0']\n",
    "    Wx = npz['arr_1']\n",
    "    Wh = npz['arr_2']\n",
    "    Wo = npz['arr_3']\n",
    "    bh = npz['arr_4']\n",
    "    h0 = npz['arr_5']\n",
    "    bo = npz['arr_6']\n",
    "    W_xz = npz['arr_7']\n",
    "    W_hz = npz['arr_8']\n",
    "    bz = npz['arr_9']\n",
    "        \n",
    "    V, D = We.shape\n",
    "    _, M = Wx.shape\n",
    "    rnn = SimpleRNN(D, M, V)\n",
    "    rnn.set_forward(We, Wx, Wh, Wo, bh, h0, bo, W_xz, W_hz, bz, activation)\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "always the witchs motto anyway \n",
      "i have outwalked the furthest city light \n",
      "one level nearer heaven overhead \n",
      "with all once on a day and said up \n"
     ]
    }
   ],
   "source": [
    "rnn = load(\"generating_poetry_rrnn_D50_M50_epochs2000_relu_vlr.npz\", T.nnet.relu)\n",
    "sentences, word2idx = get_robert_frost()\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "V = len(word2idx)\n",
    "\n",
    "# generate 4 lines at a time\n",
    "n_lines = 0\n",
    "\n",
    "# get the first word according to the probabilty distribution\n",
    "#   X = [np.random.choice(V, p=pi)]\n",
    "#   print(idx2word[X[0]])\n",
    "X = [0]\n",
    "\n",
    "while n_lines < 4:\n",
    "    \n",
    "    # py_x is the predicted distribution over words \n",
    "    # in vocabulary given word index sequence X, starting with [0]\n",
    "    py_x = rnn.predict(X)\n",
    "#     print(\"py_x shape:\", py_x.shape)\n",
    "#     print(\"py_x:\", py_x)\n",
    "    \n",
    "    py_x = py_x[-1].flatten()\n",
    "    \n",
    "    # Generate a value from V based on distribution of py_x\n",
    "    P = [np.random.choice(V, p=py_x)]\n",
    "#     X += [P]\n",
    "#     print(\"P shape:\", len(P))\n",
    "#     print(\"P:\", P)\n",
    "    # append to the sequence\n",
    "    X = np.concatenate([X, P]) \n",
    "#     print(\"X\", X)\n",
    "    # just grab the most recent prediction\n",
    "#     P = P[-1]\n",
    "    P = P[0]\n",
    "    if  P > 1:\n",
    "        word = idx2word[P]\n",
    "        print(word + \" \", end=\"\")\n",
    "    elif P == 1:\n",
    "        # end token\n",
    "        n_lines += 1\n",
    "        X = [0]\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
